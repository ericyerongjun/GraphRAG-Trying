{"chunk_id": "Lecture10_Recurrent_Neural_Networks_0", "doc_id": "Lecture10_Recurrent_Neural_Networks", "text": "COMP4434 Big Data AnalyticsLecture 10Recurrent Neural Networks HUANG Xiaoxiaohuang@comp.polyu.edu.hk\n\nNew Jersey Institute of Technology\nOutline§Vanilla Recurrent Neural Networks§Exploding and Vanishing Gradients§RNNs with Cell states§Long Short-Term Memory (LSTM)§Gated Recurrent Unit (GRU)§Sequence Learning Architectures§Sequence Learning with one RNN Layer§Sequence Learning with multiple RNN Layers§Application: Sequence-to-Sequence Model in Activities of Daily Living (ADL) Recognition\n3COMP443", "local_index": 0}
{"chunk_id": "Lecture10_Recurrent_Neural_Networks_1", "doc_id": "Lecture10_Recurrent_Neural_Networks", "text": "ers§Application: Sequence-to-Sequence Model in Activities of Daily Living (ADL) Recognition\n3COMP4434\nNew Jersey Institute of Technology\nRecurrent Neural Networks§Human brain deals with information streams. Most data is obtained, processed, and generated sequentially.§E.g., listening: soundwaves à vocabularies/sentences§E.g., reading: words in sequence§Human thoughts have persistence; humans don’t start their thinking from scratch every second. §As you read this sentence, you understand each wor", "local_index": 1}
{"chunk_id": "Lecture10_Recurrent_Neural_Networks_2", "doc_id": "Lecture10_Recurrent_Neural_Networks", "text": " start their thinking from scratch every second. §As you read this sentence, you understand each word based on your prior knowledge§The applications of multilayer perceptron and Convolutional Neural Networks are limited due to:§Only accept a fixed-size vector as input (e.g., an image) and produce a fixed-size vector as output (e.g., probabilities of different classes)§Recurrent Neural Networks (RNNs) are a family of neural networks introduced to learn sequential data.§Inspired by the temporal-de", "local_index": 2}
{"chunk_id": "Lecture10_Recurrent_Neural_Networks_3", "doc_id": "Lecture10_Recurrent_Neural_Networks", "text": "Ns) are a family of neural networks introduced to learn sequential data.§Inspired by the temporal-dependent and persistent human thoughts4COMP4434\nNew Jersey Institute of Technology\nReal-life Sequence Learning ApplicationsRNNs can be applied to various type of sequential data to learn the temporal patterns.§Time-series data (e.g., stock price) à Prediction, regression§Raw sensor data (e.g., signal, voice, handwriting) à Labels or text sequences§Text à Label (e.g., sentiment) or text sequence (e.", "local_index": 3}
{"chunk_id": "Lecture10_Recurrent_Neural_Networks_4", "doc_id": "Lecture10_Recurrent_Neural_Networks", "text": ", voice, handwriting) à Labels or text sequences§Text à Label (e.g., sentiment) or text sequence (e.g., translation, summary, answer) §Image and video à Text description (e.g., captions, scene interpretation)\n5COMP4434\nTask Input OutputActivity Recognition (Zhu et al. 2018)Sensor SignalsActivity LabelsMachine translation (Sutskever et al. 2014)English textFrench textQuestion answering (Bordes et al. 2014)QuestionAnswer Speech recognition (Graves et al. 2013)Voice Text Handwriting prediction (Gra", "local_index": 4}
{"chunk_id": "Lecture10_Recurrent_Neural_Networks_5", "doc_id": "Lecture10_Recurrent_Neural_Networks", "text": "l. 2014)QuestionAnswer Speech recognition (Graves et al. 2013)Voice Text Handwriting prediction (Graves 2013) HandwritingTextOpinion mining (Irsoy et al. 2014)Text Opinion expression\nNew Jersey Institute of Technology\nRNNs have loopsRNNs are networks with loops,  allowing information to persist.\n6COMP4434\nA chunk of neural network,A = fW , looks at some input!!and outputs a value ℎ!. A loop allows information to be passed from one step of the network to the next. \nOutput is to predict a vector h", "local_index": 5}
{"chunk_id": "Lecture10_Recurrent_Neural_Networks_6", "doc_id": "Lecture10_Recurrent_Neural_Networks", "text": " information to be passed from one step of the network to the next. \nOutput is to predict a vector ht, where final output !!=#(ℎ!) at some time steps !\n\nNew Jersey Institute of Technology\nUnrolling RNN\n§A recurrent neural network can be thought of as multiple copies of the same network, each passing a message to a successor§The diagram above shows what happens if we unroll the loop\n7COMP4434\n\nNew Jersey Institute of Technology\nPros and cons of vanilla RNNsThe recurrent structure of RNNs enables ", "local_index": 6}
{"chunk_id": "Lecture10_Recurrent_Neural_Networks_7", "doc_id": "Lecture10_Recurrent_Neural_Networks", "text": "Jersey Institute of Technology\nPros and cons of vanilla RNNsThe recurrent structure of RNNs enables the following characteristics:§Specialized for processing a sequence of values !\",…,!#§Each value !$ is processed with the same network A that preserves past information§Can scale to much longer sequences than would be practical for networks without a recurrent structure§Reusing network A reduces the required amount of parameters in the network§Can process variable-length sequences§The network com", "local_index": 7}
{"chunk_id": "Lecture10_Recurrent_Neural_Networks_8", "doc_id": "Lecture10_Recurrent_Neural_Networks", "text": "e required amount of parameters in the network§Can process variable-length sequences§The network complexity does not vary when the input length change§However, vanilla RNNs suffer from the training difficulty due to exploding and vanishing gradients8COMP4434\nNew Jersey Institute of Technology\nExploding and Vanishing Gradients\n§Exploding: If we start almost exactly on the boundary (cliff), tiny changes can make a huge difference.§Vanishing: If we start a trajectory within an attractor (plane, fla", "local_index": 8}
{"chunk_id": "Lecture10_Recurrent_Neural_Networks_9", "doc_id": "Lecture10_Recurrent_Neural_Networks", "text": "nges can make a huge difference.§Vanishing: If we start a trajectory within an attractor (plane, flat surface), small changes in where we start make no difference to where we end up.§Both cases hinder the learning process.9COMP4434\nCliff/boundary\nPlane/attractor\nNew Jersey Institute of Technology\nExploding and Vanishing Gradients in RNNs\nIn vanilla RNNs, computing this gradient involves many factors of $ (and repeated tanh)*. If we decompose the singular values of the gradient multiplication mat", "local_index": 9}
{"chunk_id": "Lecture10_Recurrent_Neural_Networks_10", "doc_id": "Lecture10_Recurrent_Neural_Networks", "text": "rs of $ (and repeated tanh)*. If we decompose the singular values of the gradient multiplication matrix,§Largest singular value > 1 à Exploding gradients§Slight error in the late time steps causes drastic updates in the early time steps à Unstable learning§Largest singular value < 1 à Vanishing gradients §Gradients passed to the early time steps is close to 0. à Uninformed correction 10* Refer to Bengio et al. (1994) or Goodfellow et al. (2016) for a complete derivation\n!!=#(%!,'%!)\nNew Jersey I", "local_index": 10}
{"chunk_id": "Lecture10_Recurrent_Neural_Networks_11", "doc_id": "Lecture10_Recurrent_Neural_Networks", "text": "Bengio et al. (1994) or Goodfellow et al. (2016) for a complete derivation\n!!=#(%!,'%!)\nNew Jersey Institute of Technology\nRNNs with Cell states§Vanilla RNN operates in a “multiplicative” way (repeated tanh)§Two recurrent cell designs were proposed and widely adopted: §Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997)§Gated Recurrent Unit (GRU) (Cho et al. 2014)§Both designs process information in an “additive” way with gates to control information flow§Sigmoid gate outputs number", "local_index": 11}
{"chunk_id": "Lecture10_Recurrent_Neural_Networks_12", "doc_id": "Lecture10_Recurrent_Neural_Networks", "text": " information in an “additive” way with gates to control information flow§Sigmoid gate outputs numbers between 0 and 1, describing how much of each component should be let through\nStandard LSTM Cell\nGRU Cell\nA Sigmoid Gate\n= Sigmoid ( Wf xt + Ut ht-1 + bf )E.g.,\nNew Jersey Institute of Technology\nActivation Functions\n12\n\nNew Jersey Institute of Technology\nLong Short-Term Memory (LSTM)\n§The key to LSTMs is the cell state.§Stores information of the past à long-term memory§Passes along time steps wi", "local_index": 12}
{"chunk_id": "Lecture10_Recurrent_Neural_Networks_13", "doc_id": "Lecture10_Recurrent_Neural_Networks", "text": "STMs is the cell state.§Stores information of the past à long-term memory§Passes along time steps with minor linear interactions à “additive”§Results in an uninterrupted gradient flow à errors in the past pertain and impact learning in the future§The LSTM cell manipulates input information with three gates.§Input gate à controls the intake of new information§Forget gate à determines what part of the cell state to be updated§Output gate à determines what part of the cell state to output13\nCell St", "local_index": 13}
{"chunk_id": "Lecture10_Recurrent_Neural_Networks_14", "doc_id": "Lecture10_Recurrent_Neural_Networks", "text": "he cell state to be updated§Output gate à determines what part of the cell state to output13\nCell State )\"\nGradient Flow\nNew Jersey Institute of Technology\nLSTM: Components & Flow\n§LSTM unit output§Output gate units§Transformed memory cell contents§Gated update to memory cell units§Forget gate units§Input gate units§Potential input to memory cell14COMP4434\n\nNew Jersey Institute of Technology\nStep-by-step LSTM Walk Through\nStep 1: Decide what information to throw away from the cell state (memory)", "local_index": 14}
{"chunk_id": "Lecture10_Recurrent_Neural_Networks_15", "doc_id": "Lecture10_Recurrent_Neural_Networks", "text": "by-step LSTM Walk Through\nStep 1: Decide what information to throw away from the cell state (memory) è§The output of the previous state ℎ%&\" and the new information !% jointly determine what to forget§ℎ%&\" contains selected features from the memory *%&\"§Forget gate +% ranges between [0,1]§Text processing example:§Cell state may include the gender of the current subject (ℎ%&\").§When the model observes a new subject (!%), it may want to forget (+%→0) the old subject in the memory (*%&\"). 15\nForget", "local_index": 15}
{"chunk_id": "Lecture10_Recurrent_Neural_Networks_16", "doc_id": "Lecture10_Recurrent_Neural_Networks", "text": "ves a new subject (!%), it may want to forget (+%→0) the old subject in the memory (*%&\"). 15\nForget gate\n\nNew Jersey Institute of Technology\nStep 2: Input gate\nStep 2: Prepare the updates for the cell state from input è§An alternative cell state 1*% is created from the new information !% with the guidance of ℎ%&\"§Input gate 2% ranges between [0,1]§Example: the model may want to add (2%→1) the gender of new subject (1*%) to the cell state to replace the old one it is forgetting 16COMP4434\nInput ", "local_index": 16}
{"chunk_id": "Lecture10_Recurrent_Neural_Networks_17", "doc_id": "Lecture10_Recurrent_Neural_Networks", "text": "der of new subject (1*%) to the cell state to replace the old one it is forgetting 16COMP4434\nInput gateAlternative cell state\n\nNew Jersey Institute of Technology\nStep 3: Update the cell state\nStep 3: Update the cell state è §The new cell state *% is comprised of information from the past +%∗*%&\" and valuable new information 2%∗ 1*%§∗ denotes elementwise multiplication§Example: the model drops the old gender information (+%∗*%&\") and adds new gender information (2%∗1*%) to form the new cell stat", "local_index": 17}
{"chunk_id": "Lecture10_Recurrent_Neural_Networks_18", "doc_id": "Lecture10_Recurrent_Neural_Networks", "text": " old gender information (+%∗*%&\") and adds new gender information (2%∗1*%) to form the new cell state (*%) 17COMP4434\nNew cell state\n\nNew Jersey Institute of Technology\nStep 4: Outputgate\nStep 4: Decide the filtered output from the new cell state è §tanh function filters the new cell state to characterize stored information§Significant information in !! à ±1§Minor details à 0§Output gate 4% ranges between 0,1§ℎ% serves as a control signal for the next time step§Example:Since the model just saw a", "local_index": 18}
{"chunk_id": "Lecture10_Recurrent_Neural_Networks_19", "doc_id": "Lecture10_Recurrent_Neural_Networks", "text": " between 0,1§ℎ% serves as a control signal for the next time step§Example:Since the model just saw a new subject (!'), it might want to output (\"'→1) information relevant to a verb (tanh(*')), e.g., singular/plural, in case a verb comes next18\nOutput gate\n\nNew Jersey Institute of Technology\nGated Recurrent Unit (GRU)\n§GRU is a variation of LSTM that also adopts the gated design.§Differences:§GRU uses an update gate 5 to substitute the input and forget gates 2% and +%§Combined the cell state *% a", "local_index": 19}
{"chunk_id": "Lecture10_Recurrent_Neural_Networks_20", "doc_id": "Lecture10_Recurrent_Neural_Networks", "text": "ses an update gate 5 to substitute the input and forget gates 2% and +%§Combined the cell state *% and hidden state ℎ% in LSTM as a single cell state ℎ%§GRU obtains similar performance compared to LSTM with fewer parameters and faster convergence. (Cho et al. 2014)19\n\nNew Jersey Institute of Technology\nGated Recurrent Unit (GRU)\n§Update gate: controls the composition of the new state§Reset gate: determines how much old information is needed in the alternative state 1ℎ%§Alternative state: contain", "local_index": 20}
{"chunk_id": "Lecture10_Recurrent_Neural_Networks_21", "doc_id": "Lecture10_Recurrent_Neural_Networks", "text": "etermines how much old information is needed in the alternative state 1ℎ%§Alternative state: contains new information§New state: replace selected old information with new information in the new state20COMP4434\n\nNew Jersey Institute of Technology\nSequence Learning Architectures§Learning on RNN is more robust when the vanishing/exploding gradient problem is resolved.§RNNs can now be applied to different Sequence Learning tasks§Recurrent NN architecture is flexible to operate over various sequences", "local_index": 21}
{"chunk_id": "Lecture10_Recurrent_Neural_Networks_22", "doc_id": "Lecture10_Recurrent_Neural_Networks", "text": "rent Sequence Learning tasks§Recurrent NN architecture is flexible to operate over various sequences of vectors.§Sequence in the input, the output, or in the most general case both§Architecture with one or more RNN layers21COMP4434\nNew Jersey Institute of Technology\nSequence Learning with One RNN Layer\n1.Standard NN mode without recurrent structure (e.g., image classification, one label for one image)2.Sequence output (e.g., image captioning, takes an image and outputs a sentence of words)3.Sequ", "local_index": 22}
{"chunk_id": "Lecture10_Recurrent_Neural_Networks_23", "doc_id": "Lecture10_Recurrent_Neural_Networks", "text": "age)2.Sequence output (e.g., image captioning, takes an image and outputs a sentence of words)3.Sequence input (e.g., sentiment analysis, a sentence is classified as expressing positive or negative sentiment).4.Sequence input and sequence output (e.g., machine translation, a sentence in English is translated into a sentence in French)5.Synced sequence input and output (e.g., video classification, label each frame of the video) 22\n1 2 3 4 5•Each rectangle is a vector and arrows represent function", "local_index": 23}
{"chunk_id": "Lecture10_Recurrent_Neural_Networks_24", "doc_id": "Lecture10_Recurrent_Neural_Networks", "text": "label each frame of the video) 22\n1 2 3 4 5•Each rectangle is a vector and arrows represent functions (e.g., matrix multiply)•Input vectors are in red, output vectors are in blue and green vectors hold the RNN's state\nNew Jersey Institute of Technology\nSequence-to-Sequence (Seq2Seq) model§Developed by Google in 2018 for use in machine translation.§Seq2seq turns one sequence into another sequence. It does so by use of arecurrent neural network(RNN) or more oftenLSTMorGRUto avoid the problem ofvan", "local_index": 24}
{"chunk_id": "Lecture10_Recurrent_Neural_Networks_25", "doc_id": "Lecture10_Recurrent_Neural_Networks", "text": "It does so by use of arecurrent neural network(RNN) or more oftenLSTMorGRUto avoid the problem ofvanishing gradient. §The primary components are one Encoder and one Decoder network. The encoder turns each item into a corresponding hidden vector containing the item and its context. The decoder reverses the process, turning the vector into an output item, using the previous output as the input context.§Encoder RNN: extract and compress the     semantics from the input sequence§Decoder RNN: generat", "local_index": 25}
{"chunk_id": "Lecture10_Recurrent_Neural_Networks_26", "doc_id": "Lecture10_Recurrent_Neural_Networks", "text": "xt.§Encoder RNN: extract and compress the     semantics from the input sequence§Decoder RNN: generate a sequence based on the input semantics§Apply to tasks such as machine      translation§Similar underlying semantics§E.g., “I love you.” to “Je t’aime.” 23COMP4434\nBBB By0 y1 y2 ym…\nAn RNN as the encoderAn RNN as the decoder\nInput sequence\nEncoded semantics\nDecoded sequence\nNew Jersey Institute of Technology\nnum_layers\n24COMP4434\nNew Jersey Institute of Technology\nBidirectional RNN\n25COMP4434\nBB", "local_index": 26}
{"chunk_id": "Lecture10_Recurrent_Neural_Networks_27", "doc_id": "Lecture10_Recurrent_Neural_Networks", "text": " Technology\nnum_layers\n24COMP4434\nNew Jersey Institute of Technology\nBidirectional RNN\n25COMP4434\nBBBB\n§Connects two recurrent units (synced many-to-many model) of opposite directions to the same output.§Captures forward and backward information from the input sequence§Apply to data whose current state (e.g., ℎ\") can be better determined when given future information (e.g., %#,%$,…,%!)§E.g., in the sentence “the bank is robbed,” the semantics of “bank” can be determined given the verb “robbed.”\n", "local_index": 27}
{"chunk_id": "Lecture10_Recurrent_Neural_Networks_28", "doc_id": "Lecture10_Recurrent_Neural_Networks", "text": "e sentence “the bank is robbed,” the semantics of “bank” can be determined given the verb “robbed.”\n\nNew Jersey Institute of Technology\nRNNApplication: High-level ADL Recognition§Activities of Daily Living (ADLs) are introduced to evaluate the self-care ability of senior citizens (Williams 2014)§Sensor-based home monitoring systems§Environment: Placed in the environment to capture changes (e.g., on/off, motion)§Wearable: Attached to the body to measure movements and physiological signals§Sensors", "local_index": 28}
{"chunk_id": "Lecture10_Recurrent_Neural_Networks_29", "doc_id": "Lecture10_Recurrent_Neural_Networks", "text": "n/off, motion)§Wearable: Attached to the body to measure movements and physiological signals§Sensors sample at 10 Hz à 0.8 million records per day§Machine learning (esp. deep learning) needed to recognize ADLs from the large amount of data§Mid-level (ML) ADLs: gestures, gait, etc.§High-level (HL) ADLs: preparing food, medical intake, senior care, etc.§Research Objective: develop a universal ADL recognition framework to extract HL-ADLs from raw sensor data26\nNew Jersey Institute of Technology\nSeq", "local_index": 29}
{"chunk_id": "Lecture10_Recurrent_Neural_Networks_30", "doc_id": "Lecture10_Recurrent_Neural_Networks", "text": "cognition framework to extract HL-ADLs from raw sensor data26\nNew Jersey Institute of Technology\nSeq2Seq-ADL Research Design\n•Intuition: Recognizing the HL-ADLs from the sensor data is similar to captioning/translation. We can generate ADL label sequence with a Seq2Seq model for the input data. The underlying semantics are similar.•A Seq2Seq model is designed to extract HL-ADLs from the activity state sequence27COMP4434\n\nNew Jersey Institute of Technology\nActivity Reconstruction§Objective: creat", "local_index": 30}
{"chunk_id": "Lecture10_Recurrent_Neural_Networks_31", "doc_id": "Lecture10_Recurrent_Neural_Networks", "text": "tate sequence27COMP4434\n\nNew Jersey Institute of Technology\nActivity Reconstruction§Objective: create temporally aligned activity representation from different data sources§Four sensors for demonstration:§a force plate à pressure on the floor §a door on/off sensor à open (o) and close (c) states§a human motion sensor§object motion sensor attached on the fridge §Step 1. Extract discrete motion states from motion sensor data with a state-of-the-art gesture recognition model§Step 2. Interpolate dis", "local_index": 31}
{"chunk_id": "Lecture10_Recurrent_Neural_Networks_32", "doc_id": "Lecture10_Recurrent_Neural_Networks", "text": "es from motion sensor data with a state-of-the-art gesture recognition model§Step 2. Interpolate discrete data from each sensor28COMP4434\n\nNew Jersey Institute of Technology\nActivity Reconstruction§Steps 3. Sample each data stream at same timestamps to construct the Activity State representations§Temporally aligned observations§Steps 4. Encode the Activity States (%§Encode categorical (discrete) values using one-hot encoding§Step 5. Organize the states vector in a matrix )§Data matrix ) aggregat", "local_index": 32}
{"chunk_id": "Lecture10_Recurrent_Neural_Networks_33", "doc_id": "Lecture10_Recurrent_Neural_Networks", "text": "alues using one-hot encoding§Step 5. Organize the states vector in a matrix )§Data matrix ) aggregates temporally aligned sensor observation sequences to represent the activity29COMP4434\n\nNew Jersey Institute of Technology\nSeq2Seq - Encoder§The encoder network takes the Activity State Sequence 6 as the input to generate the activity semantics vector 7§The encoder network adopts GRU recurrent cells to learn temporal patterns§Each hidden state ℎ$' depends on the input !$ and the previous state ℎ$&", "local_index": 33}
{"chunk_id": "Lecture10_Recurrent_Neural_Networks_34", "doc_id": "Lecture10_Recurrent_Neural_Networks", "text": " to learn temporal patterns§Each hidden state ℎ$' depends on the input !$ and the previous state ℎ$&\"'\n30COMP4434\n§7 is expected to be a condensed representation for human/object motions, object usages, and their temporal patterns during the period.\nNew Jersey Institute of Technology\nSeq2Seq - Decoder§The decoder network takes the encoded activity semantics vector 7 to generate HL-ADL label for each input vector§The decoder network also adopts GRU recurrent cells to interpret the temporal patter", "local_index": 34}
{"chunk_id": "Lecture10_Recurrent_Neural_Networks_35", "doc_id": "Lecture10_Recurrent_Neural_Networks", "text": "ch input vector§The decoder network also adopts GRU recurrent cells to interpret the temporal patterns§Multi-class classification §Softmax à probability distribution over output classes§Categorical cross-entropy loss\n31COMP4434\n\nNew Jersey Institute of Technology\nExperimental results\n§S2S_GRU model is evaluated on two different datasets§S2S_GRU is more accurate and flexible in adjusting to different real-life HL-ADL recognition applications32COMP4434\nRecall (%)Filling pillboxWatch DVDWater plant", "local_index": 35}
{"chunk_id": "Lecture10_Recurrent_Neural_Networks_36", "doc_id": "Lecture10_Recurrent_Neural_Networks", "text": "nt real-life HL-ADL recognition applications32COMP4434\nRecall (%)Filling pillboxWatch DVDWater plantsAnswer the phonePrepare gift cardPrepare soupCleaningChoose outfitS2S_GRU88.679.058.580.892.171.185.058.0HMM33.2***31.6***16.8***22.0***29.9***26.4***20.4***22.5***S2S_LSTM86.1*77.864.374.9*90.8*69.682.549.5**\nRecall (%)RelaxingCoffee timeEarly morningCleanupSandwich timeS2S_GRU61.766.572.182.679.1HMM48.819.5***6.4***17.4***40.9***S2S_LSTM60.054.9**75.983.259.9***\nRecall of S2S_GRU against benchm", "local_index": 36}
{"chunk_id": "Lecture10_Recurrent_Neural_Networks_37", "doc_id": "Lecture10_Recurrent_Neural_Networks", "text": "HMM48.819.5***6.4***17.4***40.9***S2S_LSTM60.054.9**75.983.259.9***\nRecall of S2S_GRU against benchmarks on a wearable/environment motion sensor dataset\nRecall of S2S_GRU against benchmarks on an environment sensor dataset\n*: p-value<0.05, **: p-value<0.01, ***: p-value<0.001\nNew Jersey Institute of Technology\nDevelopment of natural language processing tools\n33COMP4434\nNew Jersey Institute of Technology\nSummary§LSTM and GRU are RNNs that retain past information and update with a gated design. §T", "local_index": 37}
{"chunk_id": "Lecture10_Recurrent_Neural_Networks_38", "doc_id": "Lecture10_Recurrent_Neural_Networks", "text": "nology\nSummary§LSTM and GRU are RNNs that retain past information and update with a gated design. §The “additive” structure avoids vanishing gradient problem§RNNs allow flexible architecture designs to adapt to different sequence learning requirements.§RNNs have broad real-life applications.§Text processing, machine translation, signal extraction/recognition, image captioning§Mobile health analytics, activity of daily living, senior care 34COMP4434\nNew Jersey Institute of Technology\nImportant Re", "local_index": 38}
{"chunk_id": "Lecture10_Recurrent_Neural_Networks_39", "doc_id": "Lecture10_Recurrent_Neural_Networks", "text": "cs, activity of daily living, senior care 34COMP4434\nNew Jersey Institute of Technology\nImportant References§Bengio, Y., Simard, P., & Frasconi, P. (1994). Learning long-term dependencies with gradient descent is difficult.IEEE transactions on neural networks,5(2), 157-166.§Goodfellow, I., Bengio, Y., Courville, A., & Bengio, Y. (2016).Deep learning(Vol. 1). Cambridge: MIT press.§Graves, A. (2012). Supervised sequence labelling with recurrent neural networks. https://www.cs.toronto.edu/~graves/p", "local_index": 39}
{"chunk_id": "Lecture10_Recurrent_Neural_Networks_40", "doc_id": "Lecture10_Recurrent_Neural_Networks", "text": ". Supervised sequence labelling with recurrent neural networks. https://www.cs.toronto.edu/~graves/preprint.pdf§Jozefowicz, R., Zaremba, W., & Sutskever, I. (2015, June). An empirical exploration of recurrent network architectures. InInternational Conference on Machine Learning(pp. 2342-2350).§Lipton, Z. C., Berkowitz, J., & Elkan, C. (2015). A critical review of recurrent neural networks for sequence learning.arXiv preprint arXiv:1506.00019.§Salehinejad, H., Baarbe, J., Sankar, S., Barfett, J.,", "local_index": 40}
{"chunk_id": "Lecture10_Recurrent_Neural_Networks_41", "doc_id": "Lecture10_Recurrent_Neural_Networks", "text": "ence learning.arXiv preprint arXiv:1506.00019.§Salehinejad, H., Baarbe, J., Sankar, S., Barfett, J., Colak, E., & Valaee, S. (2017). Recent Advances in Recurrent Neural Networks.arXiv preprint arXiv:1801.01078. 35COMP4434", "local_index": 41}
{"chunk_id": "Lecture11_MapReduce_0", "doc_id": "Lecture11_MapReduce", "text": "COMP4434 Big Data AnalyticsLecture 11 MapReduceHUANG Xiaoxiaohuang@comp.polyu.edu.hk\n\nNew Jersey Institute of Technology\nMotivation: Google Example§Google§TB of Web Data stored by using Inverted index§TB to PB of Logs §Analysis: find the most popular keywords§Processing cannot be done by one machine efficiently§Needs Parallel Machines\nCOMP4434\nMemory\nDisk\nCPU\n1 Terabyte = 10^12 byte1 Petabyte = 10^15 byte\nNew Jersey Institute of Technology\nMotivation: Google Example§20+ billion web pages x 20KB ", "local_index": 0}
{"chunk_id": "Lecture11_MapReduce_1", "doc_id": "Lecture11_MapReduce", "text": "^15 byte\nNew Jersey Institute of Technology\nMotivation: Google Example§20+ billion web pages x 20KB = 400+ TB§1 computer reads 30-35 MB/sec from disk§~4 months to read the web§~1,000 hard drives to store the web§Takes even more to do something useful with the data!§Today, a standard architecture for such problems is emerging:§Cluster of commodity Linux nodes§Commodity network (ethernet) to connect them\n4COMP4434\nNew Jersey Institute of Technology\nCluster Architecture\n5COMP4434\nMem\nDisk\nCPU Mem\nD", "local_index": 1}
{"chunk_id": "Lecture11_MapReduce_2", "doc_id": "Lecture11_MapReduce", "text": " them\n4COMP4434\nNew Jersey Institute of Technology\nCluster Architecture\n5COMP4434\nMem\nDisk\nCPU Mem\nDisk\nCPU…\nSwitch\nEach rack contains 16-64 nodes\nMem\nDisk\nCPU Mem\nDisk\nCPU…\nSwitch\nSwitch1 Gbps between any pair of nodesin a rack\n2-10 Gbps backbone between racks\n\nNew Jersey Institute of Technology6COMP4434\n\nNew Jersey Institute of Technology\nChallenges in Large-scale Computing§How do you distribute computation?§How can we make it easy to write distributed programs?§Machines fail:§One server may s", "local_index": 2}
{"chunk_id": "Lecture11_MapReduce_3", "doc_id": "Lecture11_MapReduce", "text": " computation?§How can we make it easy to write distributed programs?§Machines fail:§One server may stay up 3 years (1,000 days)§If you have 1,000 servers, expect to loose 1/day§People estimated Google had ~1M machines in 2011§1,000 machines fail every day!\n7COMP4434\nNew Jersey Institute of Technology\nOrigin of MapReduce§MapReduce is a programming model Google has used successfully for processing its “big-data” sets (~ 20000 peta bytes per day)§Users specify the computation in terms of a map and ", "local_index": 3}
{"chunk_id": "Lecture11_MapReduce_4", "doc_id": "Lecture11_MapReduce", "text": "ts “big-data” sets (~ 20000 peta bytes per day)§Users specify the computation in terms of a map and a reduce function§Underlying runtime system automatically parallelizes the computation across large-scale clusters of machines§Underlying system also handles machine failures, efficient communications, and performance issues§Store files multiple times for reliabilityCOMP4434 8\nNew Jersey Institute of Technology\nExample Problem§Given: a massive data collection of documents (100 Terabytes)§Problem: ", "local_index": 4}
{"chunk_id": "Lecture11_MapReduce_5", "doc_id": "Lecture11_MapReduce", "text": "f Technology\nExample Problem§Given: a massive data collection of documents (100 Terabytes)§Problem: Count the frequency of each term in this document collection\nCOMP4434 9\n§Idea: distributed processing, i.e., solve this problem by using multiple computers\nNew Jersey Institute of Technology\nMapReduce Workflow§Map: a mapping that is responsible for dividing the data and transforming the original data into key-value pairs§Shuffle: the process of further organizing and delivering the Map output to t", "local_index": 5}
{"chunk_id": "Lecture11_MapReduce_6", "doc_id": "Lecture11_MapReduce", "text": "a into key-value pairs§Shuffle: the process of further organizing and delivering the Map output to the Reduce§the output of the Map must be sorted and segmented§then passed to the corresponding Reduce§Reduce: a merge that processes the values with the same key and then outputs to the final result\n10COMP4434\nNew Jersey Institute of Technology11COMP4434\nWorker\nWorkerWorkerWorkerWorker\nreadlocalwrite\nRemote read, sort\nOutputFile 0OutputFile 1writeSplit 0Split 1Split 2\nInput Data Output Data\nMapextr", "local_index": 6}
{"chunk_id": "Lecture11_MapReduce_7", "doc_id": "Lecture11_MapReduce", "text": "\nRemote read, sort\nOutputFile 0OutputFile 1writeSplit 0Split 1Split 2\nInput Data Output Data\nMapextract something you care about from each recordReduce aggregate, summarize, filter, or transform\nParallelization: Divide and ConquerPartition CombineIntermediate Data\nNew Jersey Institute of Technology\nMapReduce: The Map Step\n12COMP4434\nvk\nk v\nk vmapvk\nvk\n… k vmap\nInputkey-value pairsIntermediatekey-value pairs\n…k v\nGroupby key\nk v\n…\nk v\nk v v\nv v\nKey-value groups\nNew Jersey Institute of Technology\n", "local_index": 7}
{"chunk_id": "Lecture11_MapReduce_8", "doc_id": "Lecture11_MapReduce", "text": "alue pairs\n…k v\nGroupby key\nk v\n…\nk v\nk v v\nv v\nKey-value groups\nNew Jersey Institute of Technology\nMapReduce: The Reduce Step\n13COMP4434\nreduce\nreducek v\nk v\nk v\n…k v\n…\nk v\nk v v\nv v\nKey-value groupsOutput key-value pairs\nNew Jersey Institute of Technology\nhttp://kickstarthadoop.blogspot.ca/2011/04/word-count-hadoop-map-reduce-example.html\nExample Problem\n14COMP4434\nNew Jersey Institute of Technology\nWord Count Using MapReduce\n15COMP4434\nmap(key, value):// key: document name; value: text of the", "local_index": 8}
{"chunk_id": "Lecture11_MapReduce_9", "doc_id": "Lecture11_MapReduce", "text": "logy\nWord Count Using MapReduce\n15COMP4434\nmap(key, value):// key: document name; value: text of the document for each word w in value:  emit(w, 1)\nreduce(key, values):// key: a word; value: an iterator over counts result = 0 for each count v in values:  result += v emit(key, result)\nNew Jersey Institute of Technology\nMapReduce ProgramMap() {     //to emit, do pre-processingOpen File F.txt;while has lines left {   L = Read a line;   S = tokenize(L);  // S is a list of tokens   for each token in ", "local_index": 9}
{"chunk_id": "Lecture11_MapReduce_10", "doc_id": "Lecture11_MapReduce", "text": "as lines left {   L = Read a line;   S = tokenize(L);  // S is a list of tokens   for each token in S { emit(S[i], 1); // key value pair   }}}Reduce(k, value-list) {// <key, value> with same key will shuffle to the same machine// this Reduce function is invoked once per unique k     append(k, size(value-list)) to file WC.txt;}\nCOMP4434 16\nNew Jersey Institute of Technology\nSystem View<Hello, 1><Kitty, 1>…<girl, 1><fans,1>..<Keroro, 1><girl, 1><fans,1>\nMap() <Hello, 1><Kitty, 1><Keroro,1><girl, [", "local_index": 10}
{"chunk_id": "Lecture11_MapReduce_11", "doc_id": "Lecture11_MapReduce", "text": "tty, 1>…<girl, 1><fans,1>..<Keroro, 1><girl, 1><fans,1>\nMap() <Hello, 1><Kitty, 1><Keroro,1><girl, [1,1]><fans, [1,1]>reduce() reduce() \nreduce() reduce() EmitEmit…Emit..\nEmit\nN1\nN2\nN3 (e.g., capital letter goes here)\nN4 (e.g., small letter goes here)\nreduce() \nshuffling\nCOMP4434 17\nHello Kitty has a lot of girl fans Keroro has two girl fans \nNew Jersey Institute of Technology\nMap-Reduce: A diagram\n18COMP4434\nBig documentMAP:Read input and produces a set of key-value pairs\nGroup by key:Collect a", "local_index": 11}
{"chunk_id": "Lecture11_MapReduce_12", "doc_id": "Lecture11_MapReduce", "text": "m\n18COMP4434\nBig documentMAP:Read input and produces a set of key-value pairs\nGroup by key:Collect all pairs with same key(Hash merge, Shuffle, Sort, Partition)\nReduce:Collect all values belonging to the key and output\nNew Jersey Institute of Technology\nMap-Reduce: In Parallel\n19COMP4434\nAll phases are distributed with many workers doing the tasks\nNew Jersey Institute of Technology\nProgrammer Specifies§Map, Reduce, input files§Workflow:§Read inputs as a set of key-value-pairs§Map transforms inpu", "local_index": 12}
{"chunk_id": "Lecture11_MapReduce_13", "doc_id": "Lecture11_MapReduce", "text": "ifies§Map, Reduce, input files§Workflow:§Read inputs as a set of key-value-pairs§Map transforms input kv-pairs into a new set of k'v'-pairs§Sorts & Shuffles the k'v'-pairs to output nodes§All k'v'- pairs with a given k' are sent to the same reduce§Reduce processes all k'v'-pairs grouped by key into new k''v''-pairs§Write the resulting pairs to files§All phases are distributed with many workers doing the tasks\n20COMP4434\nNew Jersey Institute of Technology\nhttp://kickstarthadoop.blogspot.ca/2011/0", "local_index": 13}
{"chunk_id": "Lecture11_MapReduce_14", "doc_id": "Lecture11_MapReduce", "text": "ng the tasks\n20COMP4434\nNew Jersey Institute of Technology\nhttp://kickstarthadoop.blogspot.ca/2011/04/word-count-hadoop-map-reduce-example.html\nExample Problem Again\n21COMP4434\nNew Jersey Institute of Technology\nCase 1: SQL Execution§Given a 10TB table in HDFS: Student(id,name,year,gpa,gender)§Write a MapReduce program to compute the following query\n22\nSELECT year, AVG(gpa)FROM StudentWHERE gender = ‘Male’GROUP BY year\nCOMP4434\nQuery to look at the average gpa for the male students in each year ", "local_index": 14}
{"chunk_id": "Lecture11_MapReduce_15", "doc_id": "Lecture11_MapReduce", "text": " = ‘Male’GROUP BY year\nCOMP4434\nQuery to look at the average gpa for the male students in each year from table Student.\nNew Jersey Institute of Technology\nMapReduce ProgramMap(String key, Record value) {     // key: table name  // value: table contentfor each row in value { if (row.gender = ‘Male’)            emit(row.year, row.gpa); // intermediate key value pairs}}Reduce(String key, Iterator values) {   // key: year  // values: a list of gpasumGPA = 0;for each gpa in values  sumGPA += gpa; emi", "local_index": 15}
{"chunk_id": "Lecture11_MapReduce_16", "doc_id": "Lecture11_MapReduce", "text": "ues) {   // key: year  // values: a list of gpasumGPA = 0;for each gpa in values  sumGPA += gpa; emit(key, sumGPA/sizeof(values));} 23COMP4434\nNew Jersey Institute of Technology\nMapReduce Programming§Just write the content for Map() and Reduce() function§Don’t even know how the splits are distributed§Hadoop system will find the right block for each split\nMR program\n§Be aware that the large file is split to multiple machines§No need to take care of which machines have the block of the file\nCOMP44", "local_index": 16}
{"chunk_id": "Lecture11_MapReduce_17", "doc_id": "Lecture11_MapReduce", "text": " split to multiple machines§No need to take care of which machines have the block of the file\nCOMP4434 24\nNew Jersey Institute of Technology\nMap-Reduce: Environment§Map-Reduce environment takes care of:§Partitioning the input data§Scheduling the program’s execution across a set of machines§Performing the group by key step§Handling machine failures§Managing required inter-machine communication\nCOMP4434 25\nNew Jersey Institute of Technology\nManage Multiple Workers§Challenges:§Workers may run in an", "local_index": 17}
{"chunk_id": "Lecture11_MapReduce_18", "doc_id": "Lecture11_MapReduce", "text": "4434 25\nNew Jersey Institute of Technology\nManage Multiple Workers§Challenges:§Workers may run in any order§Workers may interrupt each other§Don’t know when workers need to communicate partial results§Workers may access shared data in any order§Solutions:§Do not allow workers to access shared data immediately§Do not allow workers to interrupt each other§When workers finish, perform batch updates§Scheduler tries to schedule map tasks “close” to physical storage location of input data§MapReduce as", "local_index": 18}
{"chunk_id": "Lecture11_MapReduce_19", "doc_id": "Lecture11_MapReduce", "text": "cheduler tries to schedule map tasks “close” to physical storage location of input data§MapReduce assumes an architecture where processors and storage are co-locatedCOMP4434 26\nNew Jersey Institute of Technology\nDealing with Failures§Map worker failure§Map tasks completed or in-progress at worker are reset to idle§Reduce workers are notified when task is rescheduled on another worker§Completed map tasks are re-executed when failure occurs because their output is stored on the local disk(s) of th", "local_index": 19}
{"chunk_id": "Lecture11_MapReduce_20", "doc_id": "Lecture11_MapReduce", "text": " tasks are re-executed when failure occurs because their output is stored on the local disk(s) of the failed machine and therefore inaccessible§Reduce worker failure§Only in-progress tasks are reset to idle §Reduce task is restarted§Completed reduce tasks do not need to be re-executed since their output is stored in a global file system§Master failure§MapReduce task is aborted and client is notified27COMP4434\nNew Jersey Institute of Technology\nWho are using MapReduce?§GoogleØOriginal proprietary", "local_index": 20}
{"chunk_id": "Lecture11_MapReduce_21", "doc_id": "Lecture11_MapReduce", "text": "ed27COMP4434\nNew Jersey Institute of Technology\nWho are using MapReduce?§GoogleØOriginal proprietary implementation§Apache Hadoop MapReduceØMost common (open-source) implementationØBuilt to specs defined by Google§Amazon Elastic MapReduceØUses Hadoop MapReduce running on Amazon EC2§Microsoft Azure HDInsight§or Google Cloud MapReduce for App Engine28COMP4434\n\nNew Jersey Institute of Technology\nHadoop Cluster Architecture\n29\nMaster\nSlave node\nTaskTracker\nDataNode\nJobTrackerNameNode\nSlave node\nTask", "local_index": 21}
{"chunk_id": "Lecture11_MapReduce_22", "doc_id": "Lecture11_MapReduce", "text": "op Cluster Architecture\n29\nMaster\nSlave node\nTaskTracker\nDataNode\nJobTrackerNameNode\nSlave node\nTaskTracker\nDataNode\nSlave node\nTaskTracker\nDataNode\nClient\nHadoop Cluster\nCOMP4434\n§A Hadoop Cluster includes a single master and multiple slave nodes.§Namenode: Central manager for the file system namespace§DataNode: Provide storage for objects§JobTracker: Central manager for running MapReduce jobs§TaskTracker: accept and runs map, reduce and shuffle\nNew Jersey Institute of Technology\nHadoop Distrib", "local_index": 22}
{"chunk_id": "Lecture11_MapReduce_23", "doc_id": "Lecture11_MapReduce", "text": "skTracker: accept and runs map, reduce and shuffle\nNew Jersey Institute of Technology\nHadoop Distributed File System (HDFS)\n30COMP4434\n§HDFS is a block-structured file system: each file will be separated into blocks (typically 64MiB)§Each block of a file is replicated across a number of data nodes to prevent loss of data\nDataNode A DataNode B DataNode CDataNode DDataNode E\nNew Jersey Institute of Technology\nHeartbeat\n31COMP4434\n§Each DataNode sends a Heartbeat message to the NameNode periodicall", "local_index": 23}
{"chunk_id": "Lecture11_MapReduce_24", "doc_id": "Lecture11_MapReduce", "text": "Technology\nHeartbeat\n31COMP4434\n§Each DataNode sends a Heartbeat message to the NameNode periodically (every 3 seconds via a TCP handshake).§The NameNode can detect a dead DataNode by the absence of Heartbeat messages for a period of time (10 minutes by default).\nNew Jersey Institute of Technology\nRe-replicating Missing Replicas\n32COMP4434\n§Missing Heartbeat Messages signify dead DataNode§NameNode will notice and instruct other DataNode to replicate data to new DataNode\nNew Jersey Institute of T", "local_index": 24}
{"chunk_id": "Lecture11_MapReduce_25", "doc_id": "Lecture11_MapReduce", "text": " will notice and instruct other DataNode to replicate data to new DataNode\nNew Jersey Institute of Technology\nMapReduce Execution Overview\nCOMP4434 33\nNew Jersey Institute of Technology\nMapReduce Execution Overview§The MapReduce library in the user program first splits the input files into M pieces of typically 16MB-64MB/piece. It then starts up many copies of the program on a cluster of machines.\n34\n§One of the the copies of the program is special: the master. The rest are workers that are assi", "local_index": 25}
{"chunk_id": "Lecture11_MapReduce_26", "doc_id": "Lecture11_MapReduce", "text": "\n34\n§One of the the copies of the program is special: the master. The rest are workers that are assigned work by the master. There are M map tasks and R reduce tasks to assign. The master picks idle workers and assigns each one either an M or R task.§A worker who is assigned a map task reads the content of the corresponding input split. It parses key/value pairs out of the input data and passes each pair to the users-defined Map function. The intermediate K/V pairs produced are buffered in memor", "local_index": 26}
{"chunk_id": "Lecture11_MapReduce_27", "doc_id": "Lecture11_MapReduce", "text": "ch pair to the users-defined Map function. The intermediate K/V pairs produced are buffered in memory. COMP4434\nNew Jersey Institute of Technology\nMapReduce Execution Overview§Periodically, the buffered pairs are written to local disk, partitioned into R regions by the partitioning function. The locations of these buffered pairs on the local disk are passed back to master, who is responsible for forwarding these locations to the reduce workers.§When a reduce worker gets the location from master,", "local_index": 27}
{"chunk_id": "Lecture11_MapReduce_28", "doc_id": "Lecture11_MapReduce", "text": "orwarding these locations to the reduce workers.§When a reduce worker gets the location from master, it uses remote calls to read the buffered data from the disks. When a reduce worker has read all intermediate data, it sorts it by the intermediate keys so all of the same occurrences are grouped together. (note: if the amount of intermediate data is too large to fit in memory, an external sort is used)\n35COMP4434\nNew Jersey Institute of Technology\nMapReduce Execution Overview§The reduce worker i", "local_index": 28}
{"chunk_id": "Lecture11_MapReduce_29", "doc_id": "Lecture11_MapReduce", "text": "used)\n35COMP4434\nNew Jersey Institute of Technology\nMapReduce Execution Overview§The reduce worker iterates over the sorted intermediate data and for each unique intermediate key encountered, it passes the key and corresponding set of intermediate values to the user’s Reduce function. The output of the Reduce function is appended to a final output file for this reduce partition.§When all map tasks and reduce tasks have been completed, the master wakes up the user program. At this point, the MapR", "local_index": 29}
{"chunk_id": "Lecture11_MapReduce_30", "doc_id": "Lecture11_MapReduce", "text": " and reduce tasks have been completed, the master wakes up the user program. At this point, the MapReduce call in the user program returns back to the user code.§After successful completion, the output of the MapReduce execution is available in the R output files.\n36COMP4434\nNew Jersey Institute of Technology\n§Assume that we have two groups of files, i.e., group A and group B. In each group, each file contains many lines of IDs. We assume that all IDs in group A are different from each other. ID", "local_index": 30}
{"chunk_id": "Lecture11_MapReduce_31", "doc_id": "Lecture11_MapReduce", "text": "file contains many lines of IDs. We assume that all IDs in group A are different from each other. IDs in group B may have repetition.§Our goal is to calculate the set difference between IDs in group A and IDs in group B.§Write brief pseudo-code for Map and Reduce workers, including the (key, value) pairs of the input and output.§Hints: (1) The set difference between two sets A and B means the set that consists of the elements of A which are not elements of B. (2) E.g., group A contains files a1 ", "local_index": 31}
{"chunk_id": "Lecture11_MapReduce_32", "doc_id": "Lecture11_MapReduce", "text": "that consists of the elements of A which are not elements of B. (2) E.g., group A contains files a1 = {ID1, ID2, ID3} and a2 = {ID4, ID5, ID6}. Group B contains files b1 = {ID1, ID5, ID9} and b2 = {ID2, ID4, ID8}. Then, the set difference between A and B is {ID3, ID6}.37COMP4434\nExercise\nNew Jersey Institute of Technology\nMap(key, value) {// key:     file name in group A or B// value: file contentfor each row in file_name:      emit(ID, A); // or emit(ID, B); intermediate KV pairs}Reduce(key, va", "local_index": 32}
{"chunk_id": "Lecture11_MapReduce_33", "doc_id": "Lecture11_MapReduce", "text": "for each row in file_name:      emit(ID, A); // or emit(ID, B); intermediate KV pairs}Reduce(key, values) { // key: IDIf values contain and only contain A:    emit(ID);} 38COMP4434\nSolution 1\nNew Jersey Institute of Technology\nMap(key, value) {// key:     file name in group B and file name in group A// value: content in filesrepetition = {};for each row in file_name_groupB {     if ID == any IDs in set(file_name_groupA):          repetition.append(ID);}emit(file_name_groupA, repetition); // inte", "local_index": 33}
{"chunk_id": "Lecture11_MapReduce_34", "doc_id": "Lecture11_MapReduce", "text": "n set(file_name_groupA):          repetition.append(ID);}emit(file_name_groupA, repetition); // intermediate KV pairs}\n39COMP4434\nSolution 2: Map\nNew Jersey Institute of Technology\nReduce(key, values) { // key: file_name_groupA// values: remain number for each file in group Arepetition_set = {};for ID in values:     if ID not in repetition_set:          repetition_set.append(ID);emit( setdiff(file_name_groupA, repetition_set) );}\n40COMP4434\nSolution 2: Reduce", "local_index": 34}
{"chunk_id": "Lecture11_MapReduce_35", "doc_id": "Lecture11_MapReduce", "text": "_name_groupA, repetition_set) );}\n40COMP4434\nSolution 2: Reduce", "local_index": 35}
{"chunk_id": "Lecture12_PageRank_0", "doc_id": "Lecture12_PageRank", "text": "COMP4434 Big Data AnalyticsLecture 12PageRankHUANG Xiaoxiaohuang@comp.polyu.edu.hk\n\nNew Jersey Institute of Technology\n§First try: Human curated Web directories§Yahoo, DMOZ, LookSmart§Second try: Web search§Information Retrieval investigates:Finding relevant docs in a small and trusted set§Newspaper articles, Patents, etc.§But: Web is huge, full of untrusted documents, random things, web spam, etc.\n3COMP4434\nPageRank Motivation: How to organize the Web?\nNew Jersey Institute of Technology\nChallen", "local_index": 0}
{"chunk_id": "Lecture12_PageRank_1", "doc_id": "Lecture12_PageRank", "text": ".\n3COMP4434\nPageRank Motivation: How to organize the Web?\nNew Jersey Institute of Technology\nChallenges in Web Search§(1) Web contains many sources of informationWho to “trust”?§Trick: Trustworthy pages may point to each other!§(2) What is the “best” answer to query “newspaper”?§No single right answer§Trick: Pages that actually know about newspapers might all be pointing to many newspapers\n4COMP4434\nNew Jersey Institute of Technology\nHint: Web as a Directed Graph\n5COMP4434\n§Nodes: Webpages§Edges", "local_index": 1}
{"chunk_id": "Lecture12_PageRank_2", "doc_id": "Lecture12_PageRank", "text": "34\nNew Jersey Institute of Technology\nHint: Web as a Directed Graph\n5COMP4434\n§Nodes: Webpages§Edges: Hyperlinks\nI teach Big Data inCOMP\nCOMP is in Faculty ofEngineering\nThe Hong Kong Polytechnic University\nNew Jersey Institute of Technology\nWeb as a Directed Graph\n6COMP4434\n\nNew Jersey Institute of Technology\n§All web pages are not equally “important”https://xhuang31.github.io vs. https://www.polyu.edu.hk§There is large diversity in the web-graph node connectivity.Let’s rank the pages by the li", "local_index": 2}
{"chunk_id": "Lecture12_PageRank_3", "doc_id": "Lecture12_PageRank", "text": "yu.edu.hk§There is large diversity in the web-graph node connectivity.Let’s rank the pages by the link structure!\n7COMP4434\nRanking Nodes on the Graph\nNew Jersey Institute of Technology\n§PageRanking§SocialRanking§PaperRanking§ScholarRanking§……\n8COMP4434\nExampleofNodeRanking\nNew Jersey Institute of Technology\n§Page is more important if it has more links§In-coming links? Out-going links?§Think of in-links as votes:§www.stanford.edu has 23,400 in-links§https://xhuang31.github.io has 0 in-link§Are a", "local_index": 3}
{"chunk_id": "Lecture12_PageRank_4", "doc_id": "Lecture12_PageRank", "text": "-links as votes:§www.stanford.edu has 23,400 in-links§https://xhuang31.github.io has 0 in-link§Are all in-links are equal?§Links from important pages count more§Recursive question! \nIdea: Links as votes\n9COMP4434\nNew Jersey Institute of Technology\nGoogle PageRank§In-coming links! Out-going links?§A page with high PageRank value§Many pages pointing to it, or§There are some pages that point to it and have high PageRank values§Example:§Page C has a higher PageRank than Page E, even though it has fe", "local_index": 4}
{"chunk_id": "Lecture12_PageRank_5", "doc_id": "Lecture12_PageRank", "text": "d have high PageRank values§Example:§Page C has a higher PageRank than Page E, even though it has fewer links to it§The link it has is of a much higher value 10COMP4434\n\nNew Jersey Institute of Technology\nIsPage==“Webpage”?\n11COMP4434\nLarry PageCo-founderofGoogle\n§Bornin March 26, 1973§FoundGoogleatSeptember 4, 1998§As of Nov 2024, own an estimated net worth of $163 billion(No.15Richest)§Beginsfrom”Larry Page andSergey Brindeveloped PageRank atStanford Universityin 1996” as part of a research pr", "local_index": 5}
{"chunk_id": "Lecture12_PageRank_6", "doc_id": "Lecture12_PageRank", "text": "m”Larry Page andSergey Brindeveloped PageRank atStanford Universityin 1996” as part of a research project about a new kind of search engine.\nNew Jersey Institute of Technology\n§Each link’s vote is proportional to the importance of its source page§If page j with importance PR(j) has n out-links, each link gets PR(j) / n votes§Page j’s own importance is the sum of the votes on its in-links\nj\n k\ni\nrj/3rj/3rj/3PR(j) = PR(i)/3+PR(k)/4ri/3rk/4\nSimple Recursive Formulation\n12COMP4434\nNew Jersey Institu", "local_index": 6}
{"chunk_id": "Lecture12_PageRank_7", "doc_id": "Lecture12_PageRank", "text": "/3rj/3rj/3PR(j) = PR(i)/3+PR(k)/4ri/3rk/4\nSimple Recursive Formulation\n12COMP4434\nNew Jersey Institute of Technology\nHow to Represent a Graph§Graph model !=#,%§!\tis a set of pages§#\tis a set of edges§Each edge $,&\t∈#\trepresents that page $\tpoints/references to page &§Adjacent List §A data structure for a graph§()*$=&:$,&∈#\tcontains each vertex & being adjacent to $§Example: ()*2 = {3, 4} 13COMP4434\n\nNew Jersey Institute of Technology\nPageRank: The “Flow” Model§A “vote” from an important page is ", "local_index": 7}
{"chunk_id": "Lecture12_PageRank_8", "doc_id": "Lecture12_PageRank", "text": "4\n\nNew Jersey Institute of Technology\nPageRank: The “Flow” Model§A “vote” from an important page is worth more§A page is important if it is pointed to by other important pages§Define a “rank” rj for page j\nå®=ji ij rr id\n y\nm\naa/2y/2a/2m\ny/2\n“Flow” equations:ry  = ry /2 + ra /2ra  = ry /2 + rmrm = ra /2!! … out-degree of node \"\nCOMP4434 14\nNew Jersey Institute of Technology\n§3 equations, 3 unknowns, no constants§No unique solution§All solutions equivalent modulo the scale factor§Additional const", "local_index": 8}
{"chunk_id": "Lecture12_PageRank_9", "doc_id": "Lecture12_PageRank", "text": "s, no constants§No unique solution§All solutions equivalent modulo the scale factor§Additional constraint forces uniqueness:§.!\t+.\"+\t.#\t=\t0§Solution: .!\t=$%,.\"\t=$%,.#\t=&%§But, we need a better method for large web-size graphs\nry  = ry /2 + ra /2ra  = ry /2 + rmrm = ra /2\nFlow equations:\nSolving the Flow Equations\nCOMP4434 15\nNew Jersey Institute of Technology\nPageRank: Matrix Formulation§Stochastic adjacency matrix 1§Let page 2 has )! out-links§If 2\t→*, then  !!\"\t=!\"\"    else   !!\"\t=\t0§! is a co", "local_index": 9}
{"chunk_id": "Lecture12_PageRank_10", "doc_id": "Lecture12_PageRank", "text": "c adjacency matrix 1§Let page 2 has )! out-links§If 2\t→*, then  !!\"\t=!\"\"    else   !!\"\t=\t0§! is a column stochastic matrix§Columns sum to 1§Rank vector .: vector with an entry per page§4! is the importance score of page 2§∑'4'=1§The flow equations can be written    !\t=\t$⋅\t! 16COMP4434\nå\n®\n=\nji\ni\nj\nr\nr\ni\nd\nNew Jersey Institute of Technology\n§Remember the flow equation:§Flow equation in the matrix form1⋅\t.=.§Suppose page i links to 3 pages, including j\nj i\nM r r=rj\n1/3 å\n®\n=\nji\ni\nj\nr\nr\ni\nd\nri.. =\n", "local_index": 10}
{"chunk_id": "Lecture12_PageRank_11", "doc_id": "Lecture12_PageRank", "text": "orm1⋅\t.=.§Suppose page i links to 3 pages, including j\nj i\nM r r=rj\n1/3 å\n®\n=\nji\ni\nj\nr\nr\ni\nd\nri.. =\nExample\nCOMP4434 17\nj\n k\ni\nrj/3rj/3rj/3\nri/3rk/4\nNew Jersey Institute of Technology\nExample: Flow Equations & M\n18COMP4434\nr = M·ry       ½    ½    0     y a   =  ½     0    1     a m       0    ½    0    m\ny\na\n m\nyamy½½0a½01m0½0\nry  = ry /2 + ra /2ra  = ry /2 + rmrm = ra /2\nNew Jersey Institute of Technology\nPower Iteration Method§Given a web graph with N nodes, where the nodes are pages and edge", "local_index": 11}
{"chunk_id": "Lecture12_PageRank_12", "doc_id": "Lecture12_PageRank", "text": "Technology\nPower Iteration Method§Given a web graph with N nodes, where the nodes are pages and edges are hyperlinks§Power iteration: a simple iterative scheme§Suppose there are N web pages§Initialize: r(0) = [1/N,….,1/N]T§Iterate: r(t+1) = M · r(t)§Stop when |r(t+1) – r(t)|1 < e\n19COMP4434\nå®+=jititj rr i)()1( ddi …. out-degree of node i\n|x|1 = å1≤i≤N|xi| is the L1 norm Can use any other vector norm, e.g., Euclidean\nNew Jersey Institute of Technology\nPageRank: How to solve?§Power Iteration:§Set", "local_index": 12}
{"chunk_id": "Lecture12_PageRank_13", "doc_id": "Lecture12_PageRank", "text": "orm, e.g., Euclidean\nNew Jersey Institute of Technology\nPageRank: How to solve?§Power Iteration:§Set 4(=1/N§1: 4′(=∑'→(*#+#§2:\t4=4′§Go to 1§Example: ry  1/3 1/3 5/12 9/24  6/15 ra = 1/3 3/6 1/3 11/24 … 6/15 rm  1/3 1/6 3/12 1/6  3/15\ny\na\n m\ny a my½½0a½01m0½0\nIteration 0, 1, 2, …\nry  = ry /2 + ra /2ra  = ry /2 + rmrm = ra /2\nCOMP4434 20\nNew Jersey Institute of Technology\nWhy Power Iteration works? (1)§Power iteration: A method for finding dominant eigenvector (the vector corresponding to the larg", "local_index": 13}
{"chunk_id": "Lecture12_PageRank_14", "doc_id": "Lecture12_PageRank", "text": "(1)§Power iteration: A method for finding dominant eigenvector (the vector corresponding to the largest eigenvalue)§.(&)=1⋅.(.)§.($)=1⋅.&=11..=1$⋅..§.(/)=1⋅.$=11$..=1/⋅..§Claim: Sequence 1⋅..,1$⋅..,…10⋅..,… approaches the dominant eigenvector of 1(1isstochastic/Markovmatrix)§ \nDetails!\nCOMP4434 21\nNOTE: x is an eigenvector with the corresponding eigenvalue λ if:!\"=$\"Optimal r is the first or principal eigenvector of M, with corresponding eigenvalue 1\nNew Jersey Institute of Technology\nWhy Power ", "local_index": 14}
{"chunk_id": "Lecture12_PageRank_15", "doc_id": "Lecture12_PageRank", "text": "ipal eigenvector of M, with corresponding eigenvalue 1\nNew Jersey Institute of Technology\nWhy Power Iteration works? (2)§Claim: Sequence 1⋅..,1$⋅..,…10⋅..,… approaches the dominant eigenvector of 1§Proof:§Assume M has n linearly independent eigenvectors, :1,:2,…,:3 with corresponding eigenvalues ;1,;2,…,;3, where ;1>;2>⋯>;3§Vectors :1,:2,…,:3 form a basis and thus we can write: 4(4)=>1\t:1+>2\t:2+⋯+>3\t:3§1.(.)=1?&\t@&+?$\t@$+⋯+?5\t@5                =>1(B:1)+>2(B:2)+⋯+>3(B:3)                =>1(;1:1)+", "local_index": 15}
{"chunk_id": "Lecture12_PageRank_16", "doc_id": "Lecture12_PageRank", "text": ">3\t:3§1.(.)=1?&\t@&+?$\t@$+⋯+?5\t@5                =>1(B:1)+>2(B:2)+⋯+>3(B:3)                =>1(;1:1)+>2(;2:2)+⋯+>3(;3:3)§Repeated multiplication on both sides produces    B64(4)=>1(;16:1)+>2(;26:2)+⋯+>3(;36:3)\nDetails!\nCOMP4434 22\nNOTE: x is an eigenvector with the corresponding eigenvalue λ if:$%='%\nNew Jersey Institute of Technology\nWhy Power Iteration works? (3)§Claim: Sequence !⋅##,!$⋅##,…!%⋅##,… approaches the dominant eigenvector of !§Proof (continued):§Repeated multiplication on both sides", "local_index": 16}
{"chunk_id": "Lecture12_PageRank_17", "doc_id": "Lecture12_PageRank", "text": ",… approaches the dominant eigenvector of !§Proof (continued):§Repeated multiplication on both sides produces  &&'(()=)*(+*&,*)+)+(++&,+)+⋯+),(+,&,,)§&&'(()=+*&)*,*+)+-!-\"&,++⋯+),-#-\"&,,§Since +*\t>++ then fractions -!-\",-$-\"…<1 and so -%-\"&=0 as 5→∞  (for all 8=2…:).§Thus: !#\"(%)≈$'%'#&'§Note if !(=0 then the method won’t converge§The largest eigenvalue of a stochastic matrix is always 1.\nDetails!\nCOMP4434 23\nNew Jersey Institute of Technology\nPageRank: Three Questions\n§Does this converge?§Does ", "local_index": 17}
{"chunk_id": "Lecture12_PageRank_18", "doc_id": "Lecture12_PageRank", "text": "\nCOMP4434 23\nNew Jersey Institute of Technology\nPageRank: Three Questions\n§Does this converge?§Does it converge to what we want?§Are results reasonable?\nå\n®\n+\n=\nji\nt\ni\nt\nj\nr\nr\ni\n)(\n)1(\nd\nMrr=or equivalently\nCOMP4434 24\nNew Jersey Institute of Technology\nDoes this converge?\n§Example:  ra  1 0 1 0   rb  0 1 0 1=\nb\na\nIteration 0, 1, 2, …\nå\n®\n+\n=\nji\nt\nit\nj\nr\nr\ni\n)(\n)1(\nd\nCOMP4434 25\nNew Jersey Institute of Technology\nDoes it converge to what we want?\n§Example:  ra  1 0 0 0   rb  0 1 0 0=\nb\na\nIterati", "local_index": 18}
{"chunk_id": "Lecture12_PageRank_19", "doc_id": "Lecture12_PageRank", "text": "te of Technology\nDoes it converge to what we want?\n§Example:  ra  1 0 0 0   rb  0 1 0 0=\nb\na\nIteration 0, 1, 2, …\nå\n®\n+\n=\nji\nt\nit\nj\nr\nr\ni\n)(\n)1(\nd\nCOMP4434 26\nNew Jersey Institute of Technology\nPageRank: Problems2 problems:§(1) Some pages are dead ends (have no out-links)§“Vote” has “nowhere” to go to§Such pages cause importance to “leak out”§(2) Spider traps: (all out-links are within the group)§“Vote” gets “stuck” in a trap§And eventually spider traps absorb all importance\nDead end\nSpider trap", "local_index": 19}
{"chunk_id": "Lecture12_PageRank_20", "doc_id": "Lecture12_PageRank", "text": "“Vote” gets “stuck” in a trap§And eventually spider traps absorb all importance\nDead end\nSpider trap\nCOMP4434 27\nNew Jersey Institute of Technology\nProblem: Spider Traps§Power Iteration:§Set 4(=1§4(=∑'→(*#+#§And iterate\n§Example: ry  1/3 2/6 3/12 5/24  0 ra = 1/3 1/6 2/12 3/24 … 0 rm  1/3 3/6 7/12 16/24  1Iteration 0, 1, 2, …\ny\na\n m\ny a my½½0a½00m0½1ry  = ry /2 + ra /2ra  = ry /2rm = ra /2 + rm\nm is a spider trap\nAll the PageRank score gets “trapped” in node m.28\nNew Jersey Institute of Technolo", "local_index": 20}
{"chunk_id": "Lecture12_PageRank_21", "doc_id": "Lecture12_PageRank", "text": "is a spider trap\nAll the PageRank score gets “trapped” in node m.28\nNew Jersey Institute of Technology\nSolution: Teleports!§The Google solution for spider traps: At each time step, the “vote” has two options§With prob. b, follow a link at random§With prob. 1-b, jump to some random page§Common values for b  are in the range 0.8 to 0.9§“Vote” will teleport out of spider trap within a few time steps\n29COMP4434\ny\na\n m\n y\na\n m\nNew Jersey Institute of Technology\nProblem: Dead Ends§Power Iteration:§Set", "local_index": 21}
{"chunk_id": "Lecture12_PageRank_22", "doc_id": "Lecture12_PageRank", "text": "29COMP4434\ny\na\n m\n y\na\n m\nNew Jersey Institute of Technology\nProblem: Dead Ends§Power Iteration:§Set 4(=1§4(=∑'→(*#+#§And iterate\n§Example: ry  1/3 2/6 3/12 5/24  0 ra = 1/3 1/6 2/12 3/24 … 0 rm  1/3 1/6 1/12 2/24  0\ny\na\n m\ny a my½½0a½00m0½0ry  = ry /2 + ra /2ra  = ry /2rm = ra /2\nHere the PageRank “leaks” out since the matrix is not stochastic.Iteration 0, 1, 2, …30\nNew Jersey Institute of Technology\nSolution: Always Teleport!§Teleports: Follow random teleport links with probability 1.0 from de", "local_index": 22}
{"chunk_id": "Lecture12_PageRank_23", "doc_id": "Lecture12_PageRank", "text": "logy\nSolution: Always Teleport!§Teleports: Follow random teleport links with probability 1.0 from dead-ends§Adjust matrix accordingly\ny\na\n m y a my½½⅓a½0⅓m0½⅓\ny a my½½0a½00m0½0\ny\na\n m\nCOMP4434 31\nNew Jersey Institute of Technology\nWhy Teleports Solve the Problem?§Spider-traps are not a problem, but with traps PageRank scores are not what we want§Solution: Never get stuck in a spider trap by teleporting out of it in a finite number of steps§Dead-ends are a problem§The matrix is not column stochas", "local_index": 23}
{"chunk_id": "Lecture12_PageRank_24", "doc_id": "Lecture12_PageRank", "text": "rting out of it in a finite number of steps§Dead-ends are a problem§The matrix is not column stochastic so our initial assumptions are not met§Solution: Make matrix column stochastic by always teleporting when there is nowhere else to go\n32COMP4434\nNew Jersey Institute of Technology\nSolution: Random Teleports§Google’s solution that does it all:At each step, random surfer has two options:§With probability b,  follow a link at random§With probability 1-b, jump to some random page§PageRank equation", "local_index": 24}
{"chunk_id": "Lecture12_PageRank_25", "doc_id": "Lecture12_PageRank", "text": "ability b,  follow a link at random§With probability 1-b, jump to some random page§PageRank equation [Larry Page and Sergey Brin 1998]!!=#\"→!$!\"%\"+(1−$)1+di … out-degree of node i\nThis formulation assumes that . has no dead ends.  We can either preprocess matrix . to remove all dead ends or explicitly follow random teleport links with probability 1.0 from dead-ends.COMP4434 33\nNew Jersey Institute of Technology\nThe Google Matrix§PageRank equation [Brin-Page, ‘98]4(=D'→(E4')'+(1−E)1G§The Google M", "local_index": 25}
{"chunk_id": "Lecture12_PageRank_26", "doc_id": "Lecture12_PageRank", "text": " of Technology\nThe Google Matrix§PageRank equation [Brin-Page, ‘98]4(=D'→(E4')'+(1−E)1G§The Google Matrix A:(=E\tB+1−E1G7×7§We have a recursive problem: .=H⋅. And the Power method still works!§What is b ?§In practice b =0.8 ~ 0.9 (make 5 steps on avg., jump)\n[1/N]NxN…N by N matrixwhere all entries are 1/N\nCOMP4434 34\nNew Jersey Institute of Technology\nRandom Teleports (b = 0.8)\n35COMP4434\nya    =m 1/31/31/30.330.200.460.240.200.520.260.180.567/33  5/3321/33. . .\ny\na\n m7/15\n7/151/15\n13/15\n1/151/15", "local_index": 26}
{"chunk_id": "Lecture12_PageRank_27", "doc_id": "Lecture12_PageRank", "text": "1/31/31/30.330.200.460.240.200.520.260.180.567/33  5/3321/33. . .\ny\na\n m7/15\n7/151/15\n13/15\n1/151/15\n1/15\n7/15\n7/15\n1/2 1/2   0     1/2   0    0      0   1/2   11/3 1/3 1/3   1/3 1/3 1/3   1/3 1/3 1/3y   7/15  7/15   1/15a   7/15  1/15   1/15m  1/15  7/15  13/15\n0.8 + 0.2M [1/N]NxN\nA\nNew Jersey Institute of Technology\nMapReduce Program for PageRankMap(key, value) {     // key: a page,  // value: page rank of the pageFor each page in Adj[key] emit(page, PR(key)/sizeof(Adj[key]);}Reduce(key, value", "local_index": 27}
{"chunk_id": "Lecture12_PageRank_28", "doc_id": "Lecture12_PageRank", "text": "age rank of the pageFor each page in Adj[key] emit(page, PR(key)/sizeof(Adj[key]);}Reduce(key, values) {   // key: a page,  // values: a list of page ranks from all its incoming pagesPR(key)=1-b;For each pagerank in values PR(key) = PR(key) + b*pagerank;emit(key, PR(key));} 36COMP4434\nNew Jersey Institute of Technology\nMapReduce Program for PageRank\n37COMP4434\nA B C DB A DC A BD B C MapB 1/3C 1/3D 1/3\nLinks.txt\nA 1B 1C 1D 1Initial PRA 1/2D 1/2\nA 1/2B 1/2\nB 1/2C 1/2\nReduceA 1/2A 1/2B 1/3B 1/2B 1/", "local_index": 28}
{"chunk_id": "Lecture12_PageRank_29", "doc_id": "Lecture12_PageRank", "text": " 1/3\nLinks.txt\nA 1B 1C 1D 1Initial PRA 1/2D 1/2\nA 1/2B 1/2\nB 1/2C 1/2\nReduceA 1/2A 1/2B 1/3B 1/2B 1/2C 1/3C 1/2\nD 1/3D 1/2\nNew Jersey Institute of Technology\nWeb Search Engines\n38COMP4434\n§Indexer§Processtheretrievedpages/documentsandrepresents theminefficientsearchdatastructures(invertedfiles)§QueryServer§Acceptthe queryfromtheuserandreturnthe result pagesbyconsultingthesearchdatastructure", "local_index": 29}
{"chunk_id": "Lecture1_Introduction_0", "doc_id": "Lecture1_Introduction", "text": "COMP4434 Big Data AnalyticsLecture 1 Introductionto Big Data AnalyticsHUANG Xiaoxiaohuang@comp.polyu.edu.hk\n\nNew Jersey Institute of Technology\nArrangement§Prerequisites:§Basic statistics, probability, linear algebra§Basic Computer Science fundamentals§programming (Python)§data structures§algorithms§database systems\n§26 hours Lectures + 13 hours Lab§Five lab sessions\n2COMP4434\nNew Jersey Institute of Technology\nWhat is big data?§Posts on social media sites§Purchase transaction records§Digital pi", "local_index": 0}
{"chunk_id": "Lecture1_Introduction_1", "doc_id": "Lecture1_Introduction", "text": " of Technology\nWhat is big data?§Posts on social media sites§Purchase transaction records§Digital pictures and videos§Software logs§Microphone & camera recordings§Cell phone GPS signals§Sensing data§Scans of government documents§Traffic data§...\nCOMP4434 3\n\nNew Jersey Institute of Technology\nDefinition of big data\nCOMP4434 4\n§Definition 1: a combination of structured, semi-structured and unstructured data collected by organizations that can be mined for information and used in machine learning p", "local_index": 1}
{"chunk_id": "Lecture1_Introduction_2", "doc_id": "Lecture1_Introduction", "text": "red data collected by organizations that can be mined for information and used in machine learning projects, predictive modeling and other advanced analytics applications.§Definition 2: “Big data is high-volume, high-velocity and high-variety information assets that demand cost-effective, innovative forms of information processing for enhanced insight and decision making.” -- Gartner §Characteristicsofbigdata (4Vs)§Volume, Velocity, Variety, Veracity\nNew Jersey Institute of Technology5\n§Volume, ", "local_index": 2}
{"chunk_id": "Lecture1_Introduction_3", "doc_id": "Lecture1_Introduction", "text": "icsofbigdata (4Vs)§Volume, Velocity, Variety, Veracity\nNew Jersey Institute of Technology5\n§Volume, Velocity, Variety, VeracityCharacteristics of big data (4Vs)\nNew Jersey Institute of Technology\nVolume: scale of data\nCOMP4434 6\n§Data volume is increasing exponentially§Generated by huge number of devices and sensors§The number of smartphone mobile network subscriptions worldwide reached almost 6.4 billion in 20222,9582,5142,0002,000\n1,3091,051931715700635626584574556445\nFacebookYouTubeWhatsApp*I", "local_index": 3}
{"chunk_id": "Lecture1_Introduction_4", "doc_id": "Lecture1_Introduction", "text": " billion in 20222,9582,5142,0002,000\n1,3091,051931715700635626584574556445\nFacebookYouTubeWhatsApp*In stagramWeChatTikTokFacebook…Douyin**TelegramSnapchatKuaishouSina WeiboQQTwitterPinterest\nNumber of active users in millions\nMost popular social networks worldwide as of January 2023, ranked by number of monthly active users (in millions)\nNew Jersey Institute of Technology\nVelocity: speed of data generation\n7\n§Data is generated fast §Data need to be processed fast §Online Data Analytics: late dec", "local_index": 4}
{"chunk_id": "Lecture1_Introduction_5", "doc_id": "Lecture1_Introduction", "text": "eneration\n7\n§Data is generated fast §Data need to be processed fast §Online Data Analytics: late decisions mean missing opportunities §e.g., 1: Based on your current location and your purchase history, send promotions right now for store next to you §e.g., 2: Sensors monitoring your activities and body, notify you if there are abnormal measurementsAmount per minuteEmails sent 231400000Cryptocurrency purchased (USD)90200000Texts sent 16000000Searches conducted on Google5900000Snaps shared on Snap", "local_index": 5}
{"chunk_id": "Lecture1_Introduction_6", "doc_id": "Lecture1_Introduction", "text": "cy purchased (USD)90200000Texts sent 16000000Searches conducted on Google5900000Snaps shared on Snapchat 2430000Pieces of content shared on Facebook1700000Swipes on Tinder 1100000Hours streamed 1000000USD spent on Amazon 443000USD sent on Venmo 437600Tweets shared on Twitter 347200Hours spent in Zoom meetings104600USD spent on DoorDash 76400\nMedia usage in an internet minute as of April 2022\nNew Jersey Institute of Technology\nVariety: data in many forms\nCOMP4434 8\n§A single application may gener", "local_index": 6}
{"chunk_id": "Lecture1_Introduction_7", "doc_id": "Lecture1_Introduction", "text": "ersey Institute of Technology\nVariety: data in many forms\nCOMP4434 8\n§A single application may generate/collect many types of data, e.g., types of data are stored in emails §Tabular data: attributes like subject, to, from§Text (in email body)§Image (in attachment)§Hyperlinks§Types of data §Relational Data (e.g., Tables)§Text Data (e.g., comments)§Semi-structured Data (e.g., XML)§Graph Data (e.g., social network)§What else?\nNew Jersey Institute of Technology\nData in Many Forms\nCOMP4434 9\nTextSign", "local_index": 7}
{"chunk_id": "Lecture1_Introduction_8", "doc_id": "Lecture1_Introduction", "text": "social network)§What else?\nNew Jersey Institute of Technology\nData in Many Forms\nCOMP4434 9\nTextSignal (Voice, Audio)ImageGraph\nJohn likes to watch movies. Mary likes movies too.\nTuple𝑥!1John also likes to watch football games.2John likes to watch movies. Mary likes movies too.\n1252002251051502551575175\n𝑨1234101.24.3021.200.8034.30.802.64002.60\nsignal1signal213.587.2412.1112.5013.498.6611.2510.9814.5713.7513.229.02\n𝑬Node1Node2Weight11 2 1.221 3 4.332 3 0.843 4 2.6\n12 43\nNew Jersey Institute of T", "local_index": 8}
{"chunk_id": "Lecture1_Introduction_9", "doc_id": "Lecture1_Introduction", "text": "4.5713.7513.229.02\n𝑬Node1Node2Weight11 2 1.221 3 4.332 3 0.843 4 2.6\n12 43\nNew Jersey Institute of Technology\nVeracity: uncertainty of data\nCOMP4434 10\n§Is the data accurate?§Measurement error§Human errors like typos in names/addresses§Does the data come from a reliable source?§What if data from different sources are not consistent?\nFake, Paid-For Reviews in Amazon\nNew Jersey Institute of Technology\nApplications of big data in AI§Artificial Intelligence: the theory and development of computer sy", "local_index": 9}
{"chunk_id": "Lecture1_Introduction_10", "doc_id": "Lecture1_Introduction", "text": "gy\nApplications of big data in AI§Artificial Intelligence: the theory and development of computer systems able to perform tasks normally requiring human intelligence§Before the Age of “big data”\n§Big data has changed AI: “AI Would Be Nothing Without Big Data”§“Data is the new oil”11COMP4434\n§ELIZA is an early natural language processing computer program created from 1964 to 1967 at MIT§On May 11, 1997, an IBM computer called IBM Deep Blue beat the world chess champion after a six-game match\n\nNew", "local_index": 10}
{"chunk_id": "Lecture1_Introduction_11", "doc_id": "Lecture1_Introduction", "text": "1997, an IBM computer called IBM Deep Blue beat the world chess champion after a six-game match\n\nNew Jersey Institute of Technology\nBig data and the development of AI\n12COMP4434\nNew Jersey Institute of Technology\nRecent breakthroughs in AI\n13COMP4434\n§At the 2017 Future of Go Summit, the Master version of AlphaGo beat Ke Jie, the number one ranked player in the world at the time, in a three-game match, after which AlphaGo was awarded professional 9-dan by the Chinese Weiqi Association\nNew Jersey", "local_index": 11}
{"chunk_id": "Lecture1_Introduction_12", "doc_id": "Lecture1_Introduction", "text": "atch, after which AlphaGo was awarded professional 9-dan by the Chinese Weiqi Association\nNew Jersey Institute of Technology\nApplication 1: Recommender system§Recommendation§Ex: Amazon, YouTube, Netflix, ……§What item for what people?§How to improve users’ satisfaction?\nCOMP4434 14\n\nNew Jersey Institute of Technology\nApplication: Recommender system§More recommender systems:§News feed§Music feed§Twitter feed\n15\n\nNew Jersey Institute of Technology\nApplication: Web search§Web search§Image search§Vir", "local_index": 12}
{"chunk_id": "Lecture1_Introduction_13", "doc_id": "Lecture1_Introduction", "text": "tter feed\n15\n\nNew Jersey Institute of Technology\nApplication: Web search§Web search§Image search§Virtual assistant§High-frequency trading\n16COMP4434\n\nNew Jersey Institute of Technology\nApplication: Chatbot (e.g., ChatGPT)§Write essays: https://youtu.be/oLjZva6JvLI§Programming:https://youtu.be/TIDA6pvjEE0https://youtu.be/B3yuK2XHmvM§Conversation:https://youtu.be/GYeJC31JcM0\n17\n\nNew Jersey Institute of Technology\nApplication: Disease treatment§Disease Treatment: Joint research between Google DeepM", "local_index": 13}
{"chunk_id": "Lecture1_Introduction_14", "doc_id": "Lecture1_Introduction", "text": " of Technology\nApplication: Disease treatment§Disease Treatment: Joint research between Google DeepMind and Moorfields Eye Hospital§Eyecare professionals diagnose eye conditions by using optical coherence tomography (OCT) scans (over 1,000 a day at Moorfields alone)§Achieving expert error rate 5.5% comparably to the two best retina specialists (6.7% and 6.8% error rate)\nCOMP4434 18\n\nNew Jersey Institute of Technology\nApplication: Autonomous driving§https://www.notateslaapp.com/news/1579/musk-liv", "local_index": 14}
{"chunk_id": "Lecture1_Introduction_15", "doc_id": "Lecture1_Introduction", "text": "titute of Technology\nApplication: Autonomous driving§https://www.notateslaapp.com/news/1579/musk-live-steams-fsd-v12-and-it-s-too-human-why-that-s-a-problem-video\n19\n\nNew Jersey Institute of Technology20\nApplication: Drug design (e.g., AlphaFold)\n§A solution to a 50-year-old grand challenge in biology\n\nNew Jersey Institute of Technology\nAn example in drug design\n21COMP4434\nNew Jersey Institute of Technology\nWe can do more\n22COMP4434\nNew Jersey Institute of Technology\nBig Data Analysis Procedure\n", "local_index": 15}
{"chunk_id": "Lecture1_Introduction_16", "doc_id": "Lecture1_Introduction", "text": "Technology\nWe can do more\n22COMP4434\nNew Jersey Institute of Technology\nBig Data Analysis Procedure\n23\n\nNew Jersey Institute of Technology\nRelations among big data analytics and AI\nCOMP4434 24\nNew Jersey Institute of Technology\nBig\tData\tAnalyticsCharacteristics\tof\tbig\tdataVolumeVelocityVariety\t(tabular,\ttext,\tVeracitytime-series,\timage,\tgraph)Applications:\tAI\twith\tbig\tdataRecommender\tsystemWeb\tsearch\nPageRankPageRank\t+\tMapReduceContent-based\tRecommendation\nAutonomous\tdrivingAlphaGoAlphaFold\t2Cha", "local_index": 16}
{"chunk_id": "Lecture1_Introduction_17", "doc_id": "Lecture1_Introduction", "text": "rch\nPageRankPageRank\t+\tMapReduceContent-based\tRecommendation\nAutonomous\tdrivingAlphaGoAlphaFold\t2ChatGPT\nFacial\trecognition\nFactorization(SVD)\nBasic\tstatistical\tanalysis\nGradient\tdescentSingular\tvalue\tdecomposition\t(SVD)\nBackpropagationRecurrent\tneural\tnetwork(text,\ttime-series)\nMachine\tlearning\nClustering:\tK-meansDimensionality\treduction\t(autoencoder,\tSVD)\nDeep\tlearning\nConvolutional\tneural\tnetwork(image)\nSupport\tvector\tmachineLogistic\tregression\tclassifier\nUnsupervised\tlearning\nF1\tscore,\tpreci", "local_index": 17}
{"chunk_id": "Lecture1_Introduction_18", "doc_id": "Lecture1_Introduction", "text": "rk(image)\nSupport\tvector\tmachineLogistic\tregression\tclassifier\nUnsupervised\tlearning\nF1\tscore,\tprecision,\trecall(Evaluation\tmetrics)Overfitting\n\t&\tcross\tvalidationMultilayer\tperceptron(autoencoder)\nLarge-scale\tdata\tanalytics\tsystems\nMapReduceHadoop\nAdjacency\tmatrix\nGraph\tanalytics\nUser-item\tinteraction\tnetwork Collaborative\tfiltering\nLinear\tregression\nSupervised\tlearning", "local_index": 18}
{"chunk_id": "Lecture2_Basic_Statistical_Analysis_0", "doc_id": "Lecture2_Basic_Statistical_Analysis", "text": "COMP4434 Big Data AnalyticsLecture 2 Basic Statistical AnalysisHUANG Xiaoxiaohuang@comp.polyu.edu.hk\n\nNew Jersey Institute of Technology\nLinear Regression§A Regression Problem-Line Fitting§E.g., “My GPA is 2.9, what will be my salary?”\n3COMP4434\nGPA(!) 1st Salary (\")3.5 200002.1 10000… …\n\nNew Jersey Institute of Technology\nNotations§!\t: input variables/attributes/features§#\t: output variable/attribute/target variable§$\t: number of training examples§%\t: number of input variables§Univariate: !=1§M", "local_index": 0}
{"chunk_id": "Lecture2_Basic_Statistical_Analysis_1", "doc_id": "Lecture2_Basic_Statistical_Analysis", "text": "bute/target variable§$\t: number of training examples§%\t: number of input variables§Univariate: !=1§Multivariate: !>1\n%\n!=1\n4\nGPA(!) 1st Salary (\")3.5200002.110000… …\nCOMP4434\nNew Jersey Institute of Technology\nModel !!\"=$\"+$#\"\nTraining Set&(\"), '(\")\t\nLearning Algorithm\nHypothesis (Model)\nGPA (&) EstimatedSalary (')\nHow do we represent ℎ?\n5COMP4434\nℎ$&=*%+*&&\nNew Jersey Institute of Technology\nHow to Set $\n✓ 0 =1\n✓ 1 =0\n✓ 0 =0\n✓ 1 =0 . 5\n✓ 0 =1\n✓ 1 =1\n0123\n01230123\n0123 0123\n0123\nh ✓ ( x ) h ✓ ( ", "local_index": 1}
{"chunk_id": "Lecture2_Basic_Statistical_Analysis_2", "doc_id": "Lecture2_Basic_Statistical_Analysis", "text": "to Set $\n✓ 0 =1\n✓ 1 =0\n✓ 0 =0\n✓ 1 =0 . 5\n✓ 0 =1\n✓ 1 =1\n0123\n01230123\n0123 0123\n0123\nh ✓ ( x ) h ✓ ( x ) h ✓ ( x )\nx x x\n6COMP4434\nNew Jersey Institute of Technology\nWhat is the Best Fitting Line?§Finding &, which makes ℎ#! closest to # for all training data !(%),#(%)§Mathematical definition: Cost Function )(+',+()\nError: (Estimated – Actual)Squared Error: to be positive\n7COMP4434\nmin#!,#\"12$2%*+\n,ℎ#(!(%))−#(%)-\nNew Jersey Institute of Technology\nCost Function &($\",$#)\n§Start with some &.,&+ (e.g", "local_index": 2}
{"chunk_id": "Lecture2_Basic_Statistical_Analysis_3", "doc_id": "Lecture2_Basic_Statistical_Analysis", "text": "ℎ#(!(%))−#(%)-\nNew Jersey Institute of Technology\nCost Function &($\",$#)\n§Start with some &.,&+ (e.g., &.=0, &+=0)§Keep refining &.,&+\tto reduce 6(&.,&+)\tuntil we hopefully end up at a minimum8COMP4434\n'(!,(\"=min#!,#\"1201%&\"\n'ℎ#(3(%))−5(%)*\n§Input: (!(+), #(+)), …, (!(,), #(,))\nOur Target\nNew Jersey Institute of Technology\nGradient Descent Algorithm\n9COMP4434\nRepeat until convergence {&.=&.−7\t896&.,&+9&.&+=&+−7\t896&.,&+9&+}\nDerivativeSlopeGradient\nLearning Rate(or Step Size) Three Problems:1.How", "local_index": 3}
{"chunk_id": "Lecture2_Basic_Statistical_Analysis_4", "doc_id": "Lecture2_Basic_Statistical_Analysis", "text": ".,&+9&.&+=&+−7\t896&.,&+9&+}\nDerivativeSlopeGradient\nLearning Rate(or Step Size) Three Problems:1.How to compute the derivative?2.How to set the learning rate?3.What is the convergence criteria?\nNew Jersey Institute of Technology\nDerivative\n/(0!)\n0!\n!% := !% – α (+ number)                                   \n+ slope\n/(0!)\n0!\n!% := !%– α (- number)                                   \n- slope\n10COMP4434\n&1=&1−796&.,&+9&1\nNew Jersey Institute of Technology\nGeometric Interpretation: Gradient Decent\n11C", "local_index": 4}
{"chunk_id": "Lecture2_Basic_Statistical_Analysis_5", "doc_id": "Lecture2_Basic_Statistical_Analysis", "text": "4\n&1=&1−796&.,&+9&1\nNew Jersey Institute of Technology\nGeometric Interpretation: Gradient Decent\n11COMP4434\n§The gradient can be interpreted as the direction and rate of fastest increase§Parameters update in reverse direction of gradient\n\nNew Jersey Institute of Technology\n!!!\"\n\"(!\",!!)\nStep Size\n/(0!)\n0!\nSmall α => slow convergence\n/(0!)\n0!\nLarge α => fail to converge\n12COMP4434\n§Learning rate / Step size α is a user parameter.\nNew Jersey Institute of Technology\nGradient\n13COMP4434\n§Gradient of", "local_index": 5}
{"chunk_id": "Lecture2_Basic_Statistical_Analysis_6", "doc_id": "Lecture2_Basic_Statistical_Analysis", "text": "Step size α is a user parameter.\nNew Jersey Institute of Technology\nGradient\n13COMP4434\n§Gradient of\t6 is the vector :6(&.,&+)=96&.,&+9&.96&.,&+9&+\n§Partial derivative 9 of a function 6(&.,&+)\tof several variables &.,&+ is its derivative with respect to one of those variables, with the others held constant\nNew Jersey Institute of Technology\nGradient ⁄+&$\",$#+$\"\n14COMP4434\n\"#!%,!&\"!%=\"\"!%12()\"6&\n7ℎ$(,(\"))−/(\")8\n=\"\"!%12()\"6&\n7!%+!&,(\")−/(\")8\n=12()\"6&\n7\"\"!%!%+!&,(\")−/(\")8\n=12()\"6&\n72!%+!&,(\")−/(\")\n", "local_index": 6}
{"chunk_id": "Lecture2_Basic_Statistical_Analysis_7", "doc_id": "Lecture2_Basic_Statistical_Analysis", "text": "$(,(\"))−/(\")8\n=\"\"!%12()\"6&\n7!%+!&,(\")−/(\")8\n=12()\"6&\n7\"\"!%!%+!&,(\")−/(\")8\n=12()\"6&\n72!%+!&,(\")−/(\")\n=1()\"6&\n7!%+!&,(\")−/(\")\n5=9:=9(;3)+,+-=+,+.+.+-=9/:;′(3)\tChain Rule\n\nNew Jersey Institute of Technology\nGradient ⁄+&$\",$#+$#\n15COMP4434\n\"#!%,!&\"!&=\"\"!&12()\"6&\n7ℎ$(,(\"))−/(\")8\n=\"\"!&12()\"6&\n7!%+!&,(\")−/(\")8\n=12()\"6&\n7\"\"!&!%+!&,(\")−/(\")8\n=12()\"6&\n72!%+!&,(\")−/(\")1,(\")\n=1()\"6&\n7!%+!&,(\")−/(\"),(\")\nNew Jersey Institute of Technology\nGradient Descent Algorithm\n16COMP4434\n2=0, !%2=!&2=0REPEAT {!%[2+1]=!%[", "local_index": 7}
{"chunk_id": "Lecture2_Basic_Statistical_Analysis_8", "doc_id": "Lecture2_Basic_Statistical_Analysis", "text": "rsey Institute of Technology\nGradient Descent Algorithm\n16COMP4434\n2=0, !%2=!&2=0REPEAT {!%[2+1]=!%[2]−6()\"6&7!%[2]+!&[2],(\")−/(\")\n!&[2+1]=!&[2]−6()\"6&7!%[2]+!&[2],(\")−/(\"),(\")2=2+1} UNTIL\t#!%2−1,!&2−1−#!%[2],!&[2]<9 OR 2>;\nNew Jersey Institute of Technology\nStop Criteria\n§If 6&.,&+\tdecreases by less than a threshold ; (e.g., 10-3) in one iteration§Each iteration, we use all ( training examples to update !%,!&§Use updated !%,!& to recalculate #!%,!& and find the decrease§Or, after < (e.g., 5000)", "local_index": 8}
{"chunk_id": "Lecture2_Basic_Statistical_Analysis_9", "doc_id": "Lecture2_Basic_Statistical_Analysis", "text": " update !%,!&§Use updated !%,!& to recalculate #!%,!& and find the decrease§Or, after < (e.g., 5000) number of iterations\n17COMP4434\nNew Jersey Institute of Technology\nGradient Descent in Action\n18\n(for fixed           , this is a function of x)\n(function of the parameters            )\nCOMP4434\nNew Jersey Institute of Technology\nGradient Descent in Action\n19\nCOMP4434\n(for fixed           , this is a function of x)\n(function of the parameters            )\n\nNew Jersey Institute of Technology\nGradi", "local_index": 9}
{"chunk_id": "Lecture2_Basic_Statistical_Analysis_10", "doc_id": "Lecture2_Basic_Statistical_Analysis", "text": " a function of x)\n(function of the parameters            )\n\nNew Jersey Institute of Technology\nGradient Descent in Action\n20\nCOMP4434\n(for fixed           , this is a function of x)\n(function of the parameters            )\n\nNew Jersey Institute of Technology\nGradient Descent in Action\n21\nCOMP4434\n(for fixed           , this is a function of x)\n(function of the parameters            )\n\nNew Jersey Institute of Technology\nGradient Descent in Action\n22\nCOMP4434\n(for fixed           , this is a funct", "local_index": 10}
{"chunk_id": "Lecture2_Basic_Statistical_Analysis_11", "doc_id": "Lecture2_Basic_Statistical_Analysis", "text": "nstitute of Technology\nGradient Descent in Action\n22\nCOMP4434\n(for fixed           , this is a function of x)\n(function of the parameters            )\n\nNew Jersey Institute of Technology\nGradient Descent in Action\n23\nCOMP4434\n(for fixed           , this is a function of x)\n(function of the parameters            )\n\nNew Jersey Institute of Technology\nGradient Descent in Action\n24\nCOMP4434\n(for fixed           , this is a function of x)\n(function of the parameters            )\n\nNew Jersey Institute", "local_index": 11}
{"chunk_id": "Lecture2_Basic_Statistical_Analysis_12", "doc_id": "Lecture2_Basic_Statistical_Analysis", "text": "           , this is a function of x)\n(function of the parameters            )\n\nNew Jersey Institute of Technology\nGradient Descent in Action\n25\nCOMP4434\n(for fixed           , this is a function of x)\n(function of the parameters            )\n\nNew Jersey Institute of Technology\nGradient Descent in Action\n26\nCOMP4434\n(for fixed           , this is a function of x)\n(function of the parameters            )\n\nNew Jersey Institute of Technology\nSummary: Univariate Linear Regression\n27COMP4434\nRepeat u", "local_index": 12}
{"chunk_id": "Lecture2_Basic_Statistical_Analysis_13", "doc_id": "Lecture2_Basic_Statistical_Analysis", "text": "     )\n\nNew Jersey Institute of Technology\nSummary: Univariate Linear Regression\n27COMP4434\nRepeat until convergence {0!=0!−4\t67/0!,0\"70!0\"=0\"−4\t67/0!,0\"70\"}\nDerivative(or Gradient)\nLearning Rate(or Step Size) \nTarget: Find best fitting line ℎ#!=0!+0\"!\nSquared Error: (Es.mated – Actual)*\nminimize /0!,0\"=\"$%∑&'\"%ℎ#(!(&))−;(&)$\nGradient Descent Algorithm\nNew Jersey Institute of Technology\nMultivariate Linear Regression\nGPA(<*)# of Exchanges (<+) Age (<,) 1st Salary (\")3.51 23200002.12 2210000… … …", "local_index": 13}
{"chunk_id": "Lecture2_Basic_Statistical_Analysis_14", "doc_id": "Lecture2_Basic_Statistical_Analysis", "text": "te Linear Regression\nGPA(<*)# of Exchanges (<+) Age (<,) 1st Salary (\")3.51 23200002.12 2210000… … … …(\n2=3\nℎ#\"=$.+$+\"++$-\"-+$=\"=\nGPA(<) 1st Salary (\")3.5200002.110000… …\n2=1\nℎ#\"=$.+$+\"\nCOMP4434 28\nℎ#!=&.+&+!++&-!- +⋯\t\n#(!%,!&,⋯)=12()\"6&\n7ℎ$(,(\"))−/(\")8\nNew Jersey Institute of Technology\nExample with Training dataThe training data contain some example measurements of the profit gained by opening an outlet in the cities with the population ranging between 30,000 and 100,000. The y-values are the ", "local_index": 14}
{"chunk_id": "Lecture2_Basic_Statistical_Analysis_15", "doc_id": "Lecture2_Basic_Statistical_Analysis", "text": "n outlet in the cities with the population ranging between 30,000 and 100,000. The y-values are the profit measured in USD, and the x-values are the populations of the city. Each city population and profit tuple constitutes one training example in training dataset.\n29COMP4434\nCity Population (104) xProfit (104) y6.48626.59875.52779.13028.518613.6627.003211.854\nNew Jersey Institute of Technology\nInitialize Gradient DescentUse training dataset to develop a linear regression model and solve it by u", "local_index": 15}
{"chunk_id": "Lecture2_Basic_Statistical_Analysis_16", "doc_id": "Lecture2_Basic_Statistical_Analysis", "text": "itialize Gradient DescentUse training dataset to develop a linear regression model and solve it by using gradient descent algorithm. Find the values of !%,!&, and cost function # in the first two iterations. (!%=0,!&=0,a=0.01)\n!%0=!&0=0\n30COMP4434\nInitial setting:\nNew Jersey Institute of Technology\nIteration 1§Update !%:7/70!0=14A&'\"\n-0!0+0\"0!&−;&\n=⁄14−;\"−;$−;.−;-=⁄14−6.5987−9.1302−13.662−11.854=−10.3110!1=0![0]−0.01/0/#!0=0.10311§Update !&:7/70\"0=14A&'\"\n-0!0+0\"0!&−;&!&\n=⁄14−;\"!\"−;$!$−;.!.−;-!-=", "local_index": 16}
{"chunk_id": "Lecture2_Basic_Statistical_Analysis_17", "doc_id": "Lecture2_Basic_Statistical_Analysis", "text": "10.3110!1=0![0]−0.01/0/#!0=0.10311§Update !&:7/70\"0=14A&'\"\n-0!0+0\"0!&−;&!&\n=⁄14−;\"!\"−;$!$−;.!.−;-!-=⁄14[ ]−6.5987×6.4862−9.1302×5.5277−13.662×8.5186−11.854×7.0032=−73.1670\"1=0\"[0]−0.01/0/#\"0=0.73167 31COMP4434\nNew Jersey Institute of Technology\nCost Update§Update #!%,!&:/0![0],0\"[0]=18A&'\"\n-0![0]+0\"[0]!(&)−;(&)$\n=⁄18;(\")$+;($)$+;(.)$+;(-)$=⁄186.5987$+9.1302$+13.662$+11.854$=56.759\n/0![1],0\"[1]=18A&'\"\n-0![1]+0\"[1]!(&)−;(&)$\n=⁄18[\n]\n0.10311+0.73167×6.4862−6.5987$+0.10311+0.73167×5.5277−9.1302$+0.1", "local_index": 17}
{"chunk_id": "Lecture2_Basic_Statistical_Analysis_18", "doc_id": "Lecture2_Basic_Statistical_Analysis", "text": "&'\"\n-0![1]+0\"[1]!(&)−;(&)$\n=⁄18[\n]\n0.10311+0.73167×6.4862−6.5987$+0.10311+0.73167×5.5277−9.1302$+0.10311+0.73167×8.5186−13.662$+0.10311+0.73167×7.0032−11.854$=15.864\n32COMP4434\nNew Jersey Institute of Technology\nIteration 2§Update !%:7/70!1=14A&'\"\n-0!1+0\"1!&−;&=⋯0!2=0![1]−0.01/0/#!1=⋯§Update !&:7/70\"1=14A&'\"\n-0!1+0\"1!&−;&!&=⋯0\"2=0\"[1]−0.01/0/#\"1=⋯§Update #!%,!&:/0![2],0\"[2]=18A&'\"\n-0![2]+0\"[2]!(&)−;(&)$=⋯ 33COMP4434\nNew Jersey Institute of Technology\nLinear Regression by Linear Algebra§Minimizin", "local_index": 18}
{"chunk_id": "Lecture2_Basic_Statistical_Analysis_19", "doc_id": "Lecture2_Basic_Statistical_Analysis", "text": "−;(&)$=⋯ 33COMP4434\nNew Jersey Institute of Technology\nLinear Regression by Linear Algebra§Minimizing function:\n§Necessary Condition: OP(#!,#\",⋯)O## =0,0≤@≤%\nmin$!,$\",⋯#(!%,!&,⋯)=min$!,$\",⋯12()\"6&\n7ℎ$(,(\"))−/(\")8\n=min$!,$\",⋯12()\"6&\n7!%+!&,&(\")+!8,8(\")+⋯−/(\")8\n  \nCOMP4434 34\nNew Jersey Institute of Technology\nNormal Equation\n7/(0!,0\",⋯)70! =1QA&'\"\n%0!+0\"!\"(&)+0$!$(&)+⋯+01!1(&)−;(&)=07/(0!,0\",⋯)70\" =1QA&'\"\n%0!+0\"!\"(&)+0$!$(&)+⋯+01!1(&)−;(&)!\"(&)=07/(0!,0\",⋯)701 = ⋯1QA&'\"\n%0!+0\"!\"(&)+0$!$(&)+⋯+01!1", "local_index": 19}
{"chunk_id": "Lecture2_Basic_Statistical_Analysis_20", "doc_id": "Lecture2_Basic_Statistical_Analysis", "text": "=1QA&'\"\n%0!+0\"!\"(&)+0$!$(&)+⋯+01!1(&)−;(&)!\"(&)=07/(0!,0\",⋯)701 = ⋯1QA&'\"\n%0!+0\"!\"(&)+0$!$(&)+⋯+01!1(&)−;(&)!1(&)=0\nCOMP4434 35\nNew Jersey Institute of Technology\nNormal Equation for Univariate Case\n̅!=1QA&'\"\n%!(&),S;=1QA&'\"\n%;(&),!;=1QA&'\"\n%!(&);(&),!$=1QA&'\"\n%(!(&))$§Notions:\n§Equations:\n§Solutions: \n1QA&'\"\n%0!+0\"!(&)−;(&)=0!+̅!0\"−S;=01QA&'\"\n%0!+0\"!(&)−;(&)!(&)=0!̅!+!$0\"−!;=0\n0!=S;∗!$−̅!∗!;!$−(̅!)$0\"=!;−̅!∗S;!$−(̅!)$COMP4434 36\nNew Jersey Institute of Technology\nSame ExampleThe training data c", "local_index": 20}
{"chunk_id": "Lecture2_Basic_Statistical_Analysis_21", "doc_id": "Lecture2_Basic_Statistical_Analysis", "text": "!)$0\"=!;−̅!∗S;!$−(̅!)$COMP4434 36\nNew Jersey Institute of Technology\nSame ExampleThe training data contain some example measurements of the profit gained by opening an outlet in the cities with the population ranging between 30,000 and 100,000. The y-values are the profit measured in USD, and the x-values are the populations of the city. Each city population and profit tuple constitutes one training example in training dataset.\n37COMP4434\nCity Population (104) xProfit (104) y6.48626.59875.52779.", "local_index": 21}
{"chunk_id": "Lecture2_Basic_Statistical_Analysis_22", "doc_id": "Lecture2_Basic_Statistical_Analysis", "text": "ng example in training dataset.\n37COMP4434\nCity Population (104) xProfit (104) y6.48626.59875.52779.13028.518613.6627.003211.854\nNew Jersey Institute of Technology\nSolution with Normal Equation̅!=14(6.4862+5.5277+8.5186+7.0032)=6.8839S;=14(6.5987+9.1302+13.662+11.854)=10.311!;=146.4862×6.5987+5.5277×9.1302+8.5186×13.662+7.0032×11.854=73.167!$=146.4862×6.4862+5.5277×5.5277+8.5186×8.5186+7.0032×7.0032=48.559\nU0!+̅!0\"−S;=0=0!+6.88390\"−10.311=0̅!0!+!$0\"−!;=6.88390!+48.550\"−73.167=0⇒W0!=−2.54710\"=1.8", "local_index": 22}
{"chunk_id": "Lecture2_Basic_Statistical_Analysis_23", "doc_id": "Lecture2_Basic_Statistical_Analysis", "text": "32=48.559\nU0!+̅!0\"−S;=0=0!+6.88390\"−10.311=0̅!0!+!$0\"−!;=6.88390!+48.550\"−73.167=0⇒W0!=−2.54710\"=1.8679/0!,0\"=⁄18−2.5471+1.8679×6.4862−6.5987$+⋯=1.5598\n38COMP4434\nNew Jersey Institute of Technology\nBasic Knowledge about Matrix\nCOMP4434 39\nBC=DC=B?&D\nNew Jersey Institute of Technology\nMultivariate Linear Regression 7/(0!,0\",⋯)70! =1QA&'\"\n%0!+0\"!\"(&)+0$!$(&)+⋯+01!1(&)−;(&)=07/(0!,0\",⋯)70\" =1QA&'\"\n%0!+0\"!\"(&)+0$!$(&)+⋯+01!1(&)−;(&)!\"(&)=07/(0!,0\",⋯)701 = ⋯1QA&'\"\n%0!+0\"!\"(&)+0$!$(&)+⋯+01!1(&)−;(&)!1", "local_index": 23}
{"chunk_id": "Lecture2_Basic_Statistical_Analysis_24", "doc_id": "Lecture2_Basic_Statistical_Analysis", "text": "!+0\"!\"(&)+0$!$(&)+⋯+01!1(&)−;(&)!\"(&)=07/(0!,0\",⋯)701 = ⋯1QA&'\"\n%0!+0\"!\"(&)+0$!$(&)+⋯+01!1(&)−;(&)!1(&)=0\nCOMP4434 40\n1%&\"\n'1@1(!+1%&\"\n'3\"(%)@1(\"+1%&\"\n'3*(%)@1(*+⋯+1%&\"\n'30%@1(0=1%&\"\n'5(%)@1\n1%&\"\n'1@3\"(%)(!+1%&\"\n'3\"(%)3\"(%)(\"+1%&\"\n'3*(%)3\"(%)(*+⋯+1%&\"\n'30%3\"%(0=1%&\"\n'5(%)3\"%\n⋯1%&\"\n'1@30(%)(!+1%&\"\n'3\"(%)30(%)(\"+1%&\"\n'3*(%)30(%)(*+⋯+1%&\"\n'30%30%(0=1%&\"\n'5(%)30%\nNew Jersey Institute of Technology\nMatrix Form of Normal Equation\nCOMP4434 41\nX=\nA&'\"\n%161A&'\"\n%!\"&61A&'\"\n%!$&61\t⋯A&'\"\n%!1&61\nA&'\"\n%16!\"(&", "local_index": 24}
{"chunk_id": "Lecture2_Basic_Statistical_Analysis_25", "doc_id": "Lecture2_Basic_Statistical_Analysis", "text": "rix Form of Normal Equation\nCOMP4434 41\nX=\nA&'\"\n%161A&'\"\n%!\"&61A&'\"\n%!$&61\t⋯A&'\"\n%!1&61\nA&'\"\n%16!\"(&)A&'\"\n%!\"&!\"(&)A&'\"\n%!$&!\"(&)\t⋯A&'\"\n%!1&!\"(&)⋯A&'\"\n%16!1(&)A&'\"\n%!\"&!1(&)A&'\"\n%!$&!1(&)\t⋯A&'\"\n%!1&!1(&)\nY=\nA&'\"\n%;(&)61\nA&'\"\n%;(&)!\"&⋯A&'\"\n%;(&)!1&\nA+=B+=AZ+B\nNew Jersey Institute of Technology\nMatrix Form of Normal Equation\n[=1!\"(\")⋯!2(\")⋯!1(\")1!\"($)⋯!2($)⋯!1($)⋮1!\"(%)⋯!2(%)⋯!1(%), 0=0!0\"⋮01, ;=;(\");($)⋮;(%) \n,$,-=,$.-=,$,%&,$.COMP4434 42\n§Matrix Form of solution & in terms of matrics < and # ?\nF", "local_index": 25}
{"chunk_id": "Lecture2_Basic_Statistical_Analysis_26", "doc_id": "Lecture2_Basic_Statistical_Analysis", "text": ");($)⋮;(%) \n,$,-=,$.-=,$,%&,$.COMP4434 42\n§Matrix Form of solution & in terms of matrics < and # ?\nFeatures Target variables", "local_index": 26}
{"chunk_id": "Lecture3_Machine_Learning_0", "doc_id": "Lecture3_Machine_Learning", "text": "COMP4434 Big Data AnalyticsLecture 3 Machine learningHUANG Xiaoxiaohuang@comp.polyu.edu.hk\n\nNew Jersey Institute of Technology\nDefinition of Machine Learning§H. SimonLearning denotes changes in the system that are adaptive in the sense that they enable the system to do the task or tasks drawn from the same population more efficiently and more effectively the next time.§T. Mitchell: Well posed machine learning – Improving performance via experienceFormally, a computer program is said to learn fro", "local_index": 0}
{"chunk_id": "Lecture3_Machine_Learning_1", "doc_id": "Lecture3_Machine_Learning", "text": "ine learning – Improving performance via experienceFormally, a computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T as measured by P improves with experience E.COMP4434 3\nNew Jersey Institute of Technology\nMachine Learning Algorithms§Supervised Learning§Training data includes desired outputs§Unsupervised Learning§Training data does not include desired outputs§Find hidden structure in data\nCOMP4434 4\n", "local_index": 1}
{"chunk_id": "Lecture3_Machine_Learning_2", "doc_id": "Lecture3_Machine_Learning", "text": "ed Learning§Training data does not include desired outputs§Find hidden structure in data\nCOMP4434 4\n§Semi-supervised Learning§Reinforcement Learning\nNew Jersey Institute of Technology\nSupervised Learning Workflow\nCOMP4434 5\nNew Jersey Institute of Technology\nSupervised Learning Task - Regression§Regression§Learning to predict a continuous/real value§Ex: housing price, gold price, stock price\nCOMP4434 6\nNew Jersey Institute of Technology\nSupervised Learning Task - Classification §Classification§L", "local_index": 2}
{"chunk_id": "Lecture3_Machine_Learning_3", "doc_id": "Lecture3_Machine_Learning", "text": "434 6\nNew Jersey Institute of Technology\nSupervised Learning Task - Classification §Classification§Learning to predict a discrete value from a predefined set of values§Ex. weather prediction, spam email filtering, product categorization, object detection, medical diagnose\nCOMP4434 7\nNew Jersey Institute of Technology\nUnsupervised Learning Task - Clustering\n§Clustering§Determine the intrinsic grouping in a set of unlabeled data§Ex. clustering in networking, image clustering\nCOMP4434 8\nNew Jersey ", "local_index": 3}
{"chunk_id": "Lecture3_Machine_Learning_4", "doc_id": "Lecture3_Machine_Learning", "text": "ing in a set of unlabeled data§Ex. clustering in networking, image clustering\nCOMP4434 8\nNew Jersey Institute of Technology\nSupervised vs Unsupervised\nCOMP4434 9\nSupervisedUnsupervised\n•Labeled Data•Direct Feedback•Predict Output•Non-labeled Data•No Feedback•Find Hidden Structure in Data\nModel\nModel\nBRY……(68, 144, 196)YB Y\nIt is Yellow\n(255, 192, 0)I can see difference\nNew Jersey Institute of Technology\nSupervised learning tasks§Classification§predicts categorical class labels§classifies data (c", "local_index": 4}
{"chunk_id": "Lecture3_Machine_Learning_5", "doc_id": "Lecture3_Machine_Learning", "text": "nology\nSupervised learning tasks§Classification§predicts categorical class labels§classifies data (constructs a model) based on the training set and the values (class labels) and uses the trained model to classify new data§return a discrete-value (label) as output, e.g., classifying Hang Seng Index (HSI)’s trend as Up, Down, Level§Regression§models continuous-valued functions, i.e., predicts unknown or missing values§Return a real-value as output, e.g., predicting HSI’s future valuesCOMP4434 10\n", "local_index": 5}
{"chunk_id": "Lecture3_Machine_Learning_6", "doc_id": "Lecture3_Machine_Learning", "text": "wn or missing values§Return a real-value as output, e.g., predicting HSI’s future valuesCOMP4434 10\nNew Jersey Institute of Technology\nTwo-Step Process\n1.Model Construction: describing a set of samples§Each sample is associate with a label attribute§The set of samples used for model construction: training set§The model is represented as mathematical formulae2.Model Usage: for future or unknown objects§The known label of test sample is compared with the result from the model§Test set is independe", "local_index": 6}
{"chunk_id": "Lecture3_Machine_Learning_7", "doc_id": "Lecture3_Machine_Learning", "text": "ects§The known label of test sample is compared with the result from the model§Test set is independent of training setCOMP4434 11\nTrainingSetData Set\nAlgorithm\nTest Set\nModel\nOptimize Parameters\nEvaluate\nNew Jersey Institute of Technology\nStructured Data: Tabular Data\nCOMP4434 12\nSepalPetalLengthWidthLengthWidth4.93.01.40.24.73.21.30.24.63.11.50.25.03.61.40.25.84.01.20.25.63.04.51.55.82.74.11.06.22.24.51.55.62.53.91.15.93.24.81.87.13.05.92.16.32.95.61.86.53.05.82.27.63.06.62.1Instance/tupleattri", "local_index": 7}
{"chunk_id": "Lecture3_Machine_Learning_8", "doc_id": "Lecture3_Machine_Learning", "text": "2.24.51.55.62.53.91.15.93.24.81.87.13.05.92.16.32.95.61.86.53.05.82.27.63.06.62.1Instance/tupleattribute/feature\nNew Jersey Institute of Technology\nTabular Data with Labels§Attributes of a table !!,!\",…,!#§We also call “attribute” as “feature”§Number of features !\trepresents the dimensionality§A data object # is represented as (#!,#\",…,##)§In some datasets, each data object has a labelCOMP4434 13\nSepalPetalClass LabelLengthWidthLengthWidth4.93.01.40.2Setosa4.73.21.30.2Setosa4.63.11.50.2Setosa5.0", "local_index": 8}
{"chunk_id": "Lecture3_Machine_Learning_9", "doc_id": "Lecture3_Machine_Learning", "text": "SepalPetalClass LabelLengthWidthLengthWidth4.93.01.40.2Setosa4.73.21.30.2Setosa4.63.11.50.2Setosa5.03.61.40.2Setosa5.84.01.20.2Setosa5.63.04.51.5Versicolor5.82.74.11.0Versicolor6.22.24.51.5Versicolor5.62.53.91.1Versicolor5.93.24.81.8Versicolor7.13.05.92.1Virginica6.32.95.61.8Virginica6.53.05.82.2Virginica7.63.06.62.1Virginica4.92.54.51.7Virginica\n\nNew Jersey Institute of Technology\nClassification (Binary vs Multi-class)\n14COMP4434\n§Binary Classification§Email: Spam / Not Spam§Tumor: Malignant / ", "local_index": 9}
{"chunk_id": "Lecture3_Machine_Learning_10", "doc_id": "Lecture3_Machine_Learning", "text": "(Binary vs Multi-class)\n14COMP4434\n§Binary Classification§Email: Spam / Not Spam§Tumor: Malignant / Benign§Covid-19: Positive / Negative §&∈{0,1}\t§Multi-class Classification§Email auto-tagging: Spam / Work / Personal§Credit Rating: Poor / Okay / Trust§Handwriting number: 0, 1, 2, 3, 4, ...§&∈{0,1,2,…}\nNew Jersey Institute of Technology\nClassification by Linear Regression§Training Examples§both are supervised learning (\",$)\t§Linear Regression: $ is a real-value, e.g., salary§What we need is discr", "local_index": 10}
{"chunk_id": "Lecture3_Machine_Learning_11", "doc_id": "Lecture3_Machine_Learning", "text": " supervised learning (\",$)\t§Linear Regression: $ is a real-value, e.g., salary§What we need is discrete label:§0: malignant; 1: benign§Can we use Linear Regression Model to do classification? Any disadvantages?§Yes, we can, but not good §ℎ$\"=)%+)!\"\n15COMP4434\nNew Jersey Institute of Technology\nClassification by Linear Regression - Example§A linear regression model classifies tumors as malignant (1) or benign(0) given their size§The linear regression model minimizes the distances between the poin", "local_index": 11}
{"chunk_id": "Lecture3_Machine_Learning_12", "doc_id": "Lecture3_Machine_Learning", "text": ") or benign(0) given their size§The linear regression model minimizes the distances between the points and the hyperplane (line for single feature)§The threshold is set as 0.5\n16COMP4434\nNew Jersey Institute of Technology\nClassification by Linear Regression - Example§After introducing a few more malignant tumor cases, the regression line shifts and a threshold of 0.5 no longer separates the classes§Conclusion:  Linear regression is sensitive to imbalance data for classification problem.\n17COMP44", "local_index": 12}
{"chunk_id": "Lecture3_Machine_Learning_13", "doc_id": "Lecture3_Machine_Learning", "text": "s§Conclusion:  Linear regression is sensitive to imbalance data for classification problem.\n17COMP4434\nNew Jersey Institute of Technology\nLogistic Regression§New model outputs probabilities§It works better in both cases using 0.5 as a threshold§The inclusion of additional points does not affect the estimated curve too much \n18COMP4434\nNew Jersey Institute of Technology\nHypothesis Model§Linear Regression:  −∞<ℎ$(<+∞       ℎ$#=/%#%+/!#!+⋯+/&#&=/'#\n§Logistic Regression: 0<ℎ$(<1§ℎ$#=2(/'#)§logistic/", "local_index": 13}
{"chunk_id": "Lecture3_Machine_Learning_14", "doc_id": "Lecture3_Machine_Learning", "text": "ession:  −∞<ℎ$(<+∞       ℎ$#=/%#%+/!#!+⋯+/&#&=/'#\n§Logistic Regression: 0<ℎ$(<1§ℎ$#=2(/'#)§logistic/sigmoid function 25=!!()!\"§ℎ$#=!!()!#$%§https://www.wolframalpha.com/y = 1/(1+e^-x), x from -6 to 6\n✓ =\n2\n6\n6\n6\n4\n✓ 0\n✓ 1\n...\n✓ n\n3\n7\n7\n7\n5\nx =\n2\n6\n6\n6\n4\nx 0\nx 1\n...\nx n\n3\n7\n7\n7\n5\n19COMP4434\nNew Jersey Institute of Technology\nRepresentation§ℎ$( represents the estimated probability that ,=1 on input (§ℎ$(=.{,=1|(, 1} means probability of ,=1, given (, under parameter values 1§.{,=0|(, 1}=1−ℎ$(§Exam", "local_index": 14}
{"chunk_id": "Lecture3_Machine_Learning_15", "doc_id": "Lecture3_Machine_Learning", "text": "(§ℎ$(=.{,=1|(, 1} means probability of ,=1, given (, under parameter values 1§.{,=0|(, 1}=1−ℎ$(§Example§#%#!= 16789:;\t<=\t>?@8\tA<;!>\t§ℎ$#=0.8: this email # has 80% chance of being spam20COMP4434\nNew Jersey Institute of Technology\nFurther Understanding§ℎ$(=31%(=!!&'!\"#$∈(0,1)§Predict ,=1 when ℎ$(≥0.5, i.e., 1%(≥0§Predict ,=0 when\t1%(<0\n21COMP4434\nNew Jersey Institute of Technology\nDecision Boundary\n§1%(=0 is the decision boundary, e.g.,§Assume ℎ$(=3(−3+(!+(\")§Decision boundary:\t−3+(!+(\"=0, i.e., (", "local_index": 15}
{"chunk_id": "Lecture3_Machine_Learning_16", "doc_id": "Lecture3_Machine_Learning", "text": "§1%(=0 is the decision boundary, e.g.,§Assume ℎ$(=3(−3+(!+(\")§Decision boundary:\t−3+(!+(\"=0, i.e., (!+(\"=3§Predict ,=1 when −3+(!+(\"≥0, i.e., (!+(\"≥3 (red)\nx1\nx2\n1\n1 2 3\n2\n3 xxxx\nxxx\n**** **\n22COMP4434\nNew Jersey Institute of Technology\nOther Decision Boundary\n§Given ℎ$(=31%(=3() 1(((+1!(!+1\"(\"+1)(!\"+1*(\"\"=3(−1+(!\"+(\"\")§If ℎ$(=3(−1+(!\"+(\"\"), draw the region that predicts ,=1 in the ((!,(\") plane\n23COMP4434\nNew Jersey Institute of Technology\nNon-Linear Decision Boundary\n§Decision boundary 1%(=0→(", "local_index": 16}
{"chunk_id": "Lecture3_Machine_Learning_17", "doc_id": "Lecture3_Machine_Learning", "text": "3COMP4434\nNew Jersey Institute of Technology\nNon-Linear Decision Boundary\n§Decision boundary 1%(=0→(!\"+(\"\"=1§Predict ,=1 when (!\"+(\"\"≥1§The region of ,=1 is outside the circle\n24COMP4434\nx1\nx2\n-1 1-1\n1\nx\nxx x*\nx\nxx ****\n**\nNew Jersey Institute of Technology\nCost Function of Linear Regression\n§Cost Function =1=min$&,$',…!\",∑-.!,ℎ$(#(-))−&(-)\"\n25COMP4434\n§Linear Regression§ℎ$ is linear §I(/) is convex§I(/)\thas a single minimum\nNew Jersey Institute of Technology\n§MSE: MeanSquare Error!+ ∑,-!+?,.−,.", "local_index": 17}
{"chunk_id": "Lecture3_Machine_Learning_18", "doc_id": "Lecture3_Machine_Learning", "text": "ex§I(/)\thas a single minimum\nNew Jersey Institute of Technology\n§MSE: MeanSquare Error!+ ∑,-!+?,.−,.\"\n§MAE: MeanAbsolute Error!+ ∑,-!+?,.−,.§MAPE(Mean Absolute Percentage Error)!((%+ ∑,-!+01%21%1% \nRegression Metrics\n26COMP4434\nNew Jersey Institute of Technology\nApply MSE to Logistic Regression\n§We can apply the same cost function for logistic regression\n27COMP4434\n§Problems§I(/) would become non-convex. Why?§It has multiple local minimums§Gradient descent will be stuck in a local minimum\nhttps:", "local_index": 18}
{"chunk_id": "Lecture3_Machine_Learning_19", "doc_id": "Lecture3_Machine_Learning", "text": "convex. Why?§It has multiple local minimums§Gradient descent will be stuck in a local minimum\nhttps://towardsdatascience.com/why-not-mse-as-a-loss-function-for-logistic-regression-589816b5e03c\nNew Jersey Institute of Technology\nLogistic Loss\n=(1)=1@A,-!\n3BCDEℎ$((,),,(,)\nBCDEℎ$(,,=F−log(ℎ$(()),=1−log(1−ℎ$(()),=0\n28COMP4434\nNew Jersey Institute of Technology\nLogistic Loss - Heavy Penalty\nBCDEℎ$(,,=F−log(ℎ$(()),=1−log(1−ℎ$(()),=0§BCDEℎ$(,,→0  §When ℎ$#→1, i.e., predict &=1§Good predication, low cos", "local_index": 19}
{"chunk_id": "Lecture3_Machine_Learning_20", "doc_id": "Lecture3_Machine_Learning", "text": "−log(ℎ$(()),=1−log(1−ℎ$(()),=0§BCDEℎ$(,,→0  §When ℎ$#→1, i.e., predict &=1§Good predication, low cost§BCDEℎ$(,,→∞  §When ℎ$#→0, i.e., predict &=0§Bad predicafon!§High cost represents penalty\n−log(:)\n29COMP4434\nNew Jersey Institute of Technology\nUnderstanding !\"#$ℎ!&,(=*BCDEℎ$(,,=F−log(ℎ$(()),=1−log(1−ℎ$(()),=0§BCDEℎ$(,,→0  §When ℎ$#→0, i.e., predict &=0§Good predication, low cost§BCDEℎ$(,,→∞  §When ℎ$#→1, i.e., predict &=1§Bad predicafon!§High cost represents penalty\n−log(1−:)\n10\n30COMP4434\nNew ", "local_index": 20}
{"chunk_id": "Lecture3_Machine_Learning_21", "doc_id": "Lecture3_Machine_Learning", "text": "n ℎ$#→1, i.e., predict &=1§Bad predicafon!§High cost represents penalty\n−log(1−:)\n10\n30COMP4434\nNew Jersey Institute of Technology\nCost Function!(#)=1'(!\"#\n$)*+,ℎ%.(!),0(!)\n)*+,ℎ%.,0=1−log(ℎ%(.))0=1−log(1−ℎ%(.))0=0)*+,ℎ%.,0= −0logℎ%(.) −(1−0)log(1−ℎ%(.))\n!#=−1'(!\"#\n$0(!)logℎ%.(!)+1−0(!)log1−ℎ%.(!)\n31COMP4434\nNew Jersey Institute of Technology\nGradient Descent Algorithm\nKI(/%,/!,⋯)K/1 =18L-.!\n,ℎ$#(-)−&(-)#1(-)\n<==−1?@!\"#\n$A(!)logℎ%C(!)+1−A(!)log1−ℎ%C(!)\nℎ%C=E=(C=11+F)%!*\n/1=/1−aKI(/%,/!,⋯)K/1\nRep", "local_index": 21}
{"chunk_id": "Lecture3_Machine_Learning_22", "doc_id": "Lecture3_Machine_Learning", "text": "-)−&(-)#1(-)\n<==−1?@!\"#\n$A(!)logℎ%C(!)+1−A(!)log1−ℎ%C(!)\nℎ%C=E=(C=11+F)%!*\n/1=/1−aKI(/%,/!,⋯)K/1\nRepeat until convergence {\n}\nLooks identical to linear regression!32COMP4434\nNew Jersey Institute of Technology\nHow about Multi-class Classification§Train a logistic regression classifier ℎ$(,)( for each class J to predict the probability of ,=1\t§On a new input (, pick the class that maximizes max, ℎ$(,)(\n33COMP4434\nNew Jersey Institute of Technology\nOne-vs-All Approach\n34COMP4434\nNew Jersey Institut", "local_index": 22}
{"chunk_id": "Lecture3_Machine_Learning_23", "doc_id": "Lecture3_Machine_Learning", "text": ",)(\n33COMP4434\nNew Jersey Institute of Technology\nOne-vs-All Approach\n34COMP4434\nNew Jersey Institute of Technology\nExercise§Assume that there is a classification problem with 4 classes. Each instance has 5 features. What is the total number of parameters, if you are solving it by using linear logistic regression and one-vs-all approach? Remember to include /%.§We have 4 classes, so we need 4 binary classifiers. In each classifier, we have /%, /!, /\", ..., /2. Thus, in total, we have 4*6 = 24 pa", "local_index": 23}
{"chunk_id": "Lecture3_Machine_Learning_24", "doc_id": "Lecture3_Machine_Learning", "text": "ry classifiers. In each classifier, we have /%, /!, /\", ..., /2. Thus, in total, we have 4*6 = 24 parameters.\n35COMP4434\nNew Jersey Institute of Technology\n§True Positives (TP):the actual class of the data point was True and the predicted is also True§True Negatives (TN): the actual class of the data point was False and the predicted is also False§False Positives (FP):the actual class of the data point was False and the predicted is True§False Negatives (FN): the actual class of the data point w", "local_index": 24}
{"chunk_id": "Lecture3_Machine_Learning_25", "doc_id": "Lecture3_Machine_Learning", "text": "point was False and the predicted is True§False Negatives (FN): the actual class of the data point was True and the predicted is False\nClassificationMetrics\n36\nActualTrueFalse\nPredictedTrueTPFP\nFalseFNTN\nCOMP4434\nNew Jersey Institute of Technology\nAccuracy=TP+TNTP+FP+FN+TN§A good measure when the target variable classes in the data are nearly balanced§60% classes in our fruit images are apples and 40% are oranges§NEVER used as a measure when the target variable classes in the data are a majority", "local_index": 25}
{"chunk_id": "Lecture3_Machine_Learning_26", "doc_id": "Lecture3_Machine_Learning", "text": " 40% are oranges§NEVER used as a measure when the target variable classes in the data are a majority of one class (Why?)\nAccuracy\n37\nActualTrueFalse\nPredictedTrueTPFP\nFalseFNTN\nCOMP4434\nNew Jersey Institute of Technology\nLimitation of AccuracyAccuracy=TP+TNTP+FP+FN+TN§Example: In daily life, 5 people in 100 people have cancer.§Consider a fake cancer detection model only outputs ‘health’, its accuracy can achieve 95%. §Although its accuracy is good, is it a good model? NO.§Observation: Accuracy p", "local_index": 26}
{"chunk_id": "Lecture3_Machine_Learning_27", "doc_id": "Lecture3_Machine_Learning", "text": "acy can achieve 95%. §Although its accuracy is good, is it a good model? NO.§Observation: Accuracy performs bad when the target variable classes in the data are a majority of one class.\n38COMP4434\nNew Jersey Institute of Technology\nPrecision=TPTP+FP§Precisionis about being precise; even if we managed to capture only one True case, and we captured it correctly, then we are 100% precise\nRecall=TPTP+FN§Recallis not so much about capturing cases correctly but more about capturing allTrue cases\nPreci", "local_index": 27}
{"chunk_id": "Lecture3_Machine_Learning_28", "doc_id": "Lecture3_Machine_Learning", "text": "FN§Recallis not so much about capturing cases correctly but more about capturing allTrue cases\nPrecision and Recall\n39\nActualTrueFalse\nPredictedTrueTPFP\nFalseFNTN\nCOMP4434\nActualTrueFalse\nPredictedTrueTPFP\nFalseFNTN\nNew Jersey Institute of Technology\nF1\tScore=2×Precision×RecallPrecision+Recall§A single score that represents both Precision and Recall§F1 Score isthe harmonic meanofPrecisionand Recall§Differentwitharithmeticmean,harmonic meanis closer to the smaller number as compared to the larger", "local_index": 28}
{"chunk_id": "Lecture3_Machine_Learning_29", "doc_id": "Lecture3_Machine_Learning", "text": "l§Differentwitharithmeticmean,harmonic meanis closer to the smaller number as compared to the larger number§IfPrecisionis0.01andRecallis0.99,thenarithmeticmeanis0.5andharmonic meanis0.0198§Therefore, F1 score of the previous cancer detection model will be 0 (“positive” refers to having cancer)\nF1 Score \n40COMP4434\nNew Jersey Institute of Technology\nExercise§Assume that there are 1,000 documents in total. Among them, 700 documents are related to big data analysis. You build a model to identify do", "local_index": 29}
{"chunk_id": "Lecture3_Machine_Learning_30", "doc_id": "Lecture3_Machine_Learning", "text": " total. Among them, 700 documents are related to big data analysis. You build a model to identify documents related to big data analysis. As a result, your model returns 800 documents, but only 550 of them are relevant to big data analysis. What is the recall of your model? What is the F1 score of your model?§TP = 550.  FP = 250. FN = 150§Precision = 550/800.§Recall = 550/700.§F1 = (2*550/800*550/700)/(550/800+550/700) = 0.733333333.\n41COMP4434", "local_index": 30}
{"chunk_id": "Lecture3_Machine_Learning_31", "doc_id": "Lecture3_Machine_Learning", "text": "700)/(550/800+550/700) = 0.733333333.\n41COMP4434", "local_index": 31}
{"chunk_id": "Lecture4_Overfitting_SVM_0", "doc_id": "Lecture4_Overfitting_SVM", "text": "COMP4434 Big Data AnalyticsLecture 4 Overfitting &Support Vector MachinesHUANG Xiaoxiaohuang@comp.polyu.edu.hk\n\nNew Jersey Institute of Technology\nModel Evaluation\n§When training the model, we can not use test set§If we have several models, e.g., linear regression and quadratic regression, how could we evaluate them?3COMP4434\nTrainingSetData set\nAlgorithm\nTest Set\nModel\nOptimize Parameters\nEvaluate!\"!=ℎ\"(&!)(&!,\"!)\nNew Jersey Institute of Technology\nUnderfitting and Overfitting§Polynomial Regres", "local_index": 0}
{"chunk_id": "Lecture4_Overfitting_SVM_1", "doc_id": "Lecture4_Overfitting_SVM", "text": "e!\"!=ℎ\"(&!)(&!,\"!)\nNew Jersey Institute of Technology\nUnderfitting and Overfitting§Polynomial Regression with Degree = 4:§ℎ!\"=$\"+$#\"+$$\"$+$%\"%+$&\"&\n4COMP4434\nhttps://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html\n\nNew Jersey Institute of Technology\nUnderfitting and Overfitting\n5\nUnderfittinga model does not fit the data well enoughOverfittinga model is too closely fit to a limited set of data and lose generalization ability\nTwo classes separated by an el", "local_index": 1}
{"chunk_id": "Lecture4_Overfitting_SVM_2", "doc_id": "Lecture4_Overfitting_SVM", "text": " closely fit to a limited set of data and lose generalization ability\nTwo classes separated by an ellipticalarc\nNew Jersey Institute of Technology\nOverfitting§If we have too many features, the hypothesis may fit the training set very well, but fail to generalize to new examples (high variance)§More broadly, variance also represents how similar the results from a model will be, if it were fed different data from the same process§The bias error is from erroneous assumptions in the learning algorit", "local_index": 2}
{"chunk_id": "Lecture4_Overfitting_SVM_3", "doc_id": "Lecture4_Overfitting_SVM", "text": "rent data from the same process§The bias error is from erroneous assumptions in the learning algorithm§The variance error is from sensitivity to small fluctuations in the training setCOMP4434 6\nNew Jersey Institute of Technology\nExample in Logistic Regression\nUnderfittingHigh biasJust rightOverfittingHigh varianceCOMP4434 7\n\nNew Jersey Institute of Technology\nAddress Overfitting§Feature Reduction§Manual selecting which features to keep (by domain knowledge)§Okay esp. when some features are reall", "local_index": 3}
{"chunk_id": "Lecture4_Overfitting_SVM_4", "doc_id": "Lecture4_Overfitting_SVM", "text": "Manual selecting which features to keep (by domain knowledge)§Okay esp. when some features are really useless§Regularization§Keep all features, but reduce their influence by giving smaller values to the parameter !)§Okay when many features, each of which contributes a bit to predicting \"COMP4434 8\nNew Jersey Institute of Technology\nRegularized Linear Regression\n§Linear Regressionℎ!\"=$\"\"\"+$#\"#+⋯+$'\"''$\",$#,…=12,-()#\n*ℎ!(\"(())−1(()$\n§Regularized Linear Regression'$\",$#,…=12,-()#\n*ℎ!(\"(())−1(()$+2-", "local_index": 4}
{"chunk_id": "Lecture4_Overfitting_SVM_5", "doc_id": "Lecture4_Overfitting_SVM", "text": "\"''$\",$#,…=12,-()#\n*ℎ!(\"(())−1(()$\n§Regularized Linear Regression'$\",$#,…=12,-()#\n*ℎ!(\"(())−1(()$+2--)#\n'$-$\n§The value of the cost function isNOTequivalent to prediction error. Our goal is to make prediction errors on test data small9\nNew Jersey Institute of Technology\nUnderstanding\n§Penalized term: penalize large parameter values !*,1≤&≤'§Parameter (: control the tradeoff§Too small: degenerate to linear regression (overfitting)§Too large: penalize all features except $\", resulting in ℎ!\"=$\" (a", "local_index": 5}
{"chunk_id": "Lecture4_Overfitting_SVM_6", "doc_id": "Lecture4_Overfitting_SVM", "text": "o linear regression (overfitting)§Too large: penalize all features except $\", resulting in ℎ!\"=$\" (a horizontal line! underfitting)\n'$=12,-()#\n*ℎ!(\"(())−1(()$+2--)#\n'$-$\nCOMP4434 10\nNew Jersey Institute of Technology\nRegularized Gradient Descentℎ!\"=$\"\"\"+$#\"#+⋯+$'\"'\n'$=12,-()#\n*ℎ!(\"(())−1(()$+2--)#\n'$-$\n3'($)3$-=1,-()#\n*ℎ!\"(()−1(()\"-(()+2$-\nRepeat until convergence {$-=$-1−24,−4,-()#\n*ℎ!\"(()−1(()\"-(()\n} COMP4434 11\nNew Jersey Institute of Technology\n'$=12,-()#\n*ℎ!(\"(())−1(()$+2--)#\n'$-$\nTypes of ", "local_index": 6}
{"chunk_id": "Lecture4_Overfitting_SVM_7", "doc_id": "Lecture4_Overfitting_SVM", "text": "(()\n} COMP4434 11\nNew Jersey Institute of Technology\n'$=12,-()#\n*ℎ!(\"(())−1(()$+2--)#\n'$-$\nTypes of Regularization Regression\n12COMP4434\n'$=12,-()#\n*ℎ!\"(−1($+2--)#\n'|$-|\n§!+: Ridge Regression\n§!,: LASSO Regression\nLASSO regression results in sparse solutions – vector with more zero coordinates.Good for high-dimensional problems – don’t have to store all coordinates!Supplement Material: Visual for Ridge Vs. LASSO Regression https://www.youtube.com/watch?v=Xm2C_gTAl8c\nNew Jersey Institute of Techn", "local_index": 7}
{"chunk_id": "Lecture4_Overfitting_SVM_8", "doc_id": "Lecture4_Overfitting_SVM", "text": "Ridge Vs. LASSO Regression https://www.youtube.com/watch?v=Xm2C_gTAl8c\nNew Jersey Institute of Technology\nRegularized Logistic Regression§Logistic Regressionℎ\"&=-.#&#+.$&$+⋯+.%&%\n1.=−145&'$\n(\"(&)logℎ\"&(&)+1−\"(&)log1−ℎ\"&(&)\n§Regularized Logistic Regression./=−134!\"#\n$5!logℎ%:! +1−5!log1−ℎ%:! +<234&\"#\n'/&(\nCOMP4434 13\nNew Jersey Institute of Technology\nRegularized Gradient Descentℎ%:=>/):)+/#:#+⋯+/':'= 11+@*%!+!,%\"+\",⋯,%#+#\n./=−134!\"#\n$5(!)logℎ%:(!)+1−5(!)log1−ℎ%:(!) +<234&\"#\n'/&(\nA.(/)A/&=134!\"#\n", "local_index": 8}
{"chunk_id": "Lecture4_Overfitting_SVM_9", "doc_id": "Lecture4_Overfitting_SVM", "text": ":'= 11+@*%!+!,%\"+\",⋯,%#+#\n./=−134!\"#\n$5(!)logℎ%:(!)+1−5(!)log1−ℎ%:(!) +<234&\"#\n'/&(\nA.(/)A/&=134!\"#\n$ℎ%:(!)−5(!):&(!)+</&\nRepeat until convergence {$-=$-1−24,−4,-()#\n*ℎ!\"(()−1(()\"-(()\n} COMP4434 14\nNew Jersey Institute of Technology\nValidation set§Task: Given data (X,Y) build a model f() to predict Y’ based on X’§Strategy: Estimate 6\t=\t89 on (:,;)Hope that the same 8(9) also works to predict unknown ;’§The “hope” is called generalization§Overfitting: If f(x) predicts well Y but is unable to pred", "local_index": 9}
{"chunk_id": "Lecture4_Overfitting_SVM_10", "doc_id": "Lecture4_Overfitting_SVM", "text": "wn ;’§The “hope” is called generalization§Overfitting: If f(x) predicts well Y but is unable to predict Y’§We want to build a model that generalizes well to unseen data§Solution: k-fold Cross-validation\n15COMP4434\nX Y\nX’ Y’Test data\nTrainingdata\nX Y\nX’\nValidation set\nTrainingset\nTest set\nNew Jersey Institute of Technology\nk-fold Cross-validation\n§The original sample is randomly partitioned into k equal sized subsamples§Of the k subsamples, a single subsample is retained as the validation data fo", "local_index": 10}
{"chunk_id": "Lecture4_Overfitting_SVM_11", "doc_id": "Lecture4_Overfitting_SVM", "text": "equal sized subsamples§Of the k subsamples, a single subsample is retained as the validation data for testing the model§The remaining k − 1 subsamples are used as training data§The cross-validation process is then repeated k times, with each of the k subsamples used exactly once as the validation data§The k results can then be averaged to produce a single estimation16COMP4434\nValidation\nNew Jersey Institute of Technology\nLeave-one-out Cross-validation\n§When k = n (the number of observations), k-", "local_index": 11}
{"chunk_id": "Lecture4_Overfitting_SVM_12", "doc_id": "Lecture4_Overfitting_SVM", "text": " Institute of Technology\nLeave-one-out Cross-validation\n§When k = n (the number of observations), k-fold cross-validation is equivalent to leave-one-out cross-validation17\nValidation\nValidation\nk-fold cross-validation\nleave-one-out cross-validation\nCOMP4434\nNew Jersey Institute of Technology\nBoston Housing (has an ethical problem)\n18\nThe Boston Housing Dataset consists of price of houses in various places in Boston. The Boston Housing Dataset has 506 cases. There are13Features in each case of th", "local_index": 12}
{"chunk_id": "Lecture4_Overfitting_SVM_13", "doc_id": "Lecture4_Overfitting_SVM", "text": "s places in Boston. The Boston Housing Dataset has 506 cases. There are13Features in each case of the dataset. Alongside with price, the dataset also provide information such as Crime (CRIM), areas of non-retail business in the town (INDUS), the age of people who own the house (AGE), and there are many other attributes.\nCRIMZNINDUSCHASNOXRMAGEDISRADTAXPTRATIOBLSTSTPrice\n0.00618.02.310.00.5386.57565.24.0901.0296.015.3396.94.9824.0\n0.0270 7.070.00.4696.42178.94.9672.0242.017.8396.99.1421.6\n… … … …", "local_index": 13}
{"chunk_id": "Lecture4_Overfitting_SVM_14", "doc_id": "Lecture4_Overfitting_SVM", "text": "5.24.0901.0296.015.3396.94.9824.0\n0.0270 7.070.00.4696.42178.94.9672.0242.017.8396.99.1421.6\n… … … … … … … … … … … … … …\nfrom sklearn.datasets import load_bostonboston_dataset = load_boston() \nCOMP4434\nNew Jersey Institute of Technology\nGenerate Training Data\n19\nBoston Housing DataSplitdataset \nPlotfigure\nZN: Proportion of residential land zoned for lots over 25,000 sq. fthttps://www.kaggle.com/tolgahancepel/boston-housing-regression-analysisCOMP4434\nNew Jersey Institute of Technology\nBuild Mode", "local_index": 14}
{"chunk_id": "Lecture4_Overfitting_SVM_15", "doc_id": "Lecture4_Overfitting_SVM", "text": "lgahancepel/boston-housing-regression-analysisCOMP4434\nNew Jersey Institute of Technology\nBuild Model\n20\nCrossvalidationTrainmodel\nℎ\"&=.#+.$&$+ .+&++⋯\t+ .$,&$,COMP4434\nNew Jersey Institute of Technology\nRegularization\n21\n= MSE0 37.1318 (overfitting)10029.9057100032.8280 (underfitting)\nThe magnitudes of coefficient indices 4,5,6 are considerably reduced after regularization with α = 100, resulting in lower mean square error\nCHAS\nNOX\nRM\nCOMP4434\nNew Jersey Institute of Technology\nHistory of Suppor", "local_index": 15}
{"chunk_id": "Lecture4_Overfitting_SVM_16", "doc_id": "Lecture4_Overfitting_SVM", "text": "in lower mean square error\nCHAS\nNOX\nRM\nCOMP4434\nNew Jersey Institute of Technology\nHistory of Support Vector Machines§SVM was first introduced in 1992 [1] §SVM becomes popular because of its success in handwritten digit recognition §1.1% test error rate for SVM. This is the same as the error rates of a carefully constructed neural network, LeNet 4.§See Section 5.11 in [2] or the discussion in [3] for details\n[1] B.E. Boser et al. A Training Algorithm for Optimal Margin Classifiers. Proceedings o", "local_index": 16}
{"chunk_id": "Lecture4_Overfitting_SVM_17", "doc_id": "Lecture4_Overfitting_SVM", "text": "for details\n[1] B.E. Boser et al. A Training Algorithm for Optimal Margin Classifiers. Proceedings of the Fifth Annual Workshop on Computational Learning Theory 5 144-152, Pittsburgh, 1992. [2] L. Bottou et al.  Comparison of classifier methods: a case study in handwritten digit recognition. Proceedings of the 12th IAPR International Conference on Pattern Recognition, vol. 2, pp. 77-82.[3] V. Vapnik. The Nature of Statistical Learning Theory. 2nd edition, Springer, 1999.22COMP4434\nNew Jersey Ins", "local_index": 17}
{"chunk_id": "Lecture4_Overfitting_SVM_18", "doc_id": "Lecture4_Overfitting_SVM", "text": "ik. The Nature of Statistical Learning Theory. 2nd edition, Springer, 1999.22COMP4434\nNew Jersey Institute of Technology\nSupport Vector Machines§Want to separate “+” from “-” using a line+++\n+++ ---- ---Which is best linear separator (defined by w)?\nwx +b = 0\n23COMP4434\nData:§Training examples: §(x(1), y1) … (x(m), ym)§Each example i:§x(i) = ( x1(i),… , xd(i) )§xj(i) is real valued§yi Î { -1, +1 }\nNew Jersey Institute of Technology\n++\n++\n+++\n+\n+ ----- ---\n-A\nB\nC\nLargest Margin§Distance from the ", "local_index": 18}
{"chunk_id": "Lecture4_Overfitting_SVM_19", "doc_id": "Lecture4_Overfitting_SVM", "text": "\nNew Jersey Institute of Technology\n++\n++\n+++\n+\n+ ----- ---\n-A\nB\nC\nLargest Margin§Distance from the separating hyperplane corresponds to the “confidence” of prediction§Example:§We are more confident about the class of A and B than of C \n24COMP4434\nNew Jersey Institute of Technology\nLargest Margin§Margin > (gamma): Distance of closest example from the decision line/hyperplane\nThe reason we define margin this way is due to theoretical convenience and existence of generalization error bounds that d", "local_index": 19}
{"chunk_id": "Lecture4_Overfitting_SVM_20", "doc_id": "Lecture4_Overfitting_SVM", "text": "argin this way is due to theoretical convenience and existence of generalization error bounds that depend on the value of margin.25\n! !\nCOMP4434\nNew Jersey Institute of Technology\nDistance from a point to a linedenotes +1denotes -1 ): wx +b = 0X – VectorW – Normal Vectorb  – Scale ValueW\n§What is the distance expression for a point 9\" to a line wx+b= 0?\n§http://mathworld.wolfram.com/Point-LineDistance2-Dimensional.html26\n<latexit sha1_base64=\"JWmQgQVG0fyIh33cS2dqoIiyt7U=\">AAACcXicjVFPSxwxHM2Mtur", "local_index": 20}
{"chunk_id": "Lecture4_Overfitting_SVM_21", "doc_id": "Lecture4_Overfitting_SVM", "text": "ance2-Dimensional.html26\n<latexit sha1_base64=\"JWmQgQVG0fyIh33cS2dqoIiyt7U=\">AAACcXicjVFPSxwxHM2Mtura1lV7EVGCS8EiLDOL/y4LohePFroq7OwOmUxGg5k/Jr9Rl0zufr7e/BJe/AJm10Fa7cEHgZf3fo8kL1EhuALPe3DcqelPn2dm5xrzX75+W2guLp2qvJSU9WgucnkeEcUEz1gPOAh2XkhG0kiws+jqaOyf3TCpeJ79hlHBBim5yHjCKQErhc37eFMHUYLvTOj9xF0cJJJQXb1qAY1zwJPtrdmKKqMDdS1BV1WtVdWwE3aM6X40GagyDTXv+maoY4NvQz608UbYbHltbwL8nvg1aaEaJ2HzTxDntExZBlQQpfq+V8BAEwmcCmYaQalYQegVuWB9SzOSMjXQk8YM/mGVGCe5tCsDPFH/TmiSKjVKIzuZErhUb72x+D+vX0KyP9A8K0pgGX05KCkFhhy", "local_index": 21}
{"chunk_id": "Lecture4_Overfitting_SVM_22", "doc_id": "Lecture4_Overfitting_SVM", "text": "+V8BAEwmcCmYaQalYQegVuWB9SzOSMjXQk8YM/mGVGCe5tCsDPFH/TmiSKjVKIzuZErhUb72x+D+vX0KyP9A8K0pgGX05KCkFhhyP68cxl4yCGFlCqOT2rpheEtsc2E8al+C/ffJ7ctpp+7vtnV/brYPDuo5ZtIo20Cby0R46QMfoBPUQRY/Od2fNWXee3BUXuxsvo65TZ5bRP3C3ngGvwb5i</latexit>\nd ( x 0 )=\n| x 0 · w + b |\np\n|| w || 2\n2\n=\n| x 0 · w + b |\nq\nP d\ni =1\nw 2\ni\nNew Jersey Institute of Technology\nDistance from a point to a line (method 2)\n27\n§                                    !⋅#=!⋅#⋅cos(\n*+,-!\n*\n.COMP4434\nCSCE 666 Pattern Analysis | Ricardo Gutierrez -", "local_index": 22}
{"chunk_id": "Lecture4_Overfitting_SVM_23", "doc_id": "Lecture4_Overfitting_SVM", "text": "                      !⋅#=!⋅#⋅cos(\n*+,-!\n*\n.COMP4434\nCSCE 666 Pattern Analysis | Ricardo Gutierrez -Osuna | CSE@TAMU  16  \n(Distance between a plane and a point)  \n \nx1 \nx2 \nA \nB \nw\n\n\nAB\nθ\nN \n   \nw\nbxw\nw\nxwxw\nw\nw,wxx,xx\nw\nwBA\nwAB\nwBA\nABcos θABAN\nA\nT\nb\nB\nT\nA\nT\n21\nT\n2B2A1B1A\n\n\n\n\n\n\n\n\n\n\n\n→→ →→→→→ →\n<latexit sha1_base64=\"bUAARbAUedxeL0Sa51H+AKhIjfY=\">AAACL3icbVDLSgMxFM34rPVVdekmWJQKtswUXxuhVhCXFewDOnXIpBkNzTxIMtYynT9y4690I6KIW//CTFtBWw8k93DOvST32AGjQur6qzYzOze/sJhaSi+vrK6tZz", "local_index": 23}
{"chunk_id": "Lecture4_Overfitting_SVM_24", "doc_id": "Lecture4_Overfitting_SVM", "text": "kmWJQKtswUXxuhVhCXFewDOnXIpBkNzTxIMtYynT9y4690I6KIW//CTFtBWw8k93DOvST32AGjQur6qzYzOze/sJhaSi+vrK6tZzY2a8IPOSZV7DOfN2wkCKMeqUoqGWkEnCDXZqRudy4Sv/5AuKC+dyN7AWm56M6jDsVIKsnKXJ7BPdPhCEe5RysqGzHMQ0XOjfggqeXij1CM929N6Qcwl+9axgFUd3E/jsx+1+zHaSuT1Qv6EHCaGGOSBWNUrMzAbPs4dIknMUNCNA09kK0IcUkxI3HaDAUJEO6gO9JU1EMuEa1ouG8Md5XSho7P1fEkHKq/JyLkCtFzbdXpInkvJr1E/M9rhtI5bUXUC0JJPDx6yAkZlD5MwoNtygmWrKcIwpyqv0J8j1R6UkWchGBMrjxNasWCcVw4uj7MlsrjOFJgG+yAHDDACSiBK1ABVYDBExiAN/CuPWsv2of2OWqd0cYzW+APtK9vfnOlvw==</latexit>", "local_index": 24}
{"chunk_id": "Lecture4_Overfitting_SVM_25", "doc_id": "Lecture4_Overfitting_SVM", "text": "WchGBMrjxNasWCcVw4uj7MlsrjOFJgG+yAHDDACSiBK1ABVYDBExiAN/CuPWsv2of2OWqd0cYzW+APtK9vfnOlvw==</latexit>\n=\n( x B 1 \u0000 x A 1 ,x B 2 \u0000 x A 2 ) >\n( \u0000 w 1 , \u0000 w 2 )\nk w k\nNew Jersey Institute of TechnologyCSCE 666 Pattern Analysis | Ricardo Gutierrez -Osuna | CSE@TAMU  22 \nSupport Vectors \n• The KKT complementary condition states that, for every point in \nthe training set, the following equality must hold  \n𝛼௜ 𝑦௜ 𝑤்𝑥௜ + 𝑏 − 1 = 0   ∀𝑖 = 1. . 𝑁 \n– Therefore, ∀𝑥, either 𝛼௜ = 0 or 𝑦௜(𝑤்𝑥௜ + 𝑏 − 1) = 0 must ", "local_index": 25}
{"chunk_id": "Lecture4_Overfitting_SVM_26", "doc_id": "Lecture4_Overfitting_SVM", "text": " \n𝛼௜ 𝑦௜ 𝑤்𝑥௜ + 𝑏 − 1 = 0   ∀𝑖 = 1. . 𝑁 \n– Therefore, ∀𝑥, either 𝛼௜ = 0 or 𝑦௜(𝑤்𝑥௜ + 𝑏 − 1) = 0 must hold \n• Those points for which 𝛼௜ > 0 must then lie on one of the two hyperplanes \nthat define the largest margin (the term 𝑦௜(𝑤்𝑥௜ + 𝑏 − 1) becomes zero only \nat these hyperplanes) \n• These points are known as the Support Vectors \n• All the other points must have 𝛼௜ = 0  \n– Note that only the SVs contribute to defining the optimal hyperplane  \nడ௃ ௪,௕,ఈ\nడ௪ = 0 ⇒ 𝑤 = ∑ 𝛼௜𝑦௜𝑥௜\nே\n௜ୀଵ   \n– NOTE: the b", "local_index": 26}
{"chunk_id": "Lecture4_Overfitting_SVM_27", "doc_id": "Lecture4_Overfitting_SVM", "text": "ontribute to defining the optimal hyperplane  \nడ௃ ௪,௕,ఈ\nడ௪ = 0 ⇒ 𝑤 = ∑ 𝛼௜𝑦௜𝑥௜\nே\n௜ୀଵ   \n– NOTE: the bias term 𝑏 is found from  \nthe KKT complementary condition  \non the support vectors  \n– Therefore, the complete dataset could  \nbe replaced by only the support vectors,  \nand the separating hyperplane would be  \nthe same \nx1 \nx2 \nSupport  \nVectors (>0) \nLinear SVM Mathematically§Let training set {(x(i), yi)}i=1..n, x(i)ÎRd, yi Î {-1, 1} be separated by a hyperplane with margin !. Then for each tr", "local_index": 27}
{"chunk_id": "Lecture4_Overfitting_SVM_28", "doc_id": "Lecture4_Overfitting_SVM", "text": "x(i), yi)}i=1..n, x(i)ÎRd, yi Î {-1, 1} be separated by a hyperplane with margin !. Then for each training example (x(i), yi):\n§For every support vector x(s) the above inequality is an equality.    After rescaling w and b by @/2 in the equality, we obtain that distance between each x(s) and the hyperplane is \nwTx(i) + b ≤ - @/2    if yi = -1wTx(i) + b ≥ @/2    if yi = 1 yi(wTx(i) + b) ≥ @/2Û\n28COMP4434\n<latexit sha1_base64=\"L1aDq/mh6GH0f2ataeshXc8x1tw=\">AAACN3icbVBLSwMxGMz6rPW16tFLsAgtQtkVXxeh6M", "local_index": 28}
{"chunk_id": "Lecture4_Overfitting_SVM_29", "doc_id": "Lecture4_Overfitting_SVM", "text": "P4434\n<latexit sha1_base64=\"L1aDq/mh6GH0f2ataeshXc8x1tw=\">AAACN3icbVBLSwMxGMz6rPW16tFLsAgtQtkVXxeh6MWTVLAP6G5LNs22odkHSVYtu/uvvPg3vOnFgyJe/Qem7R60dSAwmfmG5BsnZFRIw3jR5uYXFpeWcyv51bX1jU19a7sugohjUsMBC3jTQYIw6pOapJKRZsgJ8hxGGs7gcuQ37ggXNPBv5TAktod6PnUpRlJJHf3acjnC8bAjirHluPA+bVsyCOH48pC246IopQdOKY2TJBtIkhSew0nOnNI7esEoG2PAWWJmpAAyVDv6s9UNcOQRX2KGhGiZRijtGHFJMSNp3ooECREeoB5pKeojjwg7Hu+dwn2ldKEbcHV8Ccfq70SMPCGGnqMmPST7Ytobif95rUi6Z3ZM/TCSxMeTh9yIQRnAUYmwSznBkg0VQZhT9VeI+0gVIlXVeVWCOb3yLKkfls2T8vHNUa", "local_index": 29}
{"chunk_id": "Lecture4_Overfitting_SVM_30", "doc_id": "Lecture4_Overfitting_SVM", "text": "fq70SMPCGGnqMmPST7Ytobif95rUi6Z3ZM/TCSxMeTh9yIQRnAUYmwSznBkg0VQZhT9VeI+0gVIlXVeVWCOb3yLKkfls2T8vHNUaFykdWRA7tgDxSBCU5BBVyBKqgBDB7BK3gHH9qT9qZ9al+T0Tkty+yAP9C+fwBogK18</latexit>\ny s ( w\n>\nx\n( s )\n+ b )\n|| w ||\n=\n1\n|| w ||\nNew Jersey Institute of Technology\nLinear Support Vector Machine (SVM)\n29COMP4434\nB1\nb11\nb12\n||||\n2\n Margin\nw\n=\nwTx + b = 0wTx + b = -1 wTx + b = 1\nyi = A−1,\tif wTx(i) + b ≤ −11,\tif wTx(i) + b ≥ 1\nNew Jersey Institute of Technology\nLinear SVM Mathematically (cont.)§Then the mar", "local_index": 30}
{"chunk_id": "Lecture4_Overfitting_SVM_31", "doc_id": "Lecture4_Overfitting_SVM", "text": "\tif wTx(i) + b ≥ 1\nNew Jersey Institute of Technology\nLinear SVM Mathematically (cont.)§Then the margin can be expressed through (rescaled) w and b as:\n§Then we can formulate the quadratic optimization problem:\n§Which can be reformulated as: \nw22==rr\n30\nFind w and b such that                is maximized and for all (x(i), yi), i =1 ... m :     yi(wTx(i) + b) ≥ 1w2=rFind w and b such thatΦ(w) = ||w||2=wTw  is minimized and for all (x(i), yi), i=1 ... m :    yi (wTx(i) + b) ≥ 1\nNew margin ;′\nC′\nCO", "local_index": 31}
{"chunk_id": "Lecture4_Overfitting_SVM_32", "doc_id": "Lecture4_Overfitting_SVM", "text": "||2=wTw  is minimized and for all (x(i), yi), i=1 ... m :    yi (wTx(i) + b) ≥ 1\nNew margin ;′\nC′\nCOMP4434\nNew Jersey Institute of Technology\nWhat if the problem is not linearly separable\n31COMP4434\nNew Jersey Institute of Technology\nSVM with soft margin§Need to minimize:§ subject to:\n32\n<latexit sha1_base64=\"xzJdFV0IceMpTtYyE3BoV7c5bYk=\">AAACKnicbZDLSsNAFIYn9V5vVZduBotQEUpSvG0ELxuXFawtNG2YTCft4EwSZk7UEvM8bnwVNy6U4tYHcXpZqPWHgY//nMOZ8/ux4Bpse2DlZmbn5hcWl/LLK6tr64WNzVsdJYqyGo1EpBo+0UzwkNWAg2CNWDE", "local_index": 32}
{"chunk_id": "Lecture4_Overfitting_SVM_33", "doc_id": "Lecture4_Overfitting_SVM", "text": "SZk7UEvM8bnwVNy6U4tYHcXpZqPWHgY//nMOZ8/ux4Bpse2DlZmbn5hcWl/LLK6tr64WNzVsdJYqyGo1EpBo+0UzwkNWAg2CNWDEifcHq/t3lsF6/Z0rzKLyBfsxaknRDHnBKwFhe4dwNFKGpk6WV7Okpdf0APxhoV/A+vsSuYAGUsKsT6aX81MnaEruP3OPtSh67ind7sOcVinbZHglPgzOBIpqo6hXe3E5EE8lCoIJo3XTsGFopUcCpYFneTTSLCb0jXdY0GBLJdCsdnZrhXeN0cBAp80LAI/fnREqk1n3pm05JoKf/1obmf7VmAsFJK+VhnAAL6XhRkAgMER7mhjtcMQqib4BQxc1fMe0Rkx2YdPMmBOfvydNwWyk7R+XD64Pi2cUkjkW0jXZQCTnoGJ2hK1RFNUTRM3pF7+jDerHerIH1OW7NWZOZLfRL1tc3dF6mBg==</latexit>\n1\n2\n|| w ||\n2\n+ C\n \nm\nX\ni =1\n⇠\n2", "local_index": 33}
{"chunk_id": "Lecture4_Overfitting_SVM_34", "doc_id": "Lecture4_Overfitting_SVM", "text": "QCTnoGJ2hK1RFNUTRM3pF7+jDerHerIH1OW7NWZOZLfRL1tc3dF6mBg==</latexit>\n1\n2\n|| w ||\n2\n+ C\n \nm\nX\ni =1\n⇠\n2\ni\n!\nwTx(i) + b ≤ -1+B(    if yi = -1wTx(i) + b ≥ 1−B(\tif yi = 1             B(≥ 0\nCOMP4434\nNew Jersey Institute of Technology\nCharacteristics of SVM§The learning problem is formulated as a convex optimization problem§Efficient algorithms are available to find the global minima §High computational complexity for building the model§Robust to noise§Overfitting is handled by maximizing the margin of ", "local_index": 34}
{"chunk_id": "Lecture4_Overfitting_SVM_35", "doc_id": "Lecture4_Overfitting_SVM", "text": "omplexity for building the model§Robust to noise§Overfitting is handled by maximizing the margin of the decision boundary§SVM can handle irrelevant and redundant attributes better than many other techniques§The user needs to provide the type of kernel function & cost function (for nonlinear SVM)§Difficult to handle missing values33COMP4434", "local_index": 35}
{"chunk_id": "Lecture5_Clustering_Recommender_Systems_0", "doc_id": "Lecture5_Clustering_Recommender_Systems", "text": "COMP4434 Big Data AnalyticsLecture 5 Clustering &Recommender SystemsHUANG Xiaoxiaohuang@comp.polyu.edu.hk\n\nNew Jersey Institute of Technology\n§Cluster: A collection of data objects§similar (or related) to one another within the same group§dissimilar (or unrelated) to the objects in other groups§Cluster analysis (or clustering, data segmentation, …)§Finding similarities between data according to the characteristics found in the data and grouping similar data objects into clusters§Unsupervised lea", "local_index": 0}
{"chunk_id": "Lecture5_Clustering_Recommender_Systems_1", "doc_id": "Lecture5_Clustering_Recommender_Systems", "text": "e characteristics found in the data and grouping similar data objects into clusters§Unsupervised learning: no predefined classes (i.e., learning by observations vs. learning by examples: supervised)§Typical applications§As a stand-alone tool to get insight into data distribution §As a preprocessing step for other algorithms\nWhat is Cluster Analysis?\n3COMP4434\nNew Jersey Institute of Technology\nGiven a cloud of data points we want to understand its structure.\n4\nNew Jersey Institute of Technology\n", "local_index": 1}
{"chunk_id": "Lecture5_Clustering_Recommender_Systems_2", "doc_id": "Lecture5_Clustering_Recommender_Systems", "text": "en a cloud of data points we want to understand its structure.\n4\nNew Jersey Institute of Technology\nDocument Clustering\n5COMP4434\n\nNew Jersey Institute of Technology\nClustering for Data Understanding & Applications§Customer Segmentation: Businesses use clustering to group customers with similar purchasing behavior. This helps in targeted marketing, personalized recommendations, and product/service customization§Image Segmentation: In computer vision, clustering is used to segment images into reg", "local_index": 2}
{"chunk_id": "Lecture5_Clustering_Recommender_Systems_3", "doc_id": "Lecture5_Clustering_Recommender_Systems", "text": " customization§Image Segmentation: In computer vision, clustering is used to segment images into regions with similar features. This is useful in object detection, image recognition§Anomaly Detection: Clustering can help identify outliers or anomalies in datasets. This is crucial in fraud detection, network security, and quality control§Social Network Analysis: Clustering can group users with similar connections or behavior in social networks. This is used for community detection and influence a", "local_index": 3}
{"chunk_id": "Lecture5_Clustering_Recommender_Systems_4", "doc_id": "Lecture5_Clustering_Recommender_Systems", "text": "lar connections or behavior in social networks. This is used for community detection and influence analysis6COMP4434\nNew Jersey Institute of Technology\nClustering as a Preprocessing Tool (Utility)§Summarization: §Preprocessing for regression, classification§Compression:§Image processing: vector quantization§Finding K-nearest Neighbors:§Localizing search to one or a small number of clusters§Outlier detection:§Outliers are often viewed as those “far away” from any cluster\n7COMP4434\nNew Jersey Inst", "local_index": 4}
{"chunk_id": "Lecture5_Clustering_Recommender_Systems_5", "doc_id": "Lecture5_Clustering_Recommender_Systems", "text": " detection:§Outliers are often viewed as those “far away” from any cluster\n7COMP4434\nNew Jersey Institute of Technology\nx        xx  x      x  xx   x x  x     x     x  xx   x\nxxx    xx  x        x    x  x   xx x   xx\nx\nx\nx        xx  x      x  xx   x x  x     x     x  xx   x\nxxx    xx  x        x    x  x   xx x   xx\nx   xx  x    x    x  x    x     xx  Outlier Cluster\n8COMP4434\nExample: Clusters & Outliers\nNew Jersey Institute of Technology\nProblem definition of clustering§Given a set of points, ", "local_index": 5}
{"chunk_id": "Lecture5_Clustering_Recommender_Systems_6", "doc_id": "Lecture5_Clustering_Recommender_Systems", "text": "Outliers\nNew Jersey Institute of Technology\nProblem definition of clustering§Given a set of points, with a notion of distance between points, group the points into some number of clusters, so that §Members of a cluster are close/similar to each other§Members of different clusters are dissimilar§Usually: §Points are in a high-dimensional space§Similarity is defined using a distance measure§Euclidean, Cosine, Jaccard, edit distance, …9COMP4434\nNew Jersey Institute of Technology\nClustering Problem:", "local_index": 6}
{"chunk_id": "Lecture5_Clustering_Recommender_Systems_7", "doc_id": "Lecture5_Clustering_Recommender_Systems", "text": "n, Cosine, Jaccard, edit distance, …9COMP4434\nNew Jersey Institute of Technology\nClustering Problem: Galaxies§A catalog of 2 billion “sky objects” represents objects by their radiation in 7 dimensions (frequency bands)§Problem: Cluster into similar objects, e.g., galaxies, nearby stars, quasars, etc.§Sloan Digital Sky Survey\n10COMP4434\nNew Jersey Institute of Technology\nClustering is a hard problem!§Clustering in two dimensions looks easy§Clustering small amounts of data looks easy§Many applicat", "local_index": 7}
{"chunk_id": "Lecture5_Clustering_Recommender_Systems_8", "doc_id": "Lecture5_Clustering_Recommender_Systems", "text": "m!§Clustering in two dimensions looks easy§Clustering small amounts of data looks easy§Many applications involve not 2, but 10 or 10,000 dimensions§High-dimensional spaces look different: Almost all pairs of points are at about the same distance\n11COMP4434\nNew Jersey Institute of Technology\nClustering Problem: Music§Intuitively: Music divides into categories, and customers prefer a few categories§But what are categories really?§Represent a song by a set of customers who like it§Similar songs hav", "local_index": 8}
{"chunk_id": "Lecture5_Clustering_Recommender_Systems_9", "doc_id": "Lecture5_Clustering_Recommender_Systems", "text": "But what are categories really?§Represent a song by a set of customers who like it§Similar songs have similar sets of customers, and vice-versa\n12COMP4434\nNew Jersey Institute of Technology\nClustering Problem: MusicSpace of all songs:§Think of a space with one dimension for each customer§Values in a dimension may be 0 or 1 only§A song is a point in this space (x1, x2,…, xd), where xi = 1 iff the i th customer bought the CD§For Spotify:§Spotify lets you discover, organize, and share over 100 mill", "local_index": 9}
{"chunk_id": "Lecture5_Clustering_Recommender_Systems_10", "doc_id": "Lecture5_Clustering_Recommender_Systems", "text": " th customer bought the CD§For Spotify:§Spotify lets you discover, organize, and share over 100 million songs, over 5 million podcast titles and 350,000+ audiobooks§In 2023, Spotify has 551 million users and 220 million premium subscribers across 184 regions§Task: Find clusters of similar songs13COMP4434\nNew Jersey Institute of Technology\nFinding topics:§Represent a document by a vector  (x1, x2,…, xd), where xi = 1 iff the i th word (in some order) appears in the document§It actually doesn’t ma", "local_index": 10}
{"chunk_id": "Lecture5_Clustering_Recommender_Systems_11", "doc_id": "Lecture5_Clustering_Recommender_Systems", "text": ", xd), where xi = 1 iff the i th word (in some order) appears in the document§It actually doesn’t matter if d is infinite; i.e., we don’t limit the set of words§Documents with similar sets of words may be about the same topic\n14COMP4434\nClustering Problem: Documents\nNew Jersey Institute of Technology\nSimilarity is defined using a distance measure§Sets as vectors: §Measure similarity by the cosine distance\ncosine distance = 1 - cosine similarity§Measure similarity by Euclidean distance\n§Sets as s", "local_index": 11}
{"chunk_id": "Lecture5_Clustering_Recommender_Systems_12", "doc_id": "Lecture5_Clustering_Recommender_Systems", "text": "distance\ncosine distance = 1 - cosine similarity§Measure similarity by Euclidean distance\n§Sets as sets: §Measure similarity by the Jaccard distance15COMP4434\nd ( A , B )=\nv\nu\nu\nt\nn\nX\ni =1\n( B i \u0000 A i ) 2\n<latexit sha1_base64=\"xjBhKfsHgAHig1oUBwYBX6xktYw=\">AAACHnicbVDLSsNAFJ3UV62vqEs3g0WooCURRTcFrRuXCrYKTRomk4kOzkzizEQoIV/ixl9x40IRwZX+jdOahVoPXO7hnHuZuSdMGVXacT6tysTk1PRMdbY2N7+wuGQvr3RVkklMOjhhibwMkSKMCtLRVDNymUqCeMjIRXhzPPQv7ohUNBHnepASn6MrQWOKkTZSYO9FjdwLY3hUbI16u9iELU/dSp17KuNBTltu0Rew0Q4o3IZ", "local_index": 12}
{"chunk_id": "Lecture5_Clustering_Recommender_Systems_13", "doc_id": "Lecture5_Clustering_Recommender_Systems", "text": "MkSKMCtLRVDNymUqCeMjIRXhzPPQv7ohUNBHnepASn6MrQWOKkTZSYO9FjdwLY3hUbI16u9iELU/dSp17KuNBTltu0Rew0Q4o3IZHAd3s7xSBXXeazghwnLglqYMSp4H97kUJzjgRGjOkVM91Uu3nSGqKGSlqXqZIivANuiI9QwXiRPn56LwCbhglgnEiTQkNR+rPjRxxpQY8NJMc6Wv11xuK/3m9TMcHfk5Fmmki8PdDccagTuAwKxhRSbBmA0MQltT8FeJrJBHWJtGaCcH9e/I46e403d3m3tlu/bBdxlEFa2AdNIAL9sEhOAGnoAMwuAeP4Bm8WA/Wk/VqvX2PVqxyZxX8gvXxBZeFoEg=</latexit>\nNew Jersey Institute of Technology\nJaccard similarity§The Jaccard similarity of two sets is the size of their intersection divid", "local_index": 13}
{"chunk_id": "Lecture5_Clustering_Recommender_Systems_14", "doc_id": "Lecture5_Clustering_Recommender_Systems", "text": "nology\nJaccard similarity§The Jaccard similarity of two sets is the size of their intersection divided by the size of their union:sim(C1, C2) = |C1ÇC2|/|C1ÈC2|§Jaccard distance: d(C1, C2) = 1 - |C1ÇC2|/|C1ÈC2|\n§Document D1 is a set of its ! words§Equivalently, each document is a 0/1 vector in the space of k words§Each unique word is a dimension§Vectors are very sparse\n3 in intersection8 in unionJaccard similarity= 3/8Jaccard distance = 5/8\nCOMP4434 16\nNew Jersey Institute of Technology\nk-means C", "local_index": 14}
{"chunk_id": "Lecture5_Clustering_Recommender_Systems_15", "doc_id": "Lecture5_Clustering_Recommender_Systems", "text": "ccard similarity= 3/8Jaccard distance = 5/8\nCOMP4434 16\nNew Jersey Institute of Technology\nk-means Clustering Algorithm\n17COMP4434\n§Partitioning method: Partitioning n objects into a set of k clusters, such that the sum of squared distances is minimized (where ci is the centroid or clustroid of cluster Ci)\n§Given k, find a partition of k clusters that optimizes the chosen partitioning criterion§Global optimal: exhaustively enumerate all partitions§Heuristic methods: k-means and k-medoids algorit", "local_index": 15}
{"chunk_id": "Lecture5_Clustering_Recommender_Systems_16", "doc_id": "Lecture5_Clustering_Recommender_Systems", "text": "obal optimal: exhaustively enumerate all partitions§Heuristic methods: k-means and k-medoids algorithms§k-means (MacQueen’67, Lloyd’57/’82): Each cluster is represented by the center of the cluster§k-medoids or PAM (Partition around medoids) (Kaufman & Rousseeuw’87): Each cluster is represented by one of the objects in the cluster  \n21 )(iCpki cpEi-SS=Î=\nNew Jersey Institute of Technology\nk-means Clustering Algorithm§Assumes Euclidean space/distance§Start by picking k, the number of clusters§Ini", "local_index": 16}
{"chunk_id": "Lecture5_Clustering_Recommender_Systems_17", "doc_id": "Lecture5_Clustering_Recommender_Systems", "text": "Clustering Algorithm§Assumes Euclidean space/distance§Start by picking k, the number of clusters§Initialize clusters by picking one point per cluster§Example: Pick one point at random, then  k-1 other points, each as far away as possible from the previous points\n18COMP4434\nNew Jersey Institute of Technology19\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0 1 2 3 4 5 6 7 8 9 10 012345678910\n012345678910\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0 1 2 3 4 5 6 7 8 9 10\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0 1 2 3 4 5 6 7 8 9 10\n012345678910\n012345678910", "local_index": 17}
{"chunk_id": "Lecture5_Clustering_Recommender_Systems_18", "doc_id": "Lecture5_Clustering_Recommender_Systems", "text": "\n9\n10\n0 1 2 3 4 5 6 7 8 9 10\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0 1 2 3 4 5 6 7 8 9 10\n012345678910\n012345678910\nk=2Arbitrarily choose k means\nAssign each objects to most similar center\nUpdate the cluster means\nUpdate the cluster means\nreassignreassign\nDemo\nCOMP4434\nNew Jersey Institute of Technology\nPopulating Clusters§1) For each point, place it in the cluster whose current centroid it is nearest§2) After all points are assigned, update the locations of centroids of the k clusters§3) Reassign all points t", "local_index": 18}
{"chunk_id": "Lecture5_Clustering_Recommender_Systems_19", "doc_id": "Lecture5_Clustering_Recommender_Systems", "text": "ll points are assigned, update the locations of centroids of the k clusters§3) Reassign all points to their closest centroid§Sometimes moves points between clusters§Repeat 2 and 3 until convergence§Convergence: Points don’t move between clusters and centroids stabilize20COMP4434\nNew Jersey Institute of Technology21\nInitialization of k-means§The way to initialize the centroids was not specified. One popular way to start is to randomly choose k of the examples§The results produced depend on the in", "local_index": 19}
{"chunk_id": "Lecture5_Clustering_Recommender_Systems_20", "doc_id": "Lecture5_Clustering_Recommender_Systems", "text": "e popular way to start is to randomly choose k of the examples§The results produced depend on the initial values for the centroids, and it frequently happens that suboptimal partitions are found. The standard solution is to try a number of different starting points\nCOMP4434\n\nNew Jersey Institute of Technology\nCentroid & Clustroid§Centroid is the avg. of all (data)points in the cluster. This means centroid is an “artificial” point§Clustroid is an existing (data)point that is “closest” to all othe", "local_index": 20}
{"chunk_id": "Lecture5_Clustering_Recommender_Systems_21", "doc_id": "Lecture5_Clustering_Recommender_Systems", "text": "centroid is an “artificial” point§Clustroid is an existing (data)point that is “closest” to all other points in the cluster\n22COMP4434\nX\nCluster on3 data points\nCentroid\nClustroid\nDatapoint\nNew Jersey Institute of Technology\nx (1.5,1.5)x (4.5,0.5)x (1,1) x (4.7,1.3)Data:o … data pointx … centroido (1,2)o (2,1)o (0,0)\n23COMP4434\n§Euclidean case: each cluster has a centroid§centroid = average of its (data) points§use the node that is “closest” to the centroid as a clustroid\n§What about the non-Euc", "local_index": 21}
{"chunk_id": "Lecture5_Clustering_Recommender_Systems_22", "doc_id": "Lecture5_Clustering_Recommender_Systems", "text": " (data) points§use the node that is “closest” to the centroid as a clustroid\n§What about the non-Euclidean case?\no (5,0)o (4,1)\no (5,3)\nClustroid\nNew Jersey Institute of Technology\nClustroid (non-Euclidean Case)§Non-Euclidean: The only “locations” we can talk about are the points themselves, i.e., there is no “average” of two points§clustroid  = point “closest” to other points§Possible meanings of “closest”:§Smallest average distance to other points§Smallest sum of squares of distances to other ", "local_index": 22}
{"chunk_id": "Lecture5_Clustering_Recommender_Systems_23", "doc_id": "Lecture5_Clustering_Recommender_Systems", "text": " “closest”:§Smallest average distance to other points§Smallest sum of squares of distances to other points, e.g., for distance metric d clustroid c of cluster C is:\n§Smallest maximum distance to other pointså\nÎ Cx\nc\ncxd\n2\n),(min\n24COMP4434\nNew Jersey Institute of Technology\nPros & Cons\n25COMP4434\n§Simple iterative method§User provides “K”§Often too simple ----> bad results§Difficult to guess the correct “K”§We may not know the number of clusters before we want to find clusters§No guarantee of op", "local_index": 23}
{"chunk_id": "Lecture5_Clustering_Recommender_Systems_24", "doc_id": "Lecture5_Clustering_Recommender_Systems", "text": "orrect “K”§We may not know the number of clusters before we want to find clusters§No guarantee of optimal solution§Complexity is O( n * K * I * d )§n = number of points, K = number of clusters, I = number of iterations, d = number of attributes\nNew Jersey Institute of Technology\nLimitations: when clusters are of differing sizes\n26COMP4434\nOriginal PointsK-means (3 Clusters)\nNew Jersey Institute of Technology\nLimitations: when clusters are of differing densities\n27COMP4434\nOriginal Points\nK-means", "local_index": 24}
{"chunk_id": "Lecture5_Clustering_Recommender_Systems_25", "doc_id": "Lecture5_Clustering_Recommender_Systems", "text": " Technology\nLimitations: when clusters are of differing densities\n27COMP4434\nOriginal Points\nK-means (3 Clusters)\nNew Jersey Institute of Technology\nLimitations: when non-globular shapes\n28COMP4434\nOriginal Points\nK-means (2 Clusters)\nNew Jersey Institute of Technology\nOvercoming K-means Limitations§One solution is to find many clusters§each of them represents a part of a natural cluster§small clusters need to be put together in a post-processing step\n29COMP4434\nOriginal Points    K-means Cluste", "local_index": 25}
{"chunk_id": "Lecture5_Clustering_Recommender_Systems_26", "doc_id": "Lecture5_Clustering_Recommender_Systems", "text": "sters need to be put together in a post-processing step\n29COMP4434\nOriginal Points    K-means Clusters\n\nNew Jersey Institute of Technology30Original Points    K-means Clusters\n\nNew Jersey Institute of Technology\nRecommender System Examples§Amazon, YouTube, Netflix, …§How to improve users’ satisfaction?§What item for what people?§E.g., Recommend movies based on the predictions of user’s movie ratings\nCOMP4434 31\n\nNew Jersey Institute of Technology\nMore Recommender System Examples§News feed§Music ", "local_index": 26}
{"chunk_id": "Lecture5_Clustering_Recommender_Systems_27", "doc_id": "Lecture5_Clustering_Recommender_Systems", "text": "gs\nCOMP4434 31\n\nNew Jersey Institute of Technology\nMore Recommender System Examples§News feed§Music feed§Twitter feed\n32\n\nNew Jersey Institute of Technology\nRecommender System Types\n§Content Based (CB): recommendations are based on the assumption that if in the past a user liked a set of items with particular features, she/he will likely go for the items with similar characteristics.§Collaborative Filtering (CF): recommendations are based on the assumption that users having similar history are m", "local_index": 27}
{"chunk_id": "Lecture5_Clustering_Recommender_Systems_28", "doc_id": "Lecture5_Clustering_Recommender_Systems", "text": " Filtering (CF): recommendations are based on the assumption that users having similar history are more likely to have similar tastes/needs.33COMP4434\nNew Jersey Institute of Technology\nRecommender System Types\n34COMP4434\n\nNew Jersey Institute of Technology\nContent-based Recommender Systems\n§Give recommendations to a user based on items with “similar” content in user’s profile§Recommendation is only dependent on particular user’s historical data§Besides user-item interactions (i.e., ratings), we", "local_index": 28}
{"chunk_id": "Lecture5_Clustering_Recommender_Systems_29", "doc_id": "Lecture5_Clustering_Recommender_Systems", "text": "ly dependent on particular user’s historical data§Besides user-item interactions (i.e., ratings), we also have the item feature vectors as the inputs\n35COMP4434\nNew Jersey Institute of Technology\nPlan of Action\nlikesItems have profiles\nRedCirclesTrianglesUser profile\nmatch\nrecommend build\n36COMP4434\nNew Jersey Institute of Technology\nExample\n?  not rated yet\nMovieAliceBobCarolDaveX1 (Romance)X2 (KungFu)Love letter5 5 0 0 0.90Romancer5 ? ? 0 1 0Stay with me? 4 0 ? 0.890KungFu Panda0 0 5 4 0.20.9F", "local_index": 29}
{"chunk_id": "Lecture5_Clustering_Recommender_Systems_30", "doc_id": "Lecture5_Clustering_Recommender_Systems", "text": "ungFu)Love letter5 5 0 0 0.90Romancer5 ? ? 0 1 0Stay with me? 4 0 ? 0.890KungFu Panda0 0 5 4 0.20.9FightFightFight0 0 5 ? 0.11\n37COMP4434\n§For each item, create an item profile (a set of features)§E.g., each movie has genre, author, title, actor, director,…\nNew Jersey Institute of Technology\nSymbols: Table\nMovieAliceBobCarolDaveX1 (Romance)X2 (KungFu)Love letter5 5 0 0 0.90Romancer5 ? ? 0 1 0Stay with me? 4 0 ? 0.890KungFu Panda0 0 5 4 0.20.9FightFightFight0 0 5 ? 0.11\n38COMP4434\n\"=2: number of ", "local_index": 30}
{"chunk_id": "Lecture5_Clustering_Recommender_Systems_31", "doc_id": "Lecture5_Clustering_Recommender_Systems", "text": "with me? 4 0 ? 0.890KungFu Panda0 0 5 4 0.20.9FightFightFight0 0 5 ? 0.11\n38COMP4434\n\"=2: number of movie features\"!=4: number of users\n\"\"=5: number of movies\nNew Jersey Institute of Technology\nSymbols: Rating\n'(,*=1 if user *\thas rated movie (;\t-#,% is the raeng\nMovieAliceBobCarolDaveLove letter5 5 0 0Romancer5 ? ? 0Stay with me? 4 0 ?KungFu Panda0 0 5 4FightFightFight0 0 5 ?\n39COMP4434\n'1,2=1, -&,'=5\n'3,4=0\n0%: number of rated movies rated by user *\n0&=4\nNew Jersey Institute of Technology\nRMSE", "local_index": 31}
{"chunk_id": "Lecture5_Clustering_Recommender_Systems_32", "doc_id": "Lecture5_Clustering_Recommender_Systems", "text": "&,'=5\n'3,4=0\n0%: number of rated movies rated by user *\n0&=4\nNew Jersey Institute of Technology\nRMSE§Compare predictions with known ratings§My system predicted you would rate§The Shawshank Redemption as 4.3 stars§In reality, you gave it 5 stars§The Matrix with 3.9 stars§In reality, you gave it 4 stars§RMSE = sqrt( 1/2 * (4.3 - 5)^2 + (3.9 - 4)^2))\n40COMP4434\n\nNew Jersey Institute of Technology\nHow to solve the problem for Alice?\n41\nMovieAliceBobCarolDaveX1 (Romance)X2 (KungFu)Love letter5 5 0 0 ", "local_index": 32}
{"chunk_id": "Lecture5_Clustering_Recommender_Systems_33", "doc_id": "Lecture5_Clustering_Recommender_Systems", "text": " to solve the problem for Alice?\n41\nMovieAliceBobCarolDaveX1 (Romance)X2 (KungFu)Love letter5 5 0 0 0.90Romancer5 ? ? 0 1 0Stay with me? 4 0 ? 0.890KungFu Panda0 0 5 4 0.20.9FightFightFight0 0 5 ? 0.11\nCOMP4434\nNew Jersey Institute of Technology\nHypothesis For Alice\n42COMP4434\n§Learn parameter !!=[!\"!!!!!#!]$ by solving a Linear Regression problem§Hypothesis function       ℎ%!&=\t1&(2=!\"!&\"+!!!&!+!#!&#\n§Cost Function       !\"(\")=\"$%(\")∑&:(&,\"*\"\t\"\"+&&−(&,\"$+,$%(\")∑-*\".\"-\"$\n                     =\"$", "local_index": 33}
{"chunk_id": "Lecture5_Clustering_Recommender_Systems_34", "doc_id": "Lecture5_Clustering_Recommender_Systems", "text": "#!&#\n§Cost Function       !\"(\")=\"$%(\")∑&:(&,\"*\"\t\"\"+&&−(&,\"$+,$%(\")∑-*\".\"-\"$\n                     =\"$%(\")∑&:(&,\"*\"∑-*/.\"-\"&-&−(&,\"$+,$%(\")∑-*\".\"-\"$\nNew Jersey Institute of Technology\nIteration …\n43COMP4434\nGradient descent update\nNew Jersey Institute of Technology\nAlice’s Model\n1(&)=000=0.0250.03750=⋯=1.953.17−2.52ℎ+!2=\t1&(2=1.95+3.172&−2.522'\n44COMP4434\n\nNew Jersey Institute of Technology\nX1 (Romance)X2 (KungFu)0.901 00.8900.20.90.11\nAlice-#,,554.7700\nMovieLove letter            &\"Romancer      ", "local_index": 34}
{"chunk_id": "Lecture5_Clustering_Recommender_Systems_35", "doc_id": "Lecture5_Clustering_Recommender_Systems", "text": "nce)X2 (KungFu)0.901 00.8900.20.90.11\nAlice-#,,554.7700\nMovieLove letter            &\"Romancer             &$Stay with me        &0KungFu Panda      &1FightFightFight     &2\nRating Prediction for Alice\n§Predict user * rating movie ( with \t1%(2#\n§E.g., \t1&(2-=1.953.17−2.5210.890=4.7745\nCOMP4434\nNew Jersey Institute of Technology\nGeneral Problem\n§For each user *, learn parameter 1%∈<./&\n46\nMovieAlice\"3 Bob\"4 Carol\"5 Dave\"6 X1 (Romance)X2 (KungFu)Love letter         &\" 5 5 0 0 0.90Romancer         ", "local_index": 35}
{"chunk_id": "Lecture5_Clustering_Recommender_Systems_36", "doc_id": "Lecture5_Clustering_Recommender_Systems", "text": "e\"3 Bob\"4 Carol\"5 Dave\"6 X1 (Romance)X2 (KungFu)Love letter         &\" 5 5 0 0 0.90Romancer          &$ 5 ? ? 0 1 0Stay with me     &0 ? 4 0 ? 0.890KungFu Panda   &1 0 0 5 4 0.20.9FightFightFight  &2 0 0 5 ? 0.11\nCOMP4434\nNew Jersey Institute of Technology\nProblem Formulation§!\",$=1 if user $\thas rated movie \"§!\",$=0 if user $\thas not rated movie \"§)*,,: rating by user $\ton movie \" if !\",$=1 §* : number of features of a movie§+,∈--./: parameter vector for user $\t§.*∈--./: feature vector for movi", "local_index": 36}
{"chunk_id": "Lecture5_Clustering_Recommender_Systems_37", "doc_id": "Lecture5_Clustering_Recommender_Systems", "text": "number of features of a movie§+,∈--./: parameter vector for user $\t§.*∈--./: feature vector for movie \"\t§/, : number of rated movies rated by user $ §*0 : number of users§*1 : number of movies47COMP4434\nNew Jersey Institute of Technology\nCB Optimization Objective§Given 2&,2', ⋯, 2.\", to learn 1%: min\t+# 120%@#:2#,%3&\t1%(2#−-#,%'+A20%@43&\n.14%'\n48\nMovie&\"& (Romance)&$& (KungFu)Love letter &\" 0.90Romancer &$ 1 0Stay with me &0 0.890KungFu Panda &1 0.20.9FightFightFight &2 0.11\nUser j(&,755?00 \"(7)", "local_index": 37}
{"chunk_id": "Lecture5_Clustering_Recommender_Systems_38", "doc_id": "Lecture5_Clustering_Recommender_Systems", "text": "mancer &$ 1 0Stay with me &0 0.890KungFu Panda &1 0.20.9FightFightFight &2 0.11\nUser j(&,755?00 \"(7)=\"/(7)\"\"(7)\"$(7)=?\nCOMP4434\n5(#)=10.90\t110\t10.890\t10.20.9\t10.11\n1%∈<./&;\t2#∈<./&, 2=#=1\nNew Jersey Institute of Technology\nOptimization Objectives\n§To learn 1% (parameter for user *\t): min\t%%12-':)',+,!\t!+$&'−0',+#+12--,!\n.!-+#\n§To learn 1&,1', ⋯, 1.$\nmin%!,%&, ⋯, %'(12-+,!\n.(-':)',+,!\t!+$&'−0',+#+12-+,!\n.(--,!\n.!-+#\n49COMP4434\nNew Jersey Institute of Technology\nCB Gradient Decent Update\n50\n!-+=!-", "local_index": 38}
{"chunk_id": "Lecture5_Clustering_Recommender_Systems_39", "doc_id": "Lecture5_Clustering_Recommender_Systems", "text": "2-+,!\n.(--,!\n.!-+#\n49COMP4434\nNew Jersey Institute of Technology\nCB Gradient Decent Update\n50\n!-+=!-+−2-':)',+,!\t!+$&'−0',+&-'\n§B=0\n§B≠0!-+=!-+−2-':)',+,!\t!+$&'−0',+&-'+1!-+\nCOMP4434\nNew Jersey Institute of Technology\nPros: Content-based Approach§+: No need for data on other users§+: Able to recommend to users with unique tastes§+: Able to recommend new & unpopular items§No cold-start item problems§+: Able to provide explanations§Can provide explanations of recommended items by listing content-f", "local_index": 39}
{"chunk_id": "Lecture5_Clustering_Recommender_Systems_40", "doc_id": "Lecture5_Clustering_Recommender_Systems", "text": "s§+: Able to provide explanations§Can provide explanations of recommended items by listing content-features that caused an item to be recommended\nCOMP4434 51\nNew Jersey Institute of Technology\nCons: Content-based Approach§–: Finding the appropriate features is hard§E.g., images, movies, music§–: Recommendations for new users§How to build a user profile?§–: Overspecialization§Never recommends items outside user’s content profile§People might have multiple interests§Unable to exploit quality judgm", "local_index": 40}
{"chunk_id": "Lecture5_Clustering_Recommender_Systems_41", "doc_id": "Lecture5_Clustering_Recommender_Systems", "text": " outside user’s content profile§People might have multiple interests§Unable to exploit quality judgments of other users\nCOMP4434 52", "local_index": 41}
{"chunk_id": "Lecture6_Collaborative_Filtering_0", "doc_id": "Lecture6_Collaborative_Filtering", "text": "COMP4434 Big Data AnalyticsLecture 6 Collaborative FilteringHUANG Xiaoxiaohuang@comp.polyu.edu.hk\n\nNew Jersey Institute of Technology\nCollaborative Filtering Recommender Systems\n§Give recommendations to a user based on the preferences of “similar” users§Recommendation is dependent on other users’ historical data§We only have user-item interactions (i.e., ratings) as the inputs\n3COMP4434\nNew Jersey Institute of Technology\nCollaborative Filtering§Consider user x§Find set N of other users whose rat", "local_index": 0}
{"chunk_id": "Lecture6_Collaborative_Filtering_1", "doc_id": "Lecture6_Collaborative_Filtering", "text": " Institute of Technology\nCollaborative Filtering§Consider user x§Find set N of other users whose ratings are “similar” to x’s ratings§Estimate x’s ratings based on ratings of users in N\nx\nN\nCOMP4434 4\nNew Jersey Institute of Technology\nExample\n5\nMovieAlice!! Bob!\" Carol!# Dave!$ X1X2 \nLove letter \"% 5 5 0 0 ? ?Romancer \"& 5 ? ? 0 ? ?Stay with me \"' ? 4 0 ? ? ?KungFu Panda \"( 0 0 5 4 ? ?FightFightFight \") 0 0 5 ? ? ?\n§If both C and D like KungFu Panda and dislike Love Letter, then when C has rate", "local_index": 1}
{"chunk_id": "Lecture6_Collaborative_Filtering_2", "doc_id": "Lecture6_Collaborative_Filtering", "text": "ight \") 0 0 5 ? ? ?\n§If both C and D like KungFu Panda and dislike Love Letter, then when C has rated a new movie FightFightFight as good, it will recommend the movie to D§It learns feature itself - “Feature Learning”COMP4434\nNew Jersey Institute of Technology\nSymbols§!\",$=1 if user $\thas rated movie \"§!\",$=0 if user $\thas not rated movie \"§)#,%: rating by user $\ton movie \" if !\",$=1 §* : number of features of a movie§+%∈-&: parameter vector for user $\t§.#∈-&: feature vector for movie \"\t§/% : nu", "local_index": 2}
{"chunk_id": "Lecture6_Collaborative_Filtering_3", "doc_id": "Lecture6_Collaborative_Filtering", "text": "f features of a movie§+%∈-&: parameter vector for user $\t§.#∈-&: feature vector for movie \"\t§/% : number of rated movies rated by user $ §*' : number of users§*( : number of movies6COMP4434\nNew Jersey Institute of Technology\nApproach I: CF based on Linear Regression\n7COMP4434\n§Learn parameter !!,\t!\", …, !#and $!,\t$\", …, $$\tby solving a Linear Regression problem§!%=[!!%!\"%]&, $'=[$!'$\"']&,(=15,+=2§Hypothesis functionℎ!,#\",$=\t$'&!(%)=$$(#)\"$(!)+$'(#)\"'(!)§Cost Function         -!,$=!\"*∑(%,'):-%,'.", "local_index": 3}
{"chunk_id": "Lecture6_Collaborative_Filtering_4", "doc_id": "Lecture6_Collaborative_Filtering", "text": "Hypothesis functionℎ!,#\",$=\t$'&!(%)=$$(#)\"$(!)+$'(#)\"'(!)§Cost Function         -!,$=!\"*∑(%,'):-%,'.!\t$'&!%−0%,'\"\n\t +/\"*∑%.!#∑0.!1!0%\"+/\"*∑'.!$∑0.!1$0'\"\nNew Jersey Institute of Technology\nCollaborative Filtering Algorithm\n8\n§Initialize $!,$\", ⋯, $1! and !!,!\", ⋯, !1\" to small random values§Minimize -!!,!\",\t⋯,\t!1\",$!,$\",\t⋯,\t$1! using gradient decent§For a user with parameters $\tand a movie with learned features !, predict a rating of $&!\nCOMP4434\nNew Jersey Institute of Technology\nCF Gradient !\"(", "local_index": 4}
{"chunk_id": "Lecture6_Collaborative_Filtering_5", "doc_id": "Lecture6_Collaborative_Filtering", "text": "rned features !, predict a rating of $&!\nCOMP4434\nNew Jersey Institute of Technology\nCF Gradient !\"($,&)/!& and !\"($,&)/!& \n9COMP4434\n§Gradient for !'(\"!,$(#)'\"((!) =1*+#:*!,#+$\t$#,\"!−.!,#$(#+/\"(!\n'(\"!,$(#)'$((#) =1*+!:*!,#+$\t$#,\"!−.!,#\"(!+/$(#\n§Gradient for $\nNew Jersey Institute of Technology\nCollaborative Filtering Gradient Descent Update\n10COMP4434\n\"(!=\"(!−0*+#:*!,#+$\t$#,\"!−.!,#$(#+/\"(!\n§Gradient Descent Update for !\n$(#=$(#−0*+!:*!,#+$\t$#,\"!−.!,#\"(!+/$(#\n§Gradient Descent Update for $\nNew J", "local_index": 5}
{"chunk_id": "Lecture6_Collaborative_Filtering_6", "doc_id": "Lecture6_Collaborative_Filtering", "text": "ent Descent Update for !\n$(#=$(#−0*+!:*!,#+$\t$#,\"!−.!,#\"(!+/$(#\n§Gradient Descent Update for $\nNew Jersey Institute of Technology\nCollaborative Filtering Optimization Objective§Learn $!,$\", ⋯, $1! and !!,!\", ⋯, !1\":)\"%,\"&,\t⋯,\t\"*!,!%,!&,\t⋯,\t!*\"\n=12/0(,,.):1,,.2%\t!.3\",−2,,.&+42/0.2%\n*\"042%\n*!4.&+42/0,2%\n*!042%\n*\"4,&\n11COMP4434\nMovie\"%, \"&, Love letter \"% ? ?Romancer \"& ? ?Stay with me \"' ? ?KungFu Panda \"( ? ?FightFightFight \") ? ?\nUser 12,,%55?00\n…\nUser 552,,*\"00?4?\nNew Jersey Institute of Techno", "local_index": 6}
{"chunk_id": "Lecture6_Collaborative_Filtering_7", "doc_id": "Lecture6_Collaborative_Filtering", "text": "anda \"( ? ?FightFightFight \") ? ?\nUser 12,,%55?00\n…\nUser 552,,*\"00?4?\nNew Jersey Institute of Technology\nContent-based Optimization Objective§Given !!,!\", ⋯, !1\", to learn $!,$\", ⋯, $1! :)!%,!&,\t⋯,\t!*\"\n=12/0(,,.):1,,.2%\t!.3\",−2,,.&+42/0.2%\n*\"042%\n*!4.&\n12COMP4434\nMovie\"%, \"&, Love letter \"% 0.90Romancer \"& 1 0Stay with me \"' .890KungFu Panda \"( 0.20.9FightFightFight \") 0.11\nUser 12,,%55?00\n…\nUser 552,,*\"00?4?\nNew Jersey Institute of Technology\nApproach II: User-user collaborative filteringFrom s", "local_index": 7}
{"chunk_id": "Lecture6_Collaborative_Filtering_8", "doc_id": "Lecture6_Collaborative_Filtering", "text": "552,,*\"00?4?\nNew Jersey Institute of Technology\nApproach II: User-user collaborative filteringFrom similarity metric to recommendations:§Let rx be the vector of user x’s ratings§Let N be the set of k users most similar to x who have rated item i§Prediction for item i of user x:§32%=!0\t∑3∈533%§32%=∑#∈%7&#⋅-#'∑#∈%7&# Shorthand: (!\"=(*+,,.\nCOMP4434 13\nNew Jersey Institute of Technology\nStep 1: Finding “Similar” Users§Let rx be the vector of user x’s ratings§Jaccard similarity measure§Problem: Ignor", "local_index": 8}
{"chunk_id": "Lecture6_Collaborative_Filtering_9", "doc_id": "Lecture6_Collaborative_Filtering", "text": "g “Similar” Users§Let rx be the vector of user x’s ratings§Jaccard similarity measure§Problem: Ignores the value of the rating §Cosine similarity measure§sim(x, y) = cos(rx, ry) = -&⋅-#||-&||⋅||-#||§Problem: Treats missing ratings as “negative”§Pearson correlation coefficient§Sxy = items rated by both users x and y§Sx = items rated by user x§Sy = items rated by both usery \nrx = [*, _, _, *, ***]ry = [*, _, **, **, _]rx, ry as sets:rx = {1, 4, 5}ry = {1, 3, 4}rx, ry as points:rx = {1, 0, 0, 1, 3}", "local_index": 9}
{"chunk_id": "Lecture6_Collaborative_Filtering_10", "doc_id": "Lecture6_Collaborative_Filtering", "text": " = [*, _, **, **, _]rx, ry as sets:rx = {1, 4, 5}ry = {1, 3, 4}rx, ry as points:rx = {1, 0, 0, 1, 3}ry = {1, 0, 2, 2, 0}\nrx, ry … avg.rating of x, y\n4567,8=∑:∈;,.9<:−9<9=:−9=∑:∈;,9<:−9<>∑:∈;.9=:−9=>\nNew Jersey Institute of Technology\nJaccard similarity§The Jaccard similarity of two sets is the size of their intersection divided by the size of their union:sim(C1, C2) = |C1ÇC2|/|C1ÈC2|§Jaccard distance: d(C1, C2) = 1 - |C1ÇC2|/|C1ÈC2|\n§Cosine similarity\n3 in intersection8 in unionJaccard similarit", "local_index": 10}
{"chunk_id": "Lecture6_Collaborative_Filtering_11", "doc_id": "Lecture6_Collaborative_Filtering", "text": "nce: d(C1, C2) = 1 - |C1ÇC2|/|C1ÈC2|\n§Cosine similarity\n3 in intersection8 in unionJaccard similarity= 3/8Jaccard distance = 5/8\nCOMP4434 15\n!\"#(%,')\t=∑#,$#⋅,%#∑#,$#&⋅∑#,%#&\nNew Jersey Institute of Technology\nAn Example\n§Intuitively we want: sim(A, B) > sim(A, C)§Jaccard similarity: 1/5 < 2/4§Cosine similarity:§                                       >§0.380 > 0.322§Considers missing ratings as “negative”\n./!!\"#(%,')\t=∑#,$#⋅,%#∑#,$#&⋅∑#,%#&\nCOMP4434 16\nNew Jersey Institute of Technology\nAn Exampl", "local_index": 11}
{"chunk_id": "Lecture6_Collaborative_Filtering_12", "doc_id": "Lecture6_Collaborative_Filtering", "text": "gative”\n./!!\"#(%,')\t=∑#,$#⋅,%#∑#,$#&⋅∑#,%#&\nCOMP4434 16\nNew Jersey Institute of Technology\nAn Example\n§Pearson correlation coefficient§Sxy = items rated by both users x and y\n§sim(A, B) = 0.09 > sim(A, C)  = -0.559§A: [2/3,       ,         ,  5/3, -7/3,         ,     ]§B: [1/3, 1/3, -2/3,        ,          ,        ,     ]§C: [      ,       ,         , -5/3,   1/3,  4/3,      ]§(2/9)/(sqrt(4/9+25/9+49/9) * sqrt(1/9+1/9+4/9))§(-25/9-7/9)/(sqrt(4/9+25/9+49/9) * sqrt(25/9+1/9+16/9))\n17\n4567,8=∑:∈;,", "local_index": 12}
{"chunk_id": "Lecture6_Collaborative_Filtering_13", "doc_id": "Lecture6_Collaborative_Filtering", "text": "9+49/9) * sqrt(1/9+1/9+4/9))§(-25/9-7/9)/(sqrt(4/9+25/9+49/9) * sqrt(25/9+1/9+16/9))\n17\n4567,8=∑:∈;,.9<:−9<9=:−9=∑:∈;,9<:−9<>∑:∈;.9=:−9=>rx, ry … avg.rating of x, y\nNew Jersey Institute of Technology\nStep 2: Rating PredictionsFrom similarity metric to recommendations:§Let rx be the vector of user x’s ratings§Let N be the set of k users most similar to x who have rated item i§Prediction for item i of user x:§32%=!0\t∑3∈533%§32%=∑#∈%7&#⋅-#'∑#∈%7&#§Other options?Shorthand: (!\"=(*+,,.\nCOMP4434 18\nNew", "local_index": 13}
{"chunk_id": "Lecture6_Collaborative_Filtering_14", "doc_id": "Lecture6_Collaborative_Filtering", "text": "of user x:§32%=!0\t∑3∈533%§32%=∑#∈%7&#⋅-#'∑#∈%7&#§Other options?Shorthand: (!\"=(*+,,.\nCOMP4434 18\nNew Jersey Institute of Technology\nItem-Item Collaborative Filtering§So far: User-user collaborative filtering§Another view: Item-item§For item i, find other similar items§Estimate rating for item i based on ratings for similar items§Can use same similarity metrics and prediction functions as in user-user model\nååÎÎ×=);();(xiNj ijxiNj xjijxi srsr sij… similarity of items i and jrxj…rating of user x o", "local_index": 14}
{"chunk_id": "Lecture6_Collaborative_Filtering_15", "doc_id": "Lecture6_Collaborative_Filtering", "text": "ser model\nååÎÎ×=);();(xiNj ijxiNj xjijxi srsr sij… similarity of items i and jrxj…rating of user x on item jN(i;x)… set items rated by x similar to i\nCOMP4434 19\nNew Jersey Institute of Technology\nExercise§Consider an Item-Item Collaborative Filtering recommendation system. For a given item A, the system has identified the top 3 most similar items to A, namely B, C, and D. The cosine similarities between item A and the corresponding items are 0.4, 0.5, and 0.6, respectively.§Assuming that the sy", "local_index": 15}
{"chunk_id": "Lecture6_Collaborative_Filtering_16", "doc_id": "Lecture6_Collaborative_Filtering", "text": "between item A and the corresponding items are 0.4, 0.5, and 0.6, respectively.§Assuming that the system has collected user ratings for items B, C, and D from a specific user x, and the ratings are as follows:§Rating for item B: 5.0§Rating for item C: 4.0§Rating for item D: 3.0§Compute the predicted rating for user x on item A using the weighted average§(0.4*5+0.5*4+0.6*3)/(0.4+0.5+0.6) ≈ 3.87 20COMP\nNew Jersey Institute of Technology\nItem-Item CF (|N|=2)121110987654321 455? 311 3124452 53432142", "local_index": 16}
{"chunk_id": "Lecture6_Collaborative_Filtering_17", "doc_id": "Lecture6_Collaborative_Filtering", "text": "OMP\nNew Jersey Institute of Technology\nItem-Item CF (|N|=2)121110987654321 455? 311 3124452 534321423 245424 5224345 423316\nusers\n- estimate rating of movie 1 by user 5\nmovies\nCOMP4434 21- unknown rating- rating between 1 to 5\nNew Jersey Institute of Technology\nItem-Item CF (|N|=2)121110987654321 455? 311 3124452 534321423 245424 5224345 423316\nusers\nNeighbor selection:Identify movies similar to movie 1, rated by user 5\nmovies\n1.00-0.180.41-0.10-0.310.59\nsim(1,m)\nHere we use Pearson correlation ", "local_index": 17}
{"chunk_id": "Lecture6_Collaborative_Filtering_18", "doc_id": "Lecture6_Collaborative_Filtering", "text": "ovie 1, rated by user 5\nmovies\n1.00-0.180.41-0.10-0.310.59\nsim(1,m)\nHere we use Pearson correlation as similarity:1) Subtract mean rating mi from each movie i    m1 = (1+3+5+5+4)/5 = 3.6    row 1: [-2.6, 0, -0.6, 0, 0, 1.4, 0, 0, 1.4, 0, 0.4, 0]2) Compute similarities between rows\nNew Jersey Institute of Technology\nDetails§The mean of movie 1 is (1+3+5+5+4)/5=3.6, so it becomesrow 1: [-2.6, 0, -0.6, 0,   0,  1.4, 0, 0, 1.4, 0, 0.4, 0]§The mean of movie 3 is(2+4+1+2+3+4+3+5)/8 = 3, so it becomes ", "local_index": 18}
{"chunk_id": "Lecture6_Collaborative_Filtering_19", "doc_id": "Lecture6_Collaborative_Filtering", "text": "6, 0,   0,  1.4, 0, 0, 1.4, 0, 0.4, 0]§The mean of movie 3 is(2+4+1+2+3+4+3+5)/8 = 3, so it becomes row 3: [-1,    1,  0,    -2,  -1,  0,    0, 0,  1,   0,   2,   0]§The Pearson correlation coefficient is (2.6+1.4+0.4*2)/(sqrt(2.6^2 + 0.6^2 + 1.4^2 + 1.4^2 + 0.4^2) * sqrt(1+1+4+1+1+4) ) = 4.8/(3.346*3.464) = 0.41.\n23COMP\nNew Jersey Institute of Technology\nItem-Item CF (|N|=2)121110987654321 4552.6311 3124452 534321423 245424 5224345 423316\nusers\nPredict by taking weighted average:r1.5 = (0.41*2 ", "local_index": 19}
{"chunk_id": "Lecture6_Collaborative_Filtering_20", "doc_id": "Lecture6_Collaborative_Filtering", "text": "311 3124452 534321423 245424 5224345 423316\nusers\nPredict by taking weighted average:r1.5 = (0.41*2 + 0.59*3) / (0.41+0.59) = 2.6\nmovies\n667=∑8∈:(6;7)868⋅687∑868COMP4434 24\nsim(1,m)1.00-0.180.41-0.10-0.310.59\nNew Jersey Institute of Technology\nCF: Common Practice§Define similarity sij of items i and j§Select k nearest neighbors N(i; x)§Items most similar to i, that were rated by x§Estimate rating rxi as the weighted average: \nbaseline estimate for rxi ¡μ =  overall mean movie rating¡bx  =  ratin", "local_index": 20}
{"chunk_id": "Lecture6_Collaborative_Filtering_21", "doc_id": "Lecture6_Collaborative_Filtering", "text": "rxi as the weighted average: \nbaseline estimate for rxi ¡μ =  overall mean movie rating¡bx  =  rating deviation of user x            = (avg. rating of user x) – μ ¡bi   =  rating deviation of movie i      = (avg. rating of movie i) – μ  \nååÎÎ=);();(xiNjijxiNjxjijxisrsrBefore:\nååÎÎ -×+= );();( )(xiNj ijxiNj xjxjijxixi sbrsbr;<?=<+;<+;? COMP4434\nNew Jersey Institute of Technology\nItem-Item vs. User-User¡In practice, it has been observed that item-item often works better than user-user¡Why? Items a", "local_index": 21}
{"chunk_id": "Lecture6_Collaborative_Filtering_22", "doc_id": "Lecture6_Collaborative_Filtering", "text": "User¡In practice, it has been observed that item-item often works better than user-user¡Why? Items are simpler, users have multiple tastes¡For example, it is easier to discover items that are similar because they belong to the same genre, than it is to detect that two users are similar because they prefer one genre in common, while each also likes some genres that the other doesn’t care for.\nCOMP4434 26\nNew Jersey Institute of Technology\nThe Netflix Prize (2 Oct 2006 – 21 Sept 2009)§Training dat", "local_index": 22}
{"chunk_id": "Lecture6_Collaborative_Filtering_23", "doc_id": "Lecture6_Collaborative_Filtering", "text": "434 26\nNew Jersey Institute of Technology\nThe Netflix Prize (2 Oct 2006 – 21 Sept 2009)§Training datao100 million ratingso480,000 userso17,770 movieso6 years of data: 2000-2005 §Test dataoLast few ratings of each user (2.8 million)oEvaluation criterion: root mean squared error (RMSE)oNetflix Cinematch system RMSE: 0.9514 §Competition§2700+ teams§$1 million grand prize for 10% improvement over Netflix 27COMP4434\nNew Jersey Institute of Technology\nMillion $ Awarded Sept 21st 2009\n28COMP4434\nNew Je", "local_index": 23}
{"chunk_id": "Lecture6_Collaborative_Filtering_24", "doc_id": "Lecture6_Collaborative_Filtering", "text": "lix 27COMP4434\nNew Jersey Institute of Technology\nMillion $ Awarded Sept 21st 2009\n28COMP4434\nNew Jersey Institute of Technology\n65\nGrand Prize: 0.8563 \nNetflix: 0.9514 \nMovie average: 1.0533\nUser average: 1.0651 \nGlobal average: 1.1296 \nBasic Collaborative filtering: 0.94\nLatent factors: 0.90\nLatent factors+Biases: 0.89\nCollaborative filtering++: 0.91\n2/9/2014 Jure \u0003Leskovec,\u0003Stanford\u0003C246:\u0003Mining\u0003Massive\u0003Datasets 51\nLatent factors+Biases+Time: 0.876\nStill no prize! /\nGetting desperate.\nTry a “", "local_index": 24}
{"chunk_id": "Lecture6_Collaborative_Filtering_25", "doc_id": "Lecture6_Collaborative_Filtering", "text": "g\u0003Massive\u0003Datasets 51\nLatent factors+Biases+Time: 0.876\nStill no prize! /\nGetting desperate.\nTry a “kitchen \nsink” approach!\n29COMP4434https://www.kaggle.com/netflix-inc/netflix-prize-data\n\nNew Jersey Institute of Technology\nBellKor Recommender System§The winner of the Netflix Challenge!§Multi-scale modeling of the data:Combine top level, “regional”modeling of the data, with a refined, local view:§Global:§Overall deviations of users/movies§Factorization: §Addressing “regional” effects§Collaborat", "local_index": 25}
{"chunk_id": "Lecture6_Collaborative_Filtering_26", "doc_id": "Lecture6_Collaborative_Filtering", "text": "§Global:§Overall deviations of users/movies§Factorization: §Addressing “regional” effects§Collaborative filtering: §Extract local patterns\nGlobal effects\nFactorization\nCollaborative filtering\n31COMP4434\nNew Jersey Institute of Technology\nProblems with Error Measures§Narrow focus on accuracy sometimes misses the point§Prediction Diversity§Prediction Context§Order of predictions§In practice, we care only to predict high ratings:§RMSE might penalize a method that does well for high ratings and badl", "local_index": 26}
{"chunk_id": "Lecture6_Collaborative_Filtering_27", "doc_id": "Lecture6_Collaborative_Filtering", "text": " only to predict high ratings:§RMSE might penalize a method that does well for high ratings and badly for others\n32COMP4434\nNew Jersey Institute of Technology\nPros/Cons of Collaborative Filtering§+ Works for any kind of item§No feature selection needed§- Cold Start:§Need enough users in the system to find a match§- Sparsity: §The user/ratings matrix is sparse§Hard to find users that have rated the same items§- First rater: §Cannot recommend an item that has not been previously rated§New items, E", "local_index": 27}
{"chunk_id": "Lecture6_Collaborative_Filtering_28", "doc_id": "Lecture6_Collaborative_Filtering", "text": " same items§- First rater: §Cannot recommend an item that has not been previously rated§New items, Esoteric items§- Popularity bias: §Cannot recommend items to someone with unique taste §Tends to recommend popular itemsCOMP4434 33", "local_index": 28}
{"chunk_id": "Lecture7_Dimensionality_Reduction_0", "doc_id": "Lecture7_Dimensionality_Reduction", "text": "COMP4434 Big Data AnalyticsLecture 7 Dimensionality ReductionHUANG Xiaoxiaohuang@comp.polyu.edu.hk\n\nNew Jersey Institute of Technology\nBellKor Recommender System§The winner of the Netflix Challenge!§Multi-scale modeling of the data:Combine top level, “regional”modeling of the data, with a refined, local view:§Global:§Overall deviations of users/movies§Factorization: §Addressing “regional” effects§Collaborative filtering: §Extract local patterns\nGlobal effects\nFactorization\nCollaborative filterin", "local_index": 0}
{"chunk_id": "Lecture7_Dimensionality_Reduction_1", "doc_id": "Lecture7_Dimensionality_Reduction", "text": "Collaborative filtering: §Extract local patterns\nGlobal effects\nFactorization\nCollaborative filtering\n2COMP4434\nNew Jersey Institute of Technology\nDimensionality Reduction\n§Assumption: Data lies on or near a low d-dimensional subspace§Red axes of this subspace are effective representation of the data\n 3COMP4434\nNew Jersey Institute of Technology\nDimensionality Reduction§Goal of dimensionality reduction is to discover the red axis of data!\nRather than representingevery point with 2 coordinateswe ", "local_index": 1}
{"chunk_id": "Lecture7_Dimensionality_Reduction_2", "doc_id": "Lecture7_Dimensionality_Reduction", "text": "ction is to discover the red axis of data!\nRather than representingevery point with 2 coordinateswe represent each point with1 coordinate (corresponding tothe position of the point on the red line).By doing this we incur a bit oferror as the points do not exactly lie on the line\n4COMP4434\nNew Jersey Institute of Technology\nDimensionality Reduction§Compress / reduce dimensionality:§ E.g.,\n§106 rows; 103 columns; no updates§Random access to any cell(s); small error: OK\nThe above matrix is really “", "local_index": 2}
{"chunk_id": "Lecture7_Dimensionality_Reduction_3", "doc_id": "Lecture7_Dimensionality_Reduction", "text": "; 103 columns; no updates§Random access to any cell(s); small error: OK\nThe above matrix is really “2-dimensional.” All rows can be reconstructed by scaling [1 1 1 0 0] or [0 0 0 1 1]\n5COMP4434\nNew Jersey Institute of Technology\nWhy Reduce Dimensions?§Data preprocessing is an important part for effective machine learning and data mining§ML and DM techniques may not be effective for high-dimensional data§Dimensionality reduction is an effective approach to downsizing data§The intrinsic dimension ", "local_index": 3}
{"chunk_id": "Lecture7_Dimensionality_Reduction_4", "doc_id": "Lecture7_Dimensionality_Reduction", "text": "l data§Dimensionality reduction is an effective approach to downsizing data§The intrinsic dimension may be small§Discover hidden correlations/topicsE.g., words that occur commonly together§Remove redundant and noisy featuresE.g., not all words are useful§Interpretation and visualization§Easier storage and processing of the data6\nNew Jersey Institute of Technology\nRank of a Matrix§What is rank of a matrix A?§Number of linearly independent columns of A§E.g., Matrix A =                        has r", "local_index": 4}
{"chunk_id": "Lecture7_Dimensionality_Reduction_5", "doc_id": "Lecture7_Dimensionality_Reduction", "text": " matrix A?§Number of linearly independent columns of A§E.g., Matrix A =                        has rank r=2\n§Why? The first two rows are linearly independent, so the rank is at least 2, but all three rows are linearly dependent (the first is equal to the sum of the second and third) so the rank must be less than 3.§Why do we care about low rank?§We can write A as two “basis” vectors: [1 2 1] [-2 -3 1]§And new coordinates of : [1 0] [0 1] [1 -1]\n 7COMP4434\nNew Jersey Institute of Technology\nRank ", "local_index": 5}
{"chunk_id": "Lecture7_Dimensionality_Reduction_6", "doc_id": "Lecture7_Dimensionality_Reduction", "text": "3 1]§And new coordinates of : [1 0] [0 1] [1 -1]\n 7COMP4434\nNew Jersey Institute of Technology\nRank is “Dimensionality”§Cloud of points 3D space:§Think of point positionsas a matrix:\n§We can rewrite coordinates more efficiently!§Old basis vectors: [1 0 0] [0 1 0] [0 0 1]§New basis vectors: [1 2 1] [-2 -3 1]§Then A has new coordinates: [1 0]. B: [0 1], C: [1 -1]§Notice: We reduced the number of coordinates!\n1 row per point:ABC \nA\n8COMP4434\nNew Jersey Institute of Technology\nDimensionality Reducti", "local_index": 6}
{"chunk_id": "Lecture7_Dimensionality_Reduction_7", "doc_id": "Lecture7_Dimensionality_Reduction", "text": "rdinates!\n1 row per point:ABC \nA\n8COMP4434\nNew Jersey Institute of Technology\nDimensionality Reduction Techniques§Singular value decomposition (SVD)§Principal component analysis (PCA)§Non-negative matrix factorization (NMF)§Linear discriminant analysis (LDA)§Autoencoders§Feature selection\n9COMP4434\nNew Jersey Institute of Technology\nDimensionality Reduction by using Hidden Layers\n10\nNew Jersey Institute of Technology\nIn CF, featurelearningis Dimensionality Reduction \n11\nMovieAlice𝜃𝟏 Bob𝜃𝟐 Carol𝜃", "local_index": 7}
{"chunk_id": "Lecture7_Dimensionality_Reduction_8", "doc_id": "Lecture7_Dimensionality_Reduction", "text": "titute of Technology\nIn CF, featurelearningis Dimensionality Reduction \n11\nMovieAlice𝜃𝟏 Bob𝜃𝟐 Carol𝜃𝟑 Dave𝜃𝟒 X1X2 \nLove letter 𝑥% 5 5 0 0 ? ?Romancer 𝑥& 5 ? ? 0 ? ?Stay with me 𝑥' ? 4 0 ? ? ?KungFu Panda 𝑥( 0 0 5 4 ? ?FightFightFight 𝑥) 0 0 5 ? ? ?§§In CF, the model learns feature X1 & X2. §Represent each movie as its ratings & reduce the dimension from 4 (# users) to 2 (X1 & X2)\nHypothesis functionℎ!,#𝑥,𝜃=\t𝜃#$𝑥(!)=𝜃'(#)𝑥'(!)+𝜃((#)𝑥((!)+𝜃)(#)𝑥)(!)+𝜃*(#)𝑥*(!)\nNew Jersey Institute of Technology\nCa", "local_index": 8}
{"chunk_id": "Lecture7_Dimensionality_Reduction_9", "doc_id": "Lecture7_Dimensionality_Reduction", "text": "onℎ!,#𝑥,𝜃=\t𝜃#$𝑥(!)=𝜃'(#)𝑥'(!)+𝜃((#)𝑥((!)+𝜃)(#)𝑥)(!)+𝜃*(#)𝑥*(!)\nNew Jersey Institute of Technology\nCan we learn latent factors directly?\n§Ratings can be recovered by latent factors (low-dimensional features)\n12COMP4434\n19\n! “SVD” on Netflix data: R ≈ Q·  PT\n! For now let’s assume we can approximate the \nrating matrix R as a product of “thin” Q · PT\n! R has missing entries but let’s ignore that for now!\n! Basically, we will want the reconstruction error to be small on known \nratings and we don’t c", "local_index": 9}
{"chunk_id": "Lecture7_Dimensionality_Reduction_10", "doc_id": "Lecture7_Dimensionality_Reduction", "text": "now!\n! Basically, we will want the reconstruction error to be small on known \nratings and we don’t care about the values on the missing ones\nJ. Leskovec, A. Rajaraman, J. Ullman: Mining of Massive Datasets, http://www.mmds.org 18\n45531\n312445\n53432142\n24542\n522434\n42331\n.2-.4.1\n.5.6-.5\n.5.3-.2\n.32.11.1\n-22.1-.7\n.3.7-1\n-.92.41.4.3-.4.8-.5-2.5.3-.21.1\n1.3-.11.2-.72.91.4-1.31.4.5.7-.8\n.1-.6.7.8.4-.3.92.41.7.6-.42.1\n≈\nusers\nitems PT\nQ\nitems\nusers\nR\nSVD: A = U Σ VT\nfactors\nfactors\nLatent factor model", "local_index": 10}
{"chunk_id": "Lecture7_Dimensionality_Reduction_11", "doc_id": "Lecture7_Dimensionality_Reduction", "text": "92.41.7.6-.42.1\n≈\nusers\nitems PT\nQ\nitems\nusers\nR\nSVD: A = U Σ VT\nfactors\nfactors\nLatent factor models\nℎ!,#𝑥,𝜃=\t𝜃#$𝑥(!)=𝜃'(#)𝑥'(!)+𝜃((#)𝑥((!)+𝜃)(#)𝑥)(!)+𝜃*(#)𝑥*(!)\nNew Jersey Institute of Technology\nSingular value decomposition (SVD) - Properties\nIt is always possible to decompose a real matrix A into A = U S VT , where§U, S, V: unique§U, V: column orthonormal§UT U = I; VT V = I  (I: identity matrix)§(Columns are orthogonal unit vectors)§S: diagonal§Entries (singular values) are positive, and sor", "local_index": 11}
{"chunk_id": "Lecture7_Dimensionality_Reduction_12", "doc_id": "Lecture7_Dimensionality_Reduction", "text": "x)§(Columns are orthogonal unit vectors)§S: diagonal§Entries (singular values) are positive, and sorted in decreasing order (σ1 ³ σ2 ³ ... ³ 0) 13\nAm\nn\nSm\nn\nU\nVT» \n\nNew Jersey Institute of Technology\nSingular value decomposition (SVD) - DefinitionA[m x n] = U[m x r] S [ r x r] (V[n x r])T§A: Input data matrix§m x n matrix (e.g., m documents, n terms)§ U: Left singular vectors §m x r matrix  (m documents, r concepts)§ S: Singular values§r x r diagonal matrix (strength of each ‘concept’) (r : rank", "local_index": 12}
{"chunk_id": "Lecture7_Dimensionality_Reduction_13", "doc_id": "Lecture7_Dimensionality_Reduction", "text": "uments, r concepts)§ S: Singular values§r x r diagonal matrix (strength of each ‘concept’) (r : rank of the matrix A)§ V: Right singular vectors§n x r matrix (n terms, r concepts)14COMP4434\nNew Jersey Institute of Technology\nSVD – Example: Users-to-Movies§A = U S VT SciFi-conceptRomance-concept\n=SciFi\nRomance\nx x1   1   1   0   03   3   3   0   04   4   4   0   05   5   5   0   00   2   0   4   40   0   0   5   50   1   0   2   2\n0.14   0.02  0.010.41   0.07  0.030.55   0.09  0.040.69   0.11  0.", "local_index": 13}
{"chunk_id": "Lecture7_Dimensionality_Reduction_14", "doc_id": "Lecture7_Dimensionality_Reduction", "text": "   0   0   5   50   1   0   2   2\n0.14   0.02  0.010.41   0.07  0.030.55   0.09  0.040.69   0.11  0.050.15  -0.59   -0.650.07  -0.73  0.680.07  -0.30   -0.33\n12.5  0     00       9.5  00       0     1.3\n0.56   0.59  0.56   0.09    0.090.13  -0.03  0.13  -0.70  -0.700.41  -0.80  0.40   0.09    0.0915Columns are orthogonal unit vectors: 0.14^2 + 0.41^2 + 0.55^2 + 0.69^2 + 0.15^2 + 0.07^2 + 0.07^2 ≈1\nAliceBobCarolDave…\n§U: latent representation of movies§V: latent representation of users\nNew Jersey", "local_index": 14}
{"chunk_id": "Lecture7_Dimensionality_Reduction_15", "doc_id": "Lecture7_Dimensionality_Reduction", "text": "\nAliceBobCarolDave…\n§U: latent representation of movies§V: latent representation of users\nNew Jersey Institute of Technology\nSet smallest singular values to zeroOnly Keep Major Factors\nx x1   1   1   0   03   3   3   0   04   4   4   0   05   5   5   0   00   2   0   4   40   0   0   5   50   1   0   2   2\n0.14   0.02  -0.010.41   0.07  -0.030.55   0.09  -0.040.69   0.11  -0.050.15  -0.59   0.650.07  -0.73  -0.680.07  -0.29   0.32\n12.4  0     00       9.5  00       0     1.30.56   0.59  0.56   0", "local_index": 15}
{"chunk_id": "Lecture7_Dimensionality_Reduction_16", "doc_id": "Lecture7_Dimensionality_Reduction", "text": "07  -0.73  -0.680.07  -0.29   0.32\n12.4  0     00       9.5  00       0     1.30.56   0.59  0.56   0.09    0.090.13  -0.02  0.12  -0.70  -0.700.41  -0.80  0.40   0.09    0.09\n»\n16COMP4434\nNew Jersey Institute of Technology\nDimensionality Reduction can reduce noise\n» x x1   1   1   0   03   3   3   0   04   4   4   0   05   5   5   0   00   2   0   4   40   0   0   5   50   1   0   2   2\n0.14   0.020.41   0.070.55   0.090.69   0.110.15  -0.590.07  -0.730.07  -0.29\n12.4  0     0       9.5  \n0.56  ", "local_index": 16}
{"chunk_id": "Lecture7_Dimensionality_Reduction_17", "doc_id": "Lecture7_Dimensionality_Reduction", "text": "20.41   0.070.55   0.090.69   0.110.15  -0.590.07  -0.730.07  -0.29\n12.4  0     0       9.5  \n0.56   0.59  0.56   0.09    0.090.13  -0.02  0.12  -0.70  -0.70\n17\n»0.94  1.01   0.94  -0.00  -0.00 2.98  3.04   2.98  -0.00  -0.00 3.98  4.05   3.98  -0.00  -0.00 4.97  5.06   4.97  -0.01  -0.01 0.36  1.29   0.36  4.08   4.08-0.37  0.73  -0.37 4.92   4.92 0.18  0.65   0.18   2.04   2.04\nNew Jersey Institute of Technology\nEstimate unknown ratings as inner-products of factorsEstimate unknown ratings as i", "local_index": 17}
{"chunk_id": "Lecture7_Dimensionality_Reduction_18", "doc_id": "Lecture7_Dimensionality_Reduction", "text": "ute of Technology\nEstimate unknown ratings as inner-products of factorsEstimate unknown ratings as inner-products of factors:\n1 3 5 5 4\n5 4 4 2 1 3\n2 4 1 2 3 4 3 5\n2 4 5 4 2\n4 3 4 2 2 5\n1 3 3 2 4\nitems\n.1 -.4 .2\n-.5 .6 .5\n-.2 .3 .5\n1.1 2.1 .3\n-.7 2.1 -2\n-1 .7 .3\n1.1 -.2 .3 .5 -2 -.5 .8 -.4 .3 1.4 2.4 -.9\n-.8 .7 .5 1.4 .3 -1 1.4 2.9 -.7 1.2 -.1 1.3\n2.1 -.4 .6 1.7 2.4 .9 -.3 .4 .8 .7 -.6 .1\n~\n~\nitems\nusers\nA rank-3 SVD approximation\nusers\n  ?\n18\n? = 2.4\nCOMP4434\nNew Jersey Institute of Technology\n", "local_index": 18}
{"chunk_id": "Lecture7_Dimensionality_Reduction_19", "doc_id": "Lecture7_Dimensionality_Reduction", "text": "s\nusers\nA rank-3 SVD approximation\nusers\n  ?\n18\n? = 2.4\nCOMP4434\nNew Jersey Institute of Technology\nRecap: BellKor Recommender System§The winner of the Netflix Challenge!§Multi-scale modeling of the data:Combine top level, “regional”modeling of the data, with a refined, local view:§Global:§Overall deviations of users/movies§Factorization: §Addressing “regional” effects§Collaborative filtering: §Extract local patterns\nGlobal effects\nFactorization\nCollaborative filtering\n19COMP4434\nNew Jersey Inst", "local_index": 19}
{"chunk_id": "Lecture7_Dimensionality_Reduction_20", "doc_id": "Lecture7_Dimensionality_Reduction", "text": "tract local patterns\nGlobal effects\nFactorization\nCollaborative filtering\n19COMP4434\nNew Jersey Institute of Technology\nv1\nfirst right singular vector\nMovie 1 rating\nMovie 2 rating\n§Instead of using two coordinates (𝒙,𝒚) to describe point locations, let’s use only one coordinate 𝒛§Point’s position is its location along vector 𝒗𝟏 20COMP4434\nFrom 2D to 1D\nNew Jersey Institute of Technology\nMovie-to-User Example: from 5D to 3D§A = U S VT\n=\nAliceBobCarolDave…1   1   1   0   03   3   3   0   04   4  ", "local_index": 20}
{"chunk_id": "Lecture7_Dimensionality_Reduction_21", "doc_id": "Lecture7_Dimensionality_Reduction", "text": "User Example: from 5D to 3D§A = U S VT\n=\nAliceBobCarolDave…1   1   1   0   03   3   3   0   04   4   4   0   05   5   5   0   00   2   0   4   40   0   0   5   50   1   0   2   2\nSm\nn\nU\nVT\nLatent dimensionsaka Latent factors\n21COMP4434\nMatrixAlienSerenityCasablancaAmelie\nNew Jersey Institute of Technology\nMeaning of Singular values§The first feature is the most important one§Singular values represent the importance of features\nSciFi\nRomnce\nSciFi-concept“strength” of the SciFi-concept\n=SciFi\nRomn", "local_index": 21}
{"chunk_id": "Lecture7_Dimensionality_Reduction_22", "doc_id": "Lecture7_Dimensionality_Reduction", "text": "ent the importance of features\nSciFi\nRomnce\nSciFi-concept“strength” of the SciFi-concept\n=SciFi\nRomnce\nx x1   1   1   0   03   3   3   0   04   4   4   0   05   5   5   0   00   2   0   4   40   0   0   5   50   1   0   2   2\n0.14   0.02  -0.010.41   0.07  -0.030.55   0.09  -0.040.69   0.11  -0.050.15  -0.59   0.650.07  -0.73  -0.680.07  -0.29   0.33\n12.4  0     00       9.5  00       0     1.3\n0.56   0.59  0.56   0.09    0.090.13  -0.02  0.13  -0.70  -0.700.41  -0.80  0.40   0.09    0.0922COMP4", "local_index": 22}
{"chunk_id": "Lecture7_Dimensionality_Reduction_23", "doc_id": "Lecture7_Dimensionality_Reduction", "text": "56   0.59  0.56   0.09    0.090.13  -0.02  0.13  -0.70  -0.700.41  -0.80  0.40   0.09    0.0922COMP4434\nAliceBobCarolDave…\nNew Jersey Institute of Technology\n§A = U S VT - example:\nv1\nfirst right singular vector\nMovie 1 rating\nMovie 2 ratingvariance (‘spread’) on the v1 axis\n= x x1   1   1   0   03   3   3   0   04   4   4   0   05   5   5   0   00   2   0   4   40   0   0   5   50   1   0   2   2\n0.14   0.02  -0.010.41   0.07  -0.030.55   0.09  -0.040.69   0.11  -0.050.15  -0.59   0.650.07  -0.", "local_index": 23}
{"chunk_id": "Lecture7_Dimensionality_Reduction_24", "doc_id": "Lecture7_Dimensionality_Reduction", "text": "\n0.14   0.02  -0.010.41   0.07  -0.030.55   0.09  -0.040.69   0.11  -0.050.15  -0.59   0.650.07  -0.73  -0.680.07  -0.29   0.32\n12.4  0     00       9.5  00       0     1.30.56   0.59  0.56   0.09    0.090.13  -0.02  0.12  -0.70  -0.700.41  -0.80  0.40   0.09    0.0923COMP4434\nSingular values also represent the variance\nNew Jersey Institute of Technology\nSVD: minimizing reconstruction errors§How to choose 𝑣$? Minimize reconstruction error§Goal: Minimize the sumof reconstruction errors:          ", "local_index": 24}
{"chunk_id": "Lecture7_Dimensionality_Reduction_25", "doc_id": "Lecture7_Dimensionality_Reduction", "text": "o choose 𝑣$? Minimize reconstruction error§Goal: Minimize the sumof reconstruction errors:          ∑%&$'∑(&$)𝑎%(−𝑏%(*\n§where 𝒂() are the original values in matrix A and 𝒃() are the reconstructed onesSVD gives ‘best’ axis to project on:§‘best’ = minimizing the reconstruction errors§In other words, minimum reconstruction error\nv1\nfirst right singular vector\nMovie 1 rating\nMovie 2 rating\n24COMP4434\nNew Jersey Institute of Technology\nSVD – Best Low Rank Approx.\nA USigma\nVT=\nB USigma\nVT=\nB is best a", "local_index": 25}
{"chunk_id": "Lecture7_Dimensionality_Reduction_26", "doc_id": "Lecture7_Dimensionality_Reduction", "text": "New Jersey Institute of Technology\nSVD – Best Low Rank Approx.\nA USigma\nVT=\nB USigma\nVT=\nB is best approximation of  A\n25COMP4434\nNew Jersey Institute of Technology\nSVD – Best Low Rank Approx.§Theorem:Let A = U S VT and B = U S VT where S = diagonal rxr matrix with si=σi (i=1…k) else si=0then B is a best rank(B)=k approx. to AWhat do we mean by “best”:§B is a solution to minB ǁA-BǁF  where rank(B)=k\nΣ𝜎!!\n𝜎\"\"\n𝐴−𝐵#=&$%𝐴$%−𝐵$%&\n26COMP4434\nNew Jersey Institute of Technology\n§Best approximation for r", "local_index": 26}
{"chunk_id": "Lecture7_Dimensionality_Reduction_27", "doc_id": "Lecture7_Dimensionality_Reduction", "text": "=k\nΣ𝜎!!\n𝜎\"\"\n𝐴−𝐵#=&$%𝐴$%−𝐵$%&\n26COMP4434\nNew Jersey Institute of Technology\n§Best approximation for rank = 2§                                                    is minimum \nUsers-to-Movies Example\n»1   1   1   0   03   3   3   0   04   4   4   0   05   5   5   0   00   2   0   4   40   0   0   5   50   1   0   2   2\n0.94  1.01   0.94  -0.00  -0.00 2.98  3.04   2.98  -0.00  -0.00 3.98  4.05   3.98  -0.00  -0.00 4.97  5.06   4.97  -0.01  -0.01 0.36  1.29   0.36  4.08   4.08-0.37  0.73  -0.37 4.92  ", "local_index": 27}
{"chunk_id": "Lecture7_Dimensionality_Reduction_28", "doc_id": "Lecture7_Dimensionality_Reduction", "text": "-0.00  -0.00 4.97  5.06   4.97  -0.01  -0.01 0.36  1.29   0.36  4.08   4.08-0.37  0.73  -0.37 4.92   4.92 0.18  0.65   0.18   2.04   2.04Frobenius norm:ǁMǁF = ÖΣij Mij2\nǁA-BǁF = Ö Σij (Aij-Bij)2 \n27COMP4434\nNew Jersey Institute of Technology\nSVD - Complexity§A= U S VT: unique§SVD: picks up linear correlations§To compute SVD:§O(nm2) or O(n2m) (whichever is less)§But:§Less work, if we just want singular values§or if we want first k singular vectors§or if the matrix is sparse§Implemented in linear ", "local_index": 28}
{"chunk_id": "Lecture7_Dimensionality_Reduction_29", "doc_id": "Lecture7_Dimensionality_Reduction", "text": "ular values§or if we want first k singular vectors§or if the matrix is sparse§Implemented in linear algebra packages like§LINPACK, Matlab, SPlus, Mathematica ...§numpy.linalg.svd in Python28COMP4434", "local_index": 29}
{"chunk_id": "Lecture8_Multilayer_Perceptron_Backpropagation_0", "doc_id": "Lecture8_Multilayer_Perceptron_Backpropagation", "text": "COMP4434 Big Data AnalyticsLecture 8 Multilayer Perceptron & BackpropagationHUANG Xiaoxiaohuang@comp.polyu.edu.hk\n\nNew Jersey Institute of Technology\nNeural Network§Very powerful, but dumped for decades because of its complexity and large computation resources§First AI Winter (approximately1974–1980); Second AI Winter (1987–2000)§Resurrect in recent years because of much higher computation power (e.g., GPU) and much more training examples (big data)\n3Source: https://sefiks.com/2017/10/14/evoluti", "local_index": 0}
{"chunk_id": "Lecture8_Multilayer_Perceptron_Backpropagation_1", "doc_id": "Lecture8_Multilayer_Perceptron_Backpropagation", "text": "e.g., GPU) and much more training examples (big data)\n3Source: https://sefiks.com/2017/10/14/evolution-of-neural-networks/\nNew Jersey Institute of Technology\nXOR Problem \n4COMP4434\n\nNew Jersey Institute of Technology\nApplications§Computer Vision§Classifying large number of images in Google Image§Search for “cats” §Auto labeling an image§Face recognition§Speech Recognition§iPhone Siri§Natural Language Processing§ChatGPT\n5COMP4434\n\nNew Jersey Institute of Technology\nInspired by Human Brain§Ourbrai", "local_index": 1}
{"chunk_id": "Lecture8_Multilayer_Perceptron_Backpropagation_2", "doc_id": "Lecture8_Multilayer_Perceptron_Backpropagation", "text": "age Processing§ChatGPT\n5COMP4434\n\nNew Jersey Institute of Technology\nInspired by Human Brain§Ourbrain has lots of neurons connected together§Humanbrain is a graph/network of 100B nodes and 700T edges\n6COMP4434\n§The strength of the connections between neurons represents long term knowledge\nNew Jersey Institute of Technology\nModel§It learns new features from your input features§Its architecture is based on our brain structure§The axon terminal of one neuron connects with the dendrite of another ne", "local_index": 2}
{"chunk_id": "Lecture8_Multilayer_Perceptron_Backpropagation_3", "doc_id": "Lecture8_Multilayer_Perceptron_Backpropagation", "text": "ased on our brain structure§The axon terminal of one neuron connects with the dendrite of another neuron which consists a quite complicated neural network\n7COMP4434\n\nNew Jersey Institute of Technology\nPerceptron: The Artificial Neuron\n§!! are input nodes and ℎ\"(!) is the output§Node that takes the input represents the body of Neuron\n8COMP4434\n\nNew Jersey Institute of Technology\nA Linear Neuron\n9COMP4434\n§Take a weighted sum of the inputs and set the output as one only when the sum is more than a", "local_index": 3}
{"chunk_id": "Lecture8_Multilayer_Perceptron_Backpropagation_4", "doc_id": "Lecture8_Multilayer_Perceptron_Backpropagation", "text": "MP4434\n§Take a weighted sum of the inputs and set the output as one only when the sum is more than an arbitrary threshold !§Threshold is learn-able by adding an input with the weight −!\nNew Jersey Institute of Technology\nLinear vs Logistic Neuron\n10COMP4434\nx1\nx2\nx3\nx0\n %#%$%%%&\n&=(1ℎ\"!≥00ℎ\"!<0\nℎ\"!\n§Linear:  ℎ\"!=∑%!!!=%'!§Logistic: ℎ\"!=.%'!=$$()!\"#$&=(1ℎ\"!≥0.50ℎ\"!<0.5Activation function\nNew Jersey Institute of Technology\nAND Example\n§AND: &=!$∧!%\nθ0=?θ1=?\nθ2=?\n11COMP4434\n!! !\" ℎ0 0 00 1 01 0 01 ", "local_index": 4}
{"chunk_id": "Lecture8_Multilayer_Perceptron_Backpropagation_5", "doc_id": "Lecture8_Multilayer_Perceptron_Backpropagation", "text": " Institute of Technology\nAND Example\n§AND: &=!$∧!%\nθ0=?θ1=?\nθ2=?\n11COMP4434\n!! !\" ℎ0 0 00 1 01 0 01 1 1\nNew Jersey Institute of Technology\nAND Example\n§AND: &=!$∧!%\nθ0 = -30θ1 = 20\nθ2 = 20\ng ( z )=\n1\n1+ e \u0000 z\n12\n!! !\" %('#+'!!!+'\"!\") 0 0 %(−30)≈00 1 %(−10)≈01 0 %(−10)≈01 1 %(10)≈1COMP4434\nNew Jersey Institute of Technology\nOR Example\n§OR: &=!$∨!%\nθ0=?θ1=?θ2=?\n13COMP4434\n!! !\" ℎ0 0 00 1 11 0 11 1 1\nNew Jersey Institute of Technology\nOR Example\nθ0=-10θ1=20\nθ2=20 !! !\" %('#+'!!!+'\"!\") 0 0 %(−10)≈00", "local_index": 5}
{"chunk_id": "Lecture8_Multilayer_Perceptron_Backpropagation_6", "doc_id": "Lecture8_Multilayer_Perceptron_Backpropagation", "text": "\nNew Jersey Institute of Technology\nOR Example\nθ0=-10θ1=20\nθ2=20 !! !\" %('#+'!!!+'\"!\") 0 0 %(−10)≈00 1 %(10)≈11 0 %(10)≈11 1 %(30)≈1\n14COMP4434\ng ( z )=\n1\n1+ e \u0000 z\n§OR: &=!$∨!%\nNew Jersey Institute of Technology\nLinear Hypothesis\n15\n§AND: −30+20!$+20!%=0x2\nx1\nx2\nx1\n§OR: −10+20!$+20!%=0\nCOMP4434\nIn addition, NOT and a great number of Boolean functions can also be represented by single-layer neural networks. \nNew Jersey Institute of Technology\nNon-linear Hypothesis§XOR: However, you can’t find a s", "local_index": 6}
{"chunk_id": "Lecture8_Multilayer_Perceptron_Backpropagation_7", "doc_id": "Lecture8_Multilayer_Perceptron_Backpropagation", "text": "networks. \nNew Jersey Institute of Technology\nNon-linear Hypothesis§XOR: However, you can’t find a straight line to separate the two classesx2\nx1\n16COMP4434\n!! !\" ℎ0 0 00 1 11 0 11 1 0？\nNew Jersey Institute of Technology\nXOR Example\na1\n-102020\na2\n30-20-20\n-302020\nx1x2a1a2 h00010011111011111100\n17COMP4434\n6$=.(−10+20!$+20!%)6%=.(30−20!$−20!%)ℎ!=.(−30+206$+206%)=.(−30+20.(−10+20!$+20!%)+20.(30−20!$−20!%)) \nNew Jersey Institute of Technology\nIntuition\n18COMP4434\nhttps://towardsdatascience.com/how-n", "local_index": 7}
{"chunk_id": "Lecture8_Multilayer_Perceptron_Backpropagation_8", "doc_id": "Lecture8_Multilayer_Perceptron_Backpropagation", "text": "20!%)) \nNew Jersey Institute of Technology\nIntuition\n18COMP4434\nhttps://towardsdatascience.com/how-neural-networks-solve-the-xor-problem-59763136bdd7\nNew Jersey Institute of Technology\nNeural Network \n19COMP4434\n\nNew Jersey Institute of Technology\nRecap: Logistic RegressionClassifier§ℎ,\"=$%-\"=../0!\"#$∈(0,1)§Predict ,=1 when ℎ,\"≥0.5, i.e., %-\"≥0§Predict ,=0 when\t%-\"<0\n20COMP4434\nNew Jersey Institute of Technology\nRecap: One-Layer Neuron\n21\nx1\nx2\nx3\nx0\n %#%$%%%& ℎ\"!\n§Logistic: ℎ\"!=.%'!=$$()!\"#$&=(", "local_index": 8}
{"chunk_id": "Lecture8_Multilayer_Perceptron_Backpropagation_9", "doc_id": "Lecture8_Multilayer_Perceptron_Backpropagation", "text": "e of Technology\nRecap: One-Layer Neuron\n21\nx1\nx2\nx3\nx0\n %#%$%%%& ℎ\"!\n§Logistic: ℎ\"!=.%'!=$$()!\"#$&=(1ℎ\"!≥0.50ℎ\"!<0.5Activation function\nθ0 = -30θ1 = 20\nθ2 = 20\n!! !\" %('#+'!!!+'\"!\") 0 0 %(−30)≈00 1 %(−10)≈01 0 %(−10)≈01 1 %(10)≈1\nNew Jersey Institute of Technology\nNeural Network \n22COMP4434\na1\n-302020\na2\n10-20-20\n-102020\nx1x2a1a2 h00011010001000011101 XNOR\nNew Jersey Institute of Technology\nExample: Car Detection\n23COMP4434\nNew Jersey Institute of Technology\nPixels as features\n24COMP4434\nNew Jer", "local_index": 9}
{"chunk_id": "Lecture8_Multilayer_Perceptron_Backpropagation_10", "doc_id": "Lecture8_Multilayer_Perceptron_Backpropagation", "text": "e: Car Detection\n23COMP4434\nNew Jersey Institute of Technology\nPixels as features\n24COMP4434\nNew Jersey Institute of Technology\nHigh-order polynomials vs neural network§Need non-linear decision boundary§Need complicated hypothesis with different combinations of features\n§The computation complexity is extremely huge, hence we have to take another method, called Neural Network\n25COMP4434\nNew Jersey Institute of Technology\nFeature Learning§A network function associated with a neural network charact", "local_index": 10}
{"chunk_id": "Lecture8_Multilayer_Perceptron_Backpropagation_11", "doc_id": "Lecture8_Multilayer_Perceptron_Backpropagation", "text": "Institute of Technology\nFeature Learning§A network function associated with a neural network characterizes the relationship between input and output layers, which is parameterized by the weights§With appropriately defined network functions, various learning tasks can be performed by minimizing a cost function over the network function (weights)\n26COMP4434\n\nNew Jersey Institute of Technology\nNotations in neural networks\n27COMP4434\n!!(#): activation of unit \"\tat layer $\tΘ%!#\t: weight on link from ", "local_index": 11}
{"chunk_id": "Lecture8_Multilayer_Perceptron_Backpropagation_12", "doc_id": "Lecture8_Multilayer_Perceptron_Backpropagation", "text": "ons in neural networks\n27COMP4434\n!!(#): activation of unit \"\tat layer $\tΘ%!#\t: weight on link from !!(#) to !%#&'; !!(')= &! \n\nNew Jersey Institute of Technology\nModel Representation\n§Connections between layer 1 and layer 2\n§Connections between layer 2 and layer 3\n28COMP4434\n6$%=.Θ$#$!#+Θ$$$!$+Θ$%$!%+Θ$&$!&6%%=.Θ%#$!#+Θ%$$!$+Θ%%$!%+Θ%&$!&6&%=.Θ&#$!#+Θ&$$!$+Θ&%$!%+Θ&&$!&\nℎ\"!=6$&=.Θ$#%6#(%)+Θ$$%6$(%)+Θ$%%6%(%)+Θ$&%6&(%)\n!!(#): activation of unit \"\tat layer $\tΘ%!#\t: weight on link from !!(#) to !%", "local_index": 12}
{"chunk_id": "Lecture8_Multilayer_Perceptron_Backpropagation_13", "doc_id": "Lecture8_Multilayer_Perceptron_Backpropagation", "text": "%)+Θ$%%6%(%)+Θ$&%6&(%)\n!!(#): activation of unit \"\tat layer $\tΘ%!#\t: weight on link from !!(#) to !%#&'; !!(')= &! \nNew Jersey Institute of Technology\nExercise§Assume that we have a neuron that takes 3 inputs. Its weight vector corresponding to the 3 inputs is [−0.1, 0.4, 0.6]. The weight of the bias is w0 = 0.7. If the input vector is [0.3, −0.4, 0.5], then what is the output of this neuron? (Assume that the activation function is g(z), and you only need to compute the z.)§g(0.7 + (−0.1) ∗ 0.3 ", "local_index": 13}
{"chunk_id": "Lecture8_Multilayer_Perceptron_Backpropagation_14", "doc_id": "Lecture8_Multilayer_Perceptron_Backpropagation", "text": "ume that the activation function is g(z), and you only need to compute the z.)§g(0.7 + (−0.1) ∗ 0.3 + 0.4 ∗ (−0.4) + 0.6 ∗ 0.5) = g(0.81)§0.81\n29COMP4434\nNew Jersey Institute of Technology\nGradient Descent Framework\nΘ!,(-)=Θ!,(-)−a:;(Θ):Θ!,(-)Repeat until convergence {\n} 30COMP4434\n1=ℎ$!! 3%('(!)3*(')\n\nNew Jersey Institute of Technology\nGradient Computation Preparation§Simplification§Single sample '=1, penalized items ignored *=0 §Linear perceptron§,-='(∑%)'*ℎ+&%−1%(\n§Chain Rule§If  2=3(1) and 1", "local_index": 14}
{"chunk_id": "Lecture8_Multilayer_Perceptron_Backpropagation_15", "doc_id": "Lecture8_Multilayer_Perceptron_Backpropagation", "text": " '=1, penalized items ignored *=0 §Linear perceptron§,-='(∑%)'*ℎ+&%−1%(\n§Chain Rule§If  2=3(1) and 1=6(&), then ,-,.=,-,/7,/,.=3′(1)76′(&)§Example: for  2(&)='(&−5( (2='(1(;1=&−5), we have 2′(&)= 301760&=171=&−5\n31COMP4434\nNew Jersey Institute of Technology\nOutput Layer\n32COMP4434\n1!(!)2!\nLayer 1Layer 2Layer 3\n3$$(%)\n2&\n2'\n1&(!)\n1'(!)\n1&(')\n1!(')\nNew Jersey Institute of Technology\nGradient Derivation4Θ=12ℎ$!!−1!\"+12ℎ$!\"−1\"\"\n84Θ8Θ!!(\")=812ℎ$!!−1!\"\n8Θ!!(\") +0\n=8123!+−1!\"\n83!+ 83!+8Θ!!(\")=3!+−1!983", "local_index": 15}
{"chunk_id": "Lecture8_Multilayer_Perceptron_Backpropagation_16", "doc_id": "Lecture8_Multilayer_Perceptron_Backpropagation", "text": "ion4Θ=12ℎ$!!−1!\"+12ℎ$!\"−1\"\"\n84Θ8Θ!!(\")=812ℎ$!!−1!\"\n8Θ!!(\") +0\n=8123!+−1!\"\n83!+ 83!+8Θ!!(\")=3!+−1!983!+8Θ!!(\")\n=3!+−1!98Θ!#\"3#\"+Θ!!\"3!\"+Θ!\"\"3\"\"+Θ!+\"3+\"8Θ!!(\")=3!+−1!3!\"=:!(+)3!(\") 33\n1!(!)2!\n3$$(%)\n2&\n2'\n1&(!)\n1'(!)\n1&(')\n1!(')\n(%\t: delta)\nNew Jersey Institute of Technology\nGradient Computation\n34COMP4434\n1!(!)2!\nLayer 1Layer 2Layer 3\n84;8;!!(\")=:!(+)3!(\")\n2&\n2'\n1&(!)\n1'(!)\n1&(')\n1!(')\n:!(+)=3!+−1!\n3$$(%)\nNew Jersey Institute of Technology\nHidden Layer\n35COMP4434\n1!(!)2!\nLayer 1Layer 2Layer 3\n3%%", "local_index": 16}
{"chunk_id": "Lecture8_Multilayer_Perceptron_Backpropagation_17", "doc_id": "Lecture8_Multilayer_Perceptron_Backpropagation", "text": "\n3$$(%)\nNew Jersey Institute of Technology\nHidden Layer\n35COMP4434\n1!(!)2!\nLayer 1Layer 2Layer 3\n3%%($)\n2&\n2'\n1&(!)\n1'(!)\n1&(')\n1!(')\nNew Jersey Institute of Technology\nDerivation.Θ=12ℎ!4\"−6\"#+12ℎ!4#−6##\n8.Θ8Θ##(\")=8129\"&−6\"#\n8Θ##(\") +8129#&−6##\n8Θ##(\")\n=8129\"&−6\"#\n89\"& 89\"&8Θ##(\")+8129#&−6##\n89#& 89#&8Θ##(\")\n=9\"&−6\"89\"&8Θ##(\")+9#&−6#89#&8Θ##(\")\n=9\"&−6\"8Θ\"'#9'#+Θ\"\"#9\"#+Θ\"##9##+Θ\"&#9&#8Θ##(\")\n+9#&−6#8Θ#'#9'#+Θ#\"#9\"#+Θ###9##+Θ#&#9&#\n8Θ##(\") 36COMP4434\n(!(!))!\nLayer 1Layer 2Layer 3\n*$$(%)\n)&\n)'\n(&(", "local_index": 17}
{"chunk_id": "Lecture8_Multilayer_Perceptron_Backpropagation_18", "doc_id": "Lecture8_Multilayer_Perceptron_Backpropagation", "text": "6#8Θ#'#9'#+Θ#\"#9\"#+Θ###9##+Θ#&#9&#\n8Θ##(\") 36COMP4434\n(!(!))!\nLayer 1Layer 2Layer 3\n*$$(%)\n)&\n)'\n(&(!)\n('(!)\n(&(')\n(!(')\nNew Jersey Institute of Technology\nDerivation=9\"&−6\"8Θ\"'#9'#+Θ\"\"#9\"#+Θ\"##9##+Θ\"&#9&#\n89## :89##8Θ##(\")\n+9#&−6#8Θ#'#9'#+Θ#\"#9\"#+Θ###9##+Θ#&#9&#89## :89##8Θ##(\")\n=9\"&−6\"Θ\"##+9#&−6#Θ###:89##8Θ##\"\n=9\"&−6\"Θ\"##+9#&−6#Θ###:8Θ#'\"4'+Θ#\"\"4\"+Θ##\"4#+Θ#&\"4&8Θ##\"=9\"&−6\"Θ\"##+9#&−6#Θ###4#=;\"(&)Θ\"##+;#(&)Θ###4#\n37COMP4434\n(!(!))!\nLayer 1Layer 2Layer 3\n*$$(%)\n)&\n)'\n(&(!)\n('(!)\n(&(')\n(!(')\nNew J", "local_index": 18}
{"chunk_id": "Lecture8_Multilayer_Perceptron_Backpropagation_19", "doc_id": "Lecture8_Multilayer_Perceptron_Backpropagation", "text": "Θ\"##+;#(&)Θ###4#\n37COMP4434\n(!(!))!\nLayer 1Layer 2Layer 3\n*$$(%)\n)&\n)'\n(&(!)\n('(!)\n(&(')\n(!(')\nNew Jersey Institute of Technology\nGradient Computation\n38COMP4434\n1!(!)2!\nLayer 1Layer 2Layer 3\n2&\n2'\n1&(!)\n1'(!)\n1&(')\n1!(')\n:!(+)=3!+−1!\n:\"(+)=3\"+−1\"\n3$%(%)\n3%%(%)\n3%%($)\n8.Θ8Θ##(\")=9\"&−6\"Θ\"##+9#&−6#Θ###4#\n=;\"(&)Θ\"##+;#(&)Θ###4#\t=;#(#)4#\n84;8;!!(\")=:!(+)3!(\")\nNew Jersey Institute of Technology\nDerivation – Output Layer84Θ8Θ%*(')=8123%,−1%\"\n8Θ%*(') =8123%,−1%\"\n83%, 83%,8Θ%*'=3%,−1%983%,8Θ%*'\n=3%,−1%9", "local_index": 19}
{"chunk_id": "Lecture8_Multilayer_Perceptron_Backpropagation_20", "doc_id": "Lecture8_Multilayer_Perceptron_Backpropagation", "text": "ion – Output Layer84Θ8Θ%*(')=8123%,−1%\"\n8Θ%*(') =8123%,−1%\"\n83%, 83%,8Θ%*'=3%,−1%983%,8Θ%*'\n=3%,−1%98∑*-#.+Θ%*(')3*(')8Θ%*' =3%,−1%3*'=:%(,)3*(')\nError generated in the output layer\n39COMP4434\n1\n 1\n'!(#) '%(#&')'((#)')\n (*+(,)-) (.*(,)\n\nNew Jersey Institute of Technology\nDerivation – Hidden Layer 18.Θ8Θ()(*+\")==,-\"\n.!8.,Θ8Θ()(*+\")==,-\"\n.!8129,(/)−6,#\n89,(/) :89,/8Θ()(*+\")\n==,-\"\n.! 9,(/)−6,:8∑0-'.\"Θ,0(*)90(*)\n8Θ()(*+\") ==,-\"\n.!;,(/):8∑0-'.\"Θ,0(*)90(*)\n89((*) :89((*)8Θ()(*+\")\n==,-\"\n.\"#$;,(*1\"):Θ,(", "local_index": 20}
{"chunk_id": "Lecture8_Multilayer_Perceptron_Backpropagation_21", "doc_id": "Lecture8_Multilayer_Perceptron_Backpropagation", "text": "Θ,0(*)90(*)\n8Θ()(*+\") ==,-\"\n.!;,(/):8∑0-'.\"Θ,0(*)90(*)\n89((*) :89((*)8Θ()(*+\")\n==,-\"\n.\"#$;,(*1\"):Θ,((*):8∑)-'.\"%$Θ()(*+\")9)(*+\")\n8Θ()(*+\") \t\n==,-\"\n.\"#$;,(*1\")Θ,((*)9)*+\"=;((*)9)(*+\")\n40\nError generated in the hidden layer\n1\n 1\n'!(#) '%(#&')'((#)')\n (*+(,)-) (.*(,)\n8.Θ8Θ##(\")=;\"(&)Θ\"##+;#(&)Θ###4#\t=;#(#)4#\nNew Jersey Institute of Technology\nDirectly Applying Gradient Descent is Expensive8.Θ8Θ)2(*+#)==,-\"\n.!8.,Θ8Θ)2(*+#)==,-\"\n.!8129,(/)−6,#\n89,(/) :89,/8Θ)2(*+#)\n==,-\"\n.! 9,(/)−6,:8∑0-'.\"Θ,0(*)90(*", "local_index": 21}
{"chunk_id": "Lecture8_Multilayer_Perceptron_Backpropagation_22", "doc_id": "Lecture8_Multilayer_Perceptron_Backpropagation", "text": ")==,-\"\n.!8.,Θ8Θ)2(*+#)==,-\"\n.!8129,(/)−6,#\n89,(/) :89,/8Θ)2(*+#)\n==,-\"\n.! 9,(/)−6,:8∑0-'.\"Θ,0(*)90(*)\n8Θ)2(*+#)\n==,-\"\n.!;,(/):=0-'\n.\"8Θ,0(*)90(*)890(*):890(*)8Θ)2(*+#)\n==,-\"\n.!;,(/):=0-'\n.\"Θ,0(*):8∑3-'.\"%$Θ03(*+\")93(*+\")\n8Θ)2(*+#)\n41COMP4434\n(*+(,)-) (.*,(+/(,)0) '%(#&')'!(#)'((#)')\nNew Jersey Institute of Technology\nGeneral Network\n42\n1\n 1\n'!(#) '%(#&')'((#)')\nLayer L-2 Layer L-1 Layer L\n(*+(,)-) (.*(,)\nCOMP4434\nNew Jersey Institute of Technology\nGradient Propagation\n43COMP4434\n1!(!)2!\nLayer 1L", "local_index": 22}
{"chunk_id": "Lecture8_Multilayer_Perceptron_Backpropagation_23", "doc_id": "Lecture8_Multilayer_Perceptron_Backpropagation", "text": " (.*(,)\nCOMP4434\nNew Jersey Institute of Technology\nGradient Propagation\n43COMP4434\n1!(!)2!\nLayer 1Layer 2Layer 3\n45646&&(!)=45641&'41&'46&&(!)=8&(')1&(!)\n2&\n2'\n1&(!)\n1'(!)\n1&(')\n1!(')\n8&(')=1&'−:&=45641&'\n3$$(%)\n%'(1)\nNew Jersey Institute of Technology\nGradient Propagation\n44COMP4434\n1!(!)2!\nLayer 1Layer 2Layer 3\n45646!!(&)=8!(!)2!=8&(')Θ&!!+8!(')Θ!!!2!\n2&\n2'\n1&(!)\n1'(!)\n1&(')\n1!(')\n8&(')=1&'−:&=45641&'\n8!(')=1!'−:!=45641!'\n%'(1)Θ'22\n%2(1)Θ222\n3%%($)\n8!(!)=45641!!=8&(')Θ&!!+8!(')Θ!!!\n%'(1)\n%2(1", "local_index": 23}
{"chunk_id": "Lecture8_Multilayer_Perceptron_Backpropagation_24", "doc_id": "Lecture8_Multilayer_Perceptron_Backpropagation", "text": "45641&'\n8!(')=1!'−:!=45641!'\n%'(1)Θ'22\n%2(1)Θ222\n3%%($)\n8!(!)=45641!!=8&(')Θ&!!+8!(')Θ!!!\n%'(1)\n%2(1)\n%2(2)\nNew Jersey Institute of Technology\nBack Propagation\n:%(,)=3%,−1%\n:*(')==%-!\n.+,-:%('(!)Θ%*(')\n45COMP4434\n(!(!))!\n *$$(%)\n)&\n)'\n(&(!)\n('(!)\n(&(')\n(!(')\n8.Θ8Θ()(*+\")=;((*)9)(*+\")\n84Θ8Θ%*(')=:%(,)3*(')\nNew Jersey Institute of Technology\nBack Propagation AlgorithmTraining Set !($),&($),⋯,!(?),&(?)Set ∆!,(-)=0, for all (?,@,A)For B=1 to C Set 6($)=!(@) Performance Forward Propagation to compute", "local_index": 24}
{"chunk_id": "Lecture8_Multilayer_Perceptron_Backpropagation_25", "doc_id": "Lecture8_Multilayer_Perceptron_Backpropagation", "text": "?)Set ∆!,(-)=0, for all (?,@,A)For B=1 to C Set 6($)=!(@) Performance Forward Propagation to compute 6(%)⋯,6(A) Set D(A)=6(A)−&(@) Performance Backward Propagation to compute D(AB$)⋯,D% Set ∆!,(-)=∆!,(-)+D!(-($)6,(-), for all (?,@,A)\n46COMP4434\n:%(,)=3%,−1%\n:*(')==%-!\n.+,-:%('(!)Θ%*(')\nNew Jersey Institute of Technology\nGradient Descent Algorithm\n4Θ=12>=%-!\n/=0-!\n..ℎ$!%0−1%\"+?2>='-!\n,1!=*-!\n.+=%-!\n.+,-Θ%*(')\"\nRepeat until convergence {\n}Θ,((*)=Θ,((*)−a8.(Θ)8Θ,((*)\n84(Θ)8Θ%*(')=1>∆%*(')+?>Θ%*(')A", "local_index": 25}
{"chunk_id": "Lecture8_Multilayer_Perceptron_Backpropagation_26", "doc_id": "Lecture8_Multilayer_Perceptron_Backpropagation", "text": "\n.+,-Θ%*(')\"\nRepeat until convergence {\n}Θ,((*)=Θ,((*)−a8.(Θ)8Θ,((*)\n84(Θ)8Θ%*(')=1>∆%*(')+?>Θ%*(')A≠C1>∆%*(') A=C\nObtained by the Backward Propagation Algorithm47COMP4434\nNew Jersey Institute of Technology\nImplementation Detail§Important to randomize initial weights Θ in the network§Can’t have uniform initial weights, otherwise all updates will be identical, and the network won’t learn anything.\n48COMP4434\n1&(!)2!\nLayer 1Layer 2Layer 3\n2&\n2' 1!(!)\n1&(')\nZero Initialization Example:If all weight", "local_index": 26}
{"chunk_id": "Lecture8_Multilayer_Perceptron_Backpropagation_27", "doc_id": "Lecture8_Multilayer_Perceptron_Backpropagation", "text": "48COMP4434\n1&(!)2!\nLayer 1Layer 2Layer 3\n2&\n2' 1!(!)\n1&(')\nZero Initialization Example:If all weights are initialized to 0, 9\"(#)= 9#(#), also ;\"(#)= ;#(#).45647$&=45647&&, thus Θ'\"(\")=Θ'#(\")…, 9\"(#)= 9#(#).After each update, two hidden units in Layer 2 are identical.\nNew Jersey Institute of Technology\nExample\n49\nThe neural network given below adopts sigmoid function as its activation function C4, e.g., 9\"&=CΘ\"'#+Θ\"\"#9\"(#)+Θ\"##9#(#). The cost function is defined as .Θ=\"#9\"&−6\"#+\"#9#&−6##. \nWe co", "local_index": 27}
{"chunk_id": "Lecture8_Multilayer_Perceptron_Backpropagation_28", "doc_id": "Lecture8_Multilayer_Perceptron_Backpropagation", "text": " e.g., 9\"&=CΘ\"'#+Θ\"\"#9\"(#)+Θ\"##9#(#). The cost function is defined as .Θ=\"#9\"&−6\"#+\"#9#&−6##. \nWe consider a single data sample\t4=0.050.1 and the corresponding label 6=0.010.99. Use the training data to develop the neural network model and solve it by using gradient descent algorithm with initialized setting: Θ\"[0]=0.150.250.350.200.300.35,Θ#[0]=0.400.500.60.450.550.6, and a=0.5.\n(!(!))!\nLayer 1Layer 2Layer 3\n*$$(%)\n)&\n1\n(&(!)\n1\n(&(')\n(!(')\nhttps://mattmazur.com/2015/03/17/a-step-by-step-backpro", "local_index": 28}
{"chunk_id": "Lecture8_Multilayer_Perceptron_Backpropagation_29", "doc_id": "Lecture8_Multilayer_Perceptron_Backpropagation", "text": "yer 2Layer 3\n*$$(%)\n)&\n1\n(&(!)\n1\n(&(')\n(!(')\nhttps://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\nNew Jersey Institute of Technology\nApplication: Multi-class Classification\n50COMP4434\nPedestrianCar MotorcycleTruckℎ2D∈F3G: The number of classesWe want:\nwhen pedestrianwhen carwhen motorcyclewhen truck\nℎ2D≈1000 ℎ2D≈0100 ℎ2D≈0010 ℎ2D≈0001\nNew Jersey Institute of Technology\nMulti-class Classification\n51COMP4434\nℎ2D∈F3G: The number of classesWe want:\nwhen pedestrianwhen carwhen mot", "local_index": 29}
{"chunk_id": "Lecture8_Multilayer_Perceptron_Backpropagation_30", "doc_id": "Lecture8_Multilayer_Perceptron_Backpropagation", "text": "ass Classification\n51COMP4434\nℎ2D∈F3G: The number of classesWe want:\nwhen pedestrianwhen carwhen motorcyclewhen truck\nℎ2D≈1000 ℎ2D≈0100 ℎ2D≈0010 ℎ2D≈0001\n•Given {D!,1!,D\",1\",…,D4,14}•Must convert labels to 1-of-K representatione.g., 1%=1000 when pedestrian, 1%=0010 when motorcycle.\nNew Jersey Institute of Technology\nCost Function\n.N=−1O=,-\"\n8=)-\"\n96),logℎ!4,)+1−6),log1−ℎ!4,)\n+S2O=*-\"\n/+\"=(-\"\n.\"=,-\"\n.\"#$Θ,((*)#\n§Cost function of K-class Neural Network\n52COMP4434\n=01class:         true, predictedN", "local_index": 30}
{"chunk_id": "Lecture8_Multilayer_Perceptron_Backpropagation_31", "doc_id": "Lecture8_Multilayer_Perceptron_Backpropagation", "text": "\"\n.\"#$Θ,((*)#\n§Cost function of K-class Neural Network\n52COMP4434\n=01class:         true, predictedNot =01class:  true, predicted§The ?-th output: ℎT!!\n§Recall that cost function of Logistic Regression:.N=−1O=,-\"\n86,logℎ:4, +1−6,log1−ℎ:4, +S2O=(-\"\n;N(#\nNew Jersey Institute of Technology\nStochastic Gradient Descent§Gradient descent, follows the gradient of an entire training set downhill§Stochastic gradient descent, follows the gradient of randomly selected minbatches downhill§minibatches: The gr", "local_index": 31}
{"chunk_id": "Lecture8_Multilayer_Perceptron_Backpropagation_32", "doc_id": "Lecture8_Multilayer_Perceptron_Backpropagation", "text": " gradient descent, follows the gradient of randomly selected minbatches downhill§minibatches: The gradients are calculated and the variables are updated iteratively with subsets of all observations§randomly divides the set of observations into minibatches§For each minibatch, the gradient is computed and the vector is moved§Once all minibatches are used, you say that the iteration, or epoch, is finished and start the next one53COMP4434\nNew Jersey Institute of Technology\nThe dropout regularization", "local_index": 32}
{"chunk_id": "Lecture8_Multilayer_Perceptron_Backpropagation_33", "doc_id": "Lecture8_Multilayer_Perceptron_Backpropagation", "text": "ished and start the next one53COMP4434\nNew Jersey Institute of Technology\nThe dropout regularization\n§Randomly shutdown a subset of units in training§It is a sparse representation§It is a different net each time, but all nets share the parameters§A net with n units can be seen as a collection of 2^n possible thinned nets, all of which share weights§At test time, it is a single net with averaging*(1-p), where p is the dropout rate§Avoid overfitting54COMP4434\n\nNew Jersey Institute of Technology\nmo", "local_index": 33}
{"chunk_id": "Lecture8_Multilayer_Perceptron_Backpropagation_34", "doc_id": "Lecture8_Multilayer_Perceptron_Backpropagation", "text": "1-p), where p is the dropout rate§Avoid overfitting54COMP4434\n\nNew Jersey Institute of Technology\nmodel.eval() and torch.no_grad()§model.eval() will notify all your layers that you are in eval mode, that way, batchnorm or dropout layers will work in eval mode instead of training mode.§torch.no_grad() impacts the autograd engine and deactivate it. It will reduce memory usage and speed up computations but you won’t be able to backprop (which you don’t want in an eval script).§model.train() tells y", "local_index": 34}
{"chunk_id": "Lecture8_Multilayer_Perceptron_Backpropagation_35", "doc_id": "Lecture8_Multilayer_Perceptron_Backpropagation", "text": "ns but you won’t be able to backprop (which you don’t want in an eval script).§model.train() tells your model that you are training the model. This helps inform layers such as Dropout and BatchNorm, which are designed to behave differently during training and evaluation. 55COMP4434", "local_index": 35}
{"chunk_id": "Lecture9_Convolutional_Neural_Networks_0", "doc_id": "Lecture9_Convolutional_Neural_Networks", "text": "COMP4434 Big Data AnalyticsLecture 9Convolutional Neural Networks HUANG Xiaoxiaohuang@comp.polyu.edu.hk\n\nNew Jersey Institute of Technology\nSmaller Network?\n§From this fully connected model, do we really need all the edges? §Can some of these be shared?\nCOMP4434 3\nNew Jersey Institute of Technology\nConsider learning an image:\n4COMP4434\n§Some patterns are much smaller than the whole image\n“beak”detector\nCan represent a small region with fewer parameters\nNew Jersey Institute of Technology\nSame pat", "local_index": 0}
{"chunk_id": "Lecture9_Convolutional_Neural_Networks_1", "doc_id": "Lecture9_Convolutional_Neural_Networks", "text": "ector\nCan represent a small region with fewer parameters\nNew Jersey Institute of Technology\nSame pattern appears in different places\n5COMP4434\n“upper-left beak”detector\n“middle beak”detector\nThey can be compressed to the same parameters.\n§They can be compressed!What about training a lot of such “small” detectorsand each detector must “move around”.\nNew Jersey Institute of Technology\nMLPvsconvolutional neural network\nA CNN arranges its neurons in three dimensions (width, height, depth). Every lay", "local_index": 1}
{"chunk_id": "Lecture9_Convolutional_Neural_Networks_2", "doc_id": "Lecture9_Convolutional_Neural_Networks", "text": "onal neural network\nA CNN arranges its neurons in three dimensions (width, height, depth). Every layer of a CNN transforms the 3D input volume to a 3D output volume. In this example, the red input layer holds the image, so its width and height would be the dimensions of the image, and the depth would be 3 (Red, Green, Blue channels)6COMP4434\nA regular 3-layer Neural Network.\nNew Jersey Institute of Technology\nA convolutional layer\nA filter\nA CNN is a neural network with some convolutional layers", "local_index": 2}
{"chunk_id": "Lecture9_Convolutional_Neural_Networks_3", "doc_id": "Lecture9_Convolutional_Neural_Networks", "text": "f Technology\nA convolutional layer\nA filter\nA CNN is a neural network with some convolutional layers (and some other layers).  A convolutional layer has a number of filters that does convolutional operation. \nBeak detector\n7COMP4434\nNew Jersey Institute of Technology\nConvolution\n8COMP4434\n1000010100100011001000100100100010106 x 6 image\n1-1-1-11-1-1-11Filter 1\n-11-1-11-1-11-1Filter 2……\nThese are the network parameters to be learned.\nEach filter detects a small pattern (3 x 3) \nNew Jersey Institut", "local_index": 3}
{"chunk_id": "Lecture9_Convolutional_Neural_Networks_4", "doc_id": "Lecture9_Convolutional_Neural_Networks", "text": "e network parameters to be learned.\nEach filter detects a small pattern (3 x 3) \nNew Jersey Institute of Technology\nConvolution\n9COMP4434\n1000010100100011001000100100100010106 x 6 image\n1-1-1-11-1-1-11Filter 1\n3\n-1\nstride=1\n Dot product\nNew Jersey Institute of Technology\nConvolution\n10COMP4434\n1000010100100011001000100100100010106 x 6 image\n1-1-1-11-1-1-11Filter 1\n3\n-3\nIf stride=2\nNew Jersey Institute of Technology\nConvolution\n11COMP4434\n1000010100100011001000100100100010106 x 6 image\n1-1-1-11-1", "local_index": 4}
{"chunk_id": "Lecture9_Convolutional_Neural_Networks_5", "doc_id": "Lecture9_Convolutional_Neural_Networks", "text": "tute of Technology\nConvolution\n11COMP4434\n1000010100100011001000100100100010106 x 6 image\n1-1-1-11-1-1-11Filter 1\n3\n-1\n-3\n-1\n-3\n1\n0\n-3\n-3\n-3\n0\n1\n3\n-2\n-2\n-1\nstride=1\nNew Jersey Institute of Technology\nConvolution\n12COMP4434\n1000010100100011001000100100100010106 x 6 image\n3\n-1\n-3\n-1\n-3\n1\n0\n-3\n-3\n-3\n0\n1\n3\n-2\n-2\n-1\n-11-1-11-1-11-1Filter 2\n-1\n-1\n-1\n-1\n-1\n-1\n-2\n1\n-1\n-1\n-2\n1\n-1\n0\n-4\n3\nRepeat this for each filterstride=1\nTwo 4 x 4 imagesForming 2 x 4 x 4 matrix\nFeatureMap\nNew Jersey Institute of Technol", "local_index": 5}
{"chunk_id": "Lecture9_Convolutional_Neural_Networks_6", "doc_id": "Lecture9_Convolutional_Neural_Networks", "text": "h filterstride=1\nTwo 4 x 4 imagesForming 2 x 4 x 4 matrix\nFeatureMap\nNew Jersey Institute of Technology\nConvolution vs Fully Connected\n13COMP4434\n100001010010001100100010010010001010image\nconvolution\n-11-1-11-1-11-11-1-1-11-1-1-11\n1x2x\n……\n36x…100001010010001100100010010010001010\nFully-connected\nNew Jersey Institute of Technology\n1000010100100011001000100100100010106 x 6 image\n1-1-1-11-1-1-11Filter 1\n123…89…131415…\nOnly connect to 9 inputs, not fully connected\n4:\n10:\n16\n100001000011\n3\nfewer param", "local_index": 6}
{"chunk_id": "Lecture9_Convolutional_Neural_Networks_7", "doc_id": "Lecture9_Convolutional_Neural_Networks", "text": " 1\n123…89…131415…\nOnly connect to 9 inputs, not fully connected\n4:\n10:\n16\n100001000011\n3\nfewer parameters!COMP4434\nNew Jersey Institute of Technology\n100001010010001100100010010010001010\n1-1-1-11-1-1-11Filter 11:2:3:…7:8:9:…13:14:15:…\n4:\n10:\n16:\n100001000011\n3\n-1\nShared weights\n6 x 6 image\nFewer parameters\nEven fewer parameters15\nNew Jersey Institute of Technology\nThe whole CNN\n16\nFully Connected Feedforward network\ncat dog ……\nConvolution\nMax Pooling\nConvolution\nMax Pooling\nFlattened\nCan repeat ", "local_index": 7}
{"chunk_id": "Lecture9_Convolutional_Neural_Networks_8", "doc_id": "Lecture9_Convolutional_Neural_Networks", "text": "Feedforward network\ncat dog ……\nConvolution\nMax Pooling\nConvolution\nMax Pooling\nFlattened\nCan repeat many times\nNew Jersey Institute of Technology\nMax Pooling\n17COMP4434\n3\n-1\n-3\n-1\n-3\n1\n0\n-3\n-3\n-3\n0\n1\n3\n-2\n-2\n-1\n-11-1-11-1-11-1Filter 2\n-1\n-1\n-1\n-1\n-1\n-1\n-2\n1\n-1\n-1\n-2\n1\n-1\n0\n-4\n3\n1-1-1-11-1-1-11Filter 1\nNew Jersey Institute of Technology\nMax Pooling\n18COMP4434\n1000010100100011001000100100100010106 x 6 image\n3\n 0\n1\n3\n-1\n 1\n3\n02 x 2 imageEach filter is a channel\nNew image but smallerConv\nMaxPooling\n", "local_index": 8}
{"chunk_id": "Lecture9_Convolutional_Neural_Networks_9", "doc_id": "Lecture9_Convolutional_Neural_Networks", "text": " 6 image\n3\n 0\n1\n3\n-1\n 1\n3\n02 x 2 imageEach filter is a channel\nNew image but smallerConv\nMaxPooling\nNew Jersey Institute of Technology\nWhy Pooling\n19COMP4434\n§Subsampling pixels will not change the object\nSubsampling\nbird bird\nWe can subsample the pixels to make image smallerfewer parameters to characterize the image\nNew Jersey Institute of Technology\nConvolutional kernel§A convolutional layer has a number of filters that does convolutional operation§This image show the convolutional operation f", "local_index": 9}
{"chunk_id": "Lecture9_Convolutional_Neural_Networks_10", "doc_id": "Lecture9_Convolutional_Neural_Networks", "text": " a number of filters that does convolutional operation§This image show the convolutional operation for one filter§Each filter detects a small pattern and learns its parameter\n20COMP4434\nNew Jersey Institute of Technology\nThe whole CNN\n21\nConvolution\nMax Pooling\nConvolution\nMax Pooling\nCan repeat many times\nA new image\nThe number of channels is the number of filters\nSmaller than the original image\n3\n 0\n1\n3\n-1\n 1\n3\n0\nNew Jersey Institute of Technology\nThe whole CNN\nFully Connected Feedforward netw", "local_index": 10}
{"chunk_id": "Lecture9_Convolutional_Neural_Networks_11", "doc_id": "Lecture9_Convolutional_Neural_Networks", "text": "3\n 0\n1\n3\n-1\n 1\n3\n0\nNew Jersey Institute of Technology\nThe whole CNN\nFully Connected Feedforward network\ncat dog ……\nConvolution\nMax Pooling\nConvolution\nMax Pooling\nFlattened\nA new image\nA new image\nNew Jersey Institute of Technology23\n3\n 0\n1\n3\n-1\n 1\n3\n0 Flattened\n3\n0\n1\n3\n-1\n1\n0\n3\nFully Connected Feedforward network\nNew Jersey Institute of Technology\nA CNN compresses a fully connected network§Reducing number of connections§Shared weights on the edges§Max pooling further reduces the complexity\n24CO", "local_index": 11}
{"chunk_id": "Lecture9_Convolutional_Neural_Networks_12", "doc_id": "Lecture9_Convolutional_Neural_Networks", "text": "ng number of connections§Shared weights on the edges§Max pooling further reduces the complexity\n24COMP4434\nNew Jersey Institute of Technology\nConvolutional Neural Networks in 1998\n§LeNet: a layered model composed of convolution and subsampling operations followed by a holistic representation and ultimately a classifier for handwritten digits§CPU 25COMP4434\n\nNew Jersey Institute of Technology\nColor image: RGB 3 channels\n26\n100001010010001100100010010010001010\n100001010010001100100010010010001010\n", "local_index": 12}
{"chunk_id": "Lecture9_Convolutional_Neural_Networks_13", "doc_id": "Lecture9_Convolutional_Neural_Networks", "text": " image: RGB 3 channels\n26\n100001010010001100100010010010001010\n100001010010001100100010010010001010\n100001010010001100100010010010001010\n1-1-1-11-1-1-11Filter 1-11-1-11-1-11-1Filter 21-1-1-11-1-1-111-1-1-11-1-1-11\n-11-1-11-1-11-1-11-1-11-1-11-1\nColor image\nEach image can store discrete pixels with conventional brightness intensities between 0 and 255\nNew Jersey Institute of Technology\n3 channels -> depth of filters = 3\n27COMP4434\n§A filter must always have the same number of channels as the inpu", "local_index": 13}
{"chunk_id": "Lecture9_Convolutional_Neural_Networks_14", "doc_id": "Lecture9_Convolutional_Neural_Networks", "text": "> depth of filters = 3\n27COMP4434\n§A filter must always have the same number of channels as the input, often referred to as “depth”§Weighted sum from 3 channels\nstride=1\nNew Jersey Institute of Technology\nConvolutional Neural Networks in 2012\n§Input 227*227*3. GPU.§AlexNet: a layered model composed of convolution, subsampling, and further operations followed by a holistic representation and all-in-all a landmark classifier on ImageNet Large Scale Visual Recognition Challenge 2012§+ data; + gpu; ", "local_index": 14}
{"chunk_id": "Lecture9_Convolutional_Neural_Networks_15", "doc_id": "Lecture9_Convolutional_Neural_Networks", "text": "-all a landmark classifier on ImageNet Large Scale Visual Recognition Challenge 2012§+ data; + gpu; + non-saturating nonlinearity; + regularization28\nNew Jersey Institute of Technology\nPadding\n29\n\nNew Jersey Institute of Technology\nExercise§Suppose your input size is 64x64x16. You use a convolutional layer with 32 filters that are each 6x6, and a stride of 2 and padding of 1. What is the output size of this convolutional layer?§(64 + 2 ∗ 1 − 6)/2 + 1 = 31§The output size is 31x31x32\n30COMP4434\nN", "local_index": 15}
{"chunk_id": "Lecture9_Convolutional_Neural_Networks_16", "doc_id": "Lecture9_Convolutional_Neural_Networks", "text": "ze of this convolutional layer?§(64 + 2 ∗ 1 − 6)/2 + 1 = 31§The output size is 31x31x32\n30COMP4434\nNew Jersey Institute of Technology\nThe popular CNNs\n§LeNet, 1998§AlexNet, 2012§VGGNet, 2014§ResNet, 2015 31COMP4434\n\nNew Jersey Institute of Technology\n32\n•Input: 32*32*1•7  layers •2 conv and 4 fully connected layers for classification•60 thousand parameters•Only two complete convolutional layers (Conv, nonlinearities, and pooling as one complete layer)\n•\n224*224*3\n•\n8  layers\n•\n5 conv and 3 fully", "local_index": 16}
{"chunk_id": "Lecture9_Convolutional_Neural_Networks_17", "doc_id": "Lecture9_Convolutional_Neural_Networks", "text": "onv, nonlinearities, and pooling as one complete layer)\n•\n224*224*3\n•\n8  layers\n•\n5 conv and 3 fully classification\n•\n5 convolutional layers, and 3,4,5 stacked on top \nof each other\n•\nThree complete conv layers\n•\n60 million parameters, insufficient data\n•\nData augmentation: \n–\nPatches (224 from 256 input), translations, \nreflections\n–\nPCA, simulate changes in intensity and colors\nLeNet vs AlexNet\nNew Jersey Institute of Technology\nVGGNet§16 layers§Only 3*3 convolutions§138 million parameters\n33C", "local_index": 17}
{"chunk_id": "Lecture9_Convolutional_Neural_Networks_18", "doc_id": "Lecture9_Convolutional_Neural_Networks", "text": "New Jersey Institute of Technology\nVGGNet§16 layers§Only 3*3 convolutions§138 million parameters\n33COMP4434\n\nNew Jersey Institute of Technology\nResNet\n§152 layers§skip connections§ResNet50 34COMP4434\n\nNew Jersey Institute of Technology\nComputational complexity\n§The memory bottleneck§GPU, a few GB35COMP4434\n\nNew Jersey Institute of Technology\nCNN Application 1: AlphaGo\n36COMP4434\nNeuralNetwork(19 x 19 positions)Next move\n19 x 19 matrixBlack: 1white: -1none: 0\nFully-connected feedforward network c", "local_index": 18}
{"chunk_id": "Lecture9_Convolutional_Neural_Networks_19", "doc_id": "Lecture9_Convolutional_Neural_Networks", "text": " 19 positions)Next move\n19 x 19 matrixBlack: 1white: -1none: 0\nFully-connected feedforward network can be used\nBut CNN performs much better\nNew Jersey Institute of Technology\nAlphaGo’s policy network\n37COMP4434\nNote: AlphaGo does not use Max Pooling.The following is quotation from their Nature article:\nNew Jersey Institute of Technology\nCNN application 2: Semantic segmentation\n38\nNew Jersey Institute of Technology\n39COMP4434\n", "local_index": 19}
{"chunk_id": "Lecture9_Convolutional_Neural_Networks_20", "doc_id": "Lecture9_Convolutional_Neural_Networks", "text": "ute of Technology\n39COMP4434\n", "local_index": 20}
